[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Statistics",
    "section": "",
    "text": "Statistics and Data"
  },
  {
    "objectID": "index.html#the-central-dogma-of-statistics",
    "href": "index.html#the-central-dogma-of-statistics",
    "title": "Statistics",
    "section": "The Central Dogma of Statistics",
    "text": "The Central Dogma of Statistics\nSimilar to the central dogma in biology, statistical inference also has a core principle that guides its objectives. This central dogma centers around trying to know a characteristic about a large population. Since measuring the entire population is costly and challenging, a subset of a population called a sample is taken to estimate the characteristics of the entire population using probability."
  },
  {
    "objectID": "index.html#levels-of-data",
    "href": "index.html#levels-of-data",
    "title": "Statistics",
    "section": "Levels of Data",
    "text": "Levels of Data\nMeasurement, or data collection, is “the assignment of numerals to objects or events according to rules” (Stevens 1946). In general, when collecting and analyzing data, it is important to understand the levels of measurement. These levels determine the precision of the variables, which in turn affects the types of statistical analyses that can be conducted. There are four levels of data measurement: nominal, ordinal, interval, and ratio. Each level builds upon the previous one, increasing in precision and mathematical complexity.\n\nNominal Data\nNominal data is the simplest level of data in which objects are assigned a number based on the category they belong to. It refers to variables that are categorized into qualitative groups or labels without any order or hierarchy. The only information provided by nominal data is the group an observation belongs to. Examples of nominal data include hair color, gender, and religion. Mathematical operations are limited for nominal data, with the mode being the only appropriate measure. Nominal data requires that categories be mutually exclusive (categories do not overlap) and exhaustive (every object belongs to a category and there are no leftover objects).\n\n\nOrdinal Data\nOrdinal data builds upon nominal data by categorizing variables into ordered or hierarchical groups. Although the categories follow a specific order, the difference between them is not defined. Ordinal data allows for comparisons like “less than” and “greater than,” as well as equality and inequality. However, operations such as addition and subtraction are not applicable. Examples of ordinal data include education level, academic grades, and the Likert Scale.\n\n\nInterval Data\nInterval data further builds upon ordinal data by specifying equal intervals between subsequent values. This allows for the calculation of the arithmetic mean, as well as the mode and median. Interval data also allows for linear transformations, but ratios cannot be applied due to the lack of a meaningful zero value. Examples of interval data include temperature measured in degrees Celsius or Fahrenheit and calendar years.\n\n\nRatio Data\nRatio data is similar to interval data but includes a meaningful zero value, allowing for the calculation of ratios between data points. With this level of measurement, all mathematical operations, including multiplication and division, are applicable. Examples of ratio data include weight, height, age, and amounts of money. Since this data type has an absolute zero, this means that we can natively make ratio comparisons. For example, if a boy weighs 100 lb. and a girl weighs 200 lbs. it is legal from this information to say that since 100 = 200/2, the boy is half the weight of the girl. This is not legal for temperature. If place A is 15 degrees Celsius (59 F) and place B is 30 degrees Celsius (86 F), it is not legal to say that place B is twice as hot as place A.\n\n\nLevels of measurement chart\n\nAcceptable mathematical operations for each level of data. (Warne 2020)\n\n\nMathematical Function\nNominal\nOrdinal\nInterval\nRatio\n\n\n\n\nClassification\nY\nY\nY\nY\n\n\nCounting\nY\nY\nY\nY\n\n\nProportion/Percentages\nY\nY\nY\nY\n\n\nRank Ordering\nN\nY\nY\nY\n\n\nAddition\nN\nN\nY\nY\n\n\nSubtraction\nN\nN\nY\nY\n\n\nDividing to form averages\nN\nN\nY\nY\n\n\nDividing to form ratios\nN\nN\nN\nY\n\n\n\n\n\n\n\nStevens, S. S. 1946. “On the Theory of Scales of Measurement.” Science 103 (2684): 677–80. https://doi.org/10.1126/science.103.2684.677.\n\n\nWarne, R. T. 2020. Statistics for the Social Sciences: A General Linear Model Approach. Cambridge University Press. https://books.google.com/books?id=Lw0FEAAAQBAJ."
  },
  {
    "objectID": "i_univariate.html#the-data",
    "href": "i_univariate.html#the-data",
    "title": "1  Univariate Data",
    "section": "1.1 The Data",
    "text": "1.1 The Data\nUnivariate data is a fundamental concept in the field of statistics, providing valuable insights into a single variable’s characteristics within a dataset. The analysis of univariate data focuses on two primary aspects: central tendency and variability. Central tendency describes the central value of the data, such as the mean, median, or mode, while variability explores how the data points spread around the central value, using measures like range, variance, and standard deviation. In this section, we will delve into the intricacies of univariate data analysis, using the ‘mpg’ (miles per gallon) column from the well-known ‘mtcars’ dataset as our example. This approach will enable us to better understand the distribution and behavior of fuel efficiency in various car models, ultimately illuminating the importance of univariate data analysis in making informed decisions and predictions.\nThe univariate data\nThe ‘mtcars’ dataset is a classic dataset in the field of statistics and data analysis, originating from the 1974 edition of Motor Trend magazine. It contains data on 32 different car models, focusing primarily on various technical aspects and performance indicators of automobiles from the early 1970s. The dataset includes information such as miles per gallon (mpg), number of cylinders, horsepower, weight, and other specifications. The ‘mpg’ column within the ‘mtcars’ dataset represents the fuel efficiency of each car model, measured in miles per gallon. In other words, it indicates the number of miles a car can travel on one gallon of fuel.\n\n\nCode\nlibrary(tidyverse)\nlibrary(knitr)\nlibrary(kableExtra)\nlibrary(plotly)\n\nmtcars %>% \n    head(6) %>%\n    select(mpg) %>% \n    kbl() %>%\n    kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\"))\n\n\n\n\n \n  \n      \n    mpg \n  \n \n\n  \n    Mazda RX4 \n    21.0 \n  \n  \n    Mazda RX4 Wag \n    21.0 \n  \n  \n    Datsun 710 \n    22.8 \n  \n  \n    Hornet 4 Drive \n    21.4 \n  \n  \n    Hornet Sportabout \n    18.7 \n  \n  \n    Valiant \n    18.1 \n  \n\n\n\n\n\nVisualizing univariate data\nIn the visualizations below, notice two things. The a lot of the points tend to cluster around a central point, and that the rest of the points tend to deviate away.\n\n\nCode\nfig1 = (bind_rows(\n    (mtcars %>% \n     mutate(mpg = mean(mpg),\n            X=0,\n            type='0: The average')),\n    (mtcars %>% \n      mutate(X=0,\n             type='1: The values'))\n  ) %>% \n    ggplot(aes(x=X, y = mpg, frame = type)) +\n    geom_point(color = 'darkblue',\n               fill='lightblue',\n               size=2.5,\n               shape=21,\n               alpha=.9) +\n    geom_hline(aes(yintercept = mean(mpg)), \n            linetype = \"dashed\", \n            color = \"red\", \n            size = .75,\n            alpha=.5) +\n    theme_classic()  +\n    xlab(\"\") +\n    theme(axis.ticks.x = element_blank(), \n          axis.text.x = element_blank()) +\n    coord_cartesian(xlim=c(-1,10))\n) %>% ggplotly() %>% \n  animation_opts(\n    200, easing = \"elastic\", redraw = F)\n\n\nfig2 = (bind_rows(\n  mtcars %>% mutate(type='1: The values'),  \n  mtcars %>% mutate(type='0: The average',mpg = mean(mpg))\n  ) %>% \n  ggplot(aes(y=mpg, frame=type))+\n  geom_boxplot(color = 'darkblue',\n               fill='lightblue',\n               size=1)+\n  theme_classic() +\n  theme(axis.ticks.x = element_blank(), \n        axis.text.x = element_blank())) %>% \n  ggplotly() %>% \n  animation_opts(\n    200, easing = \"elastic\", redraw = F)\n\nsubplot(fig1, fig2) %>% \n  layout(title = 'Side By Side Subplots')"
  },
  {
    "objectID": "i_univariate.html#central-tendency",
    "href": "i_univariate.html#central-tendency",
    "title": "1  Univariate Data",
    "section": "1.2 Central Tendency",
    "text": "1.2 Central Tendency\nCentral tendency is a measure that aims to describe the center or typical value of a dataset. There are three primary measures of central tendency: mean, median, and mode. Each of these measures provides a different perspective on the central value of the data and can be more or less appropriate depending on the nature of the data and its distribution.\n\n1.2.1 Mean\nMean: The mean, also known as the arithmetic average, is the sum of all data points divided by the total number of data points. It is a widely used measure of central tendency and provides a good indication of the overall level of the dataset. However, the mean can be sensitive to outliers or extreme values, which may result in a distorted representation of the central value.\n\\[\n\\bar{Y} = \\frac{\\sum_{i=1}^n Y_i}{n}\n\\]\nCalculating the mean for the ‘mpg’ column involves summing up the fuel efficiency values of all car models and dividing by the total number of car models in the dataset. The mean mpg provides an indication of the average fuel efficiency across all car models, although it may be influenced by any extreme values or outliers in the data.\n\nn = 32            # number of cars\nY_i = mtcars$mpg  # list of car mpgs\n\n\nY_bar = sum(Y_i)/n\n\nprint(paste(\"The mean value of mpg is\", Y_bar))\n\n[1] \"The mean value of mpg is 20.090625\"\n\n\n\n\n1.2.2 Median\nMedian: The median is the middle value of a dataset when the data points are arranged in ascending or descending order. If there is an odd number of data points, the median is the middle value; if there is an even number of data points, the median is the average of the two middle values. The median is less sensitive to outliers or extreme values than the mean and is particularly useful for datasets with skewed distributions.\n\nY_median = median(Y_i)\n\nprint(paste(\"The median value of mpg is\", Y_median))\n\n[1] \"The median value of mpg is 19.2\"\n\n\n\n\n1.2.3 Mode\nMode: The mode is the value that occurs most frequently in a dataset. A dataset can have multiple modes, no mode, or a single mode. The mode can be a useful measure of central tendency for categorical or discrete data, where the mean and median may not be appropriate. However, the mode may not always provide a meaningful representation of the central value, especially in datasets with several modes or a low frequency of the most common value.\n\nY_mode = median(Y_i)\n\nprint(paste(\"The modal value of mpg is\", Y_mode))\n\n[1] \"The modal value of mpg is 19.2\""
  },
  {
    "objectID": "i_univariate.html#variability",
    "href": "i_univariate.html#variability",
    "title": "1  Univariate Data",
    "section": "1.3 Variability",
    "text": "1.3 Variability\n\n1.3.1 Variance\nVariance is a measure of variability or dispersion in a dataset, representing the average of the squared differences from the mean. It helps us understand how spread out the data points are from the average value. To calculate the variance for univariate data, follow these steps:\n\nCalculate the mean: First, find the mean (arithmetic average) of the dataset by adding up all the data points and dividing the sum by the total number of data points.\nCalculate the deviations: For each data point, calculate the deviation from the mean by subtracting the mean from the individual data point’s value.\nSquare the deviations: Square each deviation obtained in step 2. This step removes the negative signs and emphasizes larger deviations from the mean.\nCalculate the sum of squared deviations: Add up all the squared deviations from step 3. This is called the Total Sum of Squares.\nDivide the Total Sum of Squares by the sum: This is the variance. It represents the average squared deviation of the data points from the mean.\n\nStep One: The mean\nSee above\nStep Two: Distance from the mean\nEvery dot below represents a car in which each vertical location of the dot represents how high or low the mpg is. We can notice that there are some near the average and some farther away from the average. (The average is the horizontal dashed red line in the middle)\nWe can make a list of distances to represent how far away from the mean each car/point is. The cars/points with a very long red line either have a really high or really low mpg while the cars/points with a very small red line have a more average mpg.\n\n\nCode\nmtcars2 = mtcars %>% mutate(row = row_number())\n\nmylm <- lm(mpg~row, data=mtcars2)\n\n\ndf1 <- bind_rows(\n  \n  mtcars2 %>% mutate(\n           type='1, the distances',\n           xmin = row - (mpg-mylm$fit),\n           xmax = row,\n           ymin = mean(mpg),\n           ymax = mpg,\n           rect_alpha=.2),\n  \n  mtcars2 %>% mutate(\n           mpg = mean(mpg)+.1, \n           type='0, the average',\n           xmin = row,\n           xmax = row + (mean(mpg)-mpg) +1,\n           ymin = mpg,\n           ymax = mean(mpg),\n           rect_alpha=.01)\n  ) %>% \n    select(row, mpg, type, xmin, xmax, ymin, ymax,rect_alpha)\n\n(df1 %>% \n  ggplot(aes(row, mpg)) +\n  geom_segment(aes(x = row, xend = row, y = mpg, yend = mean(mpg), frame=type), \n               color = 'red', \n               alpha=.9)+\n  geom_point(aes(frame = type),color = 'darkblue',\n               fill='lightblue',\n               size=2.5,\n               shape=21,\n               alpha=.9) +\n  theme_classic() +\n  geom_hline(aes(yintercept = mean(mpg)), \n             linetype = \"dashed\", \n             color = \"red\", \n             size = .75,\n             alpha=.3) +\n  xlab(\"\") +\n  theme(axis.ticks.x = element_blank(), \n        axis.text.x = element_blank()) \n  \n  # geom_rect(aes(frame = type, xmin=xmin, xmax=xmax, ymin=ymin, ymax=ymax), \n  #           alpha=.3, \n  #           fill='gray',\n  #           color='red')\n  ) %>% \n  ggplotly() %>% animation_opts(\n    2000, easing = \"elastic\", redraw = FALSE\n  )\n\n\n\n\n\n\nTo calculate the distance from the mean for any given point/car, we just need two things. The miles per gallon of the car, and the mean mpg.\n\\[\n\\text{Distance from the mean} = Y_i - \\bar{Y}\n\\]\n\n# Car 22 has an mpg of 15.5 Below is the distance from the mean for car 22\n\n15.5 - Y_bar\n\n[1] -4.590625\n\n\nStep Three: Square the deviations:\nSquares are squares.\nWhen you have a literal square where the side is \\(n\\) units long, the area of that square is \\(n^2\\). We can take the distances and square them. Below is what this would look like visually.\n\n\nCode\nmtcars2 = mtcars %>% mutate(row = row_number())\n\nmylm <- lm(mpg~row, data=mtcars2)\n\n\ndf1 <- bind_rows(\n  \n  mtcars2 %>% mutate(\n           type='1, the squared distances',\n           xmin = row - (mpg-mylm$fit),\n           xmax = row,\n           ymin = mean(mpg),\n           ymax = mpg,\n           rect_alpha=.2),\n  \n  mtcars2 %>% mutate(\n           mpg = mean(mpg)+.1, \n           type='0, the average',\n           xmin = row,\n           xmax = row + (mean(mpg)-mpg) +1,\n           ymin = mpg,\n           ymax = mean(mpg),\n           rect_alpha=.01)\n  ) %>% \n    select(row, mpg, type, xmin, xmax, ymin, ymax,rect_alpha)\n\n(df1 %>% \n  ggplot(aes(row, mpg)) +\n  geom_point(aes(frame = type),color = 'darkblue',\n               fill='lightblue',\n               size=2.5,\n               shape=21,\n               alpha=.9) +\n  theme_classic() +\n  geom_hline(aes(yintercept = mean(mpg)), \n             linetype = \"dashed\", \n             color = \"red\", \n             size = .75,\n             alpha=.3) +\n  xlab(\"\") +\n  theme(axis.ticks.x = element_blank(), \n        axis.text.x = element_blank()) +\n  geom_rect(aes(frame = type, xmin=xmin, xmax=xmax, ymin=ymin, ymax=ymax),\n            alpha=.3,\n            fill='gray',\n            color='red') +\n  coord_cartesian(xlim=c(-6,40))\n  ) %>% \n  ggplotly() %>% animation_opts(\n    2000, easing = \"elastic\", redraw = FALSE\n  )\n\n\n\n\n\n\n\n\n\n\n\n\nWhy we square the deviations\n\n\n\n\n\nSquaring the deviations in the calculation of variance serves several important purposes in understanding the variability or dispersion of a dataset.\nFirst, squaring the deviations eliminates any negative signs that may arise from subtracting the mean from individual data points. Without squaring, the sum of deviations would always equal zero, rendering it an ineffective measure of variability. By squaring the deviations, we ensure that all values are positive, allowing us to properly assess the dispersion.\nSecond, squaring the deviations emphasizes larger deviations from the mean, giving more weight to outliers or extreme values in the dataset. This property is particularly useful in detecting and understanding the impact of outliers on the overall variability of the data. By emphasizing these larger deviations, the variance provides a comprehensive picture of the spread of the data points, including the influence of extreme values.\nLastly, squaring the deviations ensures that the variance has mathematical properties that are useful in statistical analysis, such as being additive for independent variables. This property allows for the combination of variances from multiple variables or datasets, facilitating more complex analyses and comparisons.\n\n\n\nBelow is the code to find the squared distance from the mean:\n\n# Car 22 has an mpg of 15.5 Below is the distance from the mean for car 22\n\n(15.5 - Y_bar)^2\n\n[1] 21.07384\n\n\nStep Four: Calculate the sum of squared deviations\n\nFrom the plot above, look at the squares create above. We can sum up all the squares to make a mega-square. This is called the \\(\\text{Total Sum of Squares}\\) or \\(\\text{SSTO}\\)\n\\[\n\\text{SSTO} = \\sum_{i=1}^n \\left(Y_i - \\bar{Y}\\right)^2\n\\]\nBelow is the code for the total sum of squares\n\nSSTO <- sum((Y_i - Y_bar)^2)\n\nFive: Divide the Total Sum of Squares by the sum\n\nThe sum of observations divided by the number of observations is an average. We have the total sum of squares, but instead of dividing it by the number of observations, we will divide it by number of observations - 1.\n\n\n\n\n\n\nDegrees of Freedom\n\n\n\n\n\nDegrees of freedom refer to the number of independent pieces of information that are available in the sample. Estimates about the population can be calculated from our sample. For example, the mean can be generated from our sample. However, if we use an estimate to get another estimate- we have lost a degree of freedom. For example, the formula for variance uses the mean, meaning a “degree of freedom” is lost- thus degrees of freedom is equal to \\(n-1\\).\n\n\n\n\\[\n\\text{Variance} = \\sigma^2= \\frac{\\text{SSTO}}{\\text{degrees of freedom}}\n\\]\nThe code for the variance is below\n\nvariance = SSTO / (n-1)\n\nvariance\n\n[1] 36.3241\n\n\n\n\n1.3.2 Standard Deviation\nThe variance represents the average squared distance away from the mean- so to see just the distance away from the mean, square root it.\n\\[\n\\sqrt{\\sigma^2} = \\sigma\n\\]\nor in other words:\n\\[\n\\sqrt{\\text{Variance}} = \\text{Standard Deviation}\n\\]\n\n\nstandard_deviation = sqrt(variance)\n\nstandard_deviation\n\n[1] 6.026948\n\n\nBelow, the purple dashed lines represent one standard deviation above and below the mean.\n\n\nCode\nmtcars2 = mtcars %>% mutate(row = row_number())\n\nmylm <- lm(mpg~row, data=mtcars2)\n\n\ndf1 <- bind_rows(\n  \n  mtcars2 %>% mutate(\n           type='1, the distances',\n           xmin = row - (mpg-mylm$fit),\n           xmax = row,\n           ymin = mean(mpg),\n           ymax = mpg,\n           rect_alpha=.2),\n  \n  mtcars2 %>% mutate(\n           mpg = mean(mpg)+.1, \n           type='0, the average',\n           xmin = row,\n           xmax = row + (mean(mpg)-mpg) +1,\n           ymin = mpg,\n           ymax = mean(mpg),\n           rect_alpha=.01)\n  ) %>% \n    select(row, mpg, type, xmin, xmax, ymin, ymax,rect_alpha)\n\n(df1 %>% \n  ggplot(aes(row, mpg)) +\n  geom_segment(aes(x = row, xend = row, y = mpg, yend = mean(mpg), frame=type), \n               color = 'red', \n               alpha=.9)+\n  geom_point(aes(frame = type), color = 'darkblue',\n               fill='lightblue',\n               size=2.5,\n               shape=21,\n               alpha=.9) +\n  theme_classic() +\n  geom_hline(aes(yintercept = mean(mpg)), \n             linetype = \"dashed\", \n             color = \"red\", \n             size = .75,\n             alpha=.3) +\n  xlab(\"\") +\n  geom_hline(aes(yintercept = Y_bar + standard_deviation), alpha=.5, color = \"purple\", linetype = \"dashed\") +\n  geom_hline(aes(yintercept = Y_bar - standard_deviation), alpha=.5, color = \"purple\", linetype = \"dashed\") +\n  theme(axis.ticks.x = element_blank(), \n        axis.text.x = element_blank()) \n  ) %>% \n  ggplotly() %>% animation_opts(\n    2000, easing = \"elastic\", redraw = FALSE\n  )"
  },
  {
    "objectID": "i_distributions.html#calculating-z-scores",
    "href": "i_distributions.html#calculating-z-scores",
    "title": "2  Univariate Distributions",
    "section": "2.2 Calculating Z-scores",
    "text": "2.2 Calculating Z-scores\nA z-score is a standardized measure that indicates the relative position of a data point within a distribution. It is calculated using the following formula:\n\\(z = \\frac{(x - \\mu)}{\\sigma}\\)\nwhere:\n\n\\(z\\) is the z-score\n\\(x\\) is the individual observation\n\\(\\mu\\) is the mean of the distribution\n\\(\\sigma\\) is the standard deviation of the distribution\n\n\n\n\n\n\n68-95-99.7 Rule: Approximately 68% of the observations fall within one standard deviation of the mean (\\(\\mu \\pm \\sigma\\)), 95% within two standard deviations (\\(\\mu \\pm 2\\sigma\\)), and 99.7% within three standard deviations (\\(\\mu \\pm 3\\sigma\\))."
  },
  {
    "objectID": "i_bivariate.html#the-data",
    "href": "i_bivariate.html#the-data",
    "title": "4  Bivariate Data",
    "section": "4.1 The Data",
    "text": "4.1 The Data\nThe ‘mtcars’ dataset is a classic dataset that has been widely used in the field of statistics and data analysis. It comprises data on 32 different car models from the 1970s, offering a comprehensive snapshot of various automobile attributes from that era. In this textbook, we will focus on two key variables from the ‘mtcars’ dataset: ‘mpg’ (miles per gallon) and ‘weight’ (the weight of the car in pounds).\n‘Mpg’ represents the fuel efficiency of a car, with higher values indicating better efficiency. It is an important metric for consumers and manufacturers alike, as it provides information about the overall performance and cost-effectiveness of a vehicle. On the other hand, ‘weight’ is a critical characteristic that impacts numerous aspects of a car’s performance, including acceleration, handling, and fuel consumption. By examining the relationship between ‘mpg’ and ‘weight’ through the lens of regression analysis, we will gain a better understanding of how these two variables interact and influence one another, providing valuable insights into the dynamics of automobile design and performance.\n\n\nCode\nlibrary(tidyverse)\nlibrary(knitr)\nlibrary(kableExtra)\nlibrary(plotly)\n\nmtcars %>% \n    head(6) %>%\n    select(wt, mpg) %>% \n    kbl() %>%\n    kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\"))\n\n\n\n\n \n  \n      \n    wt \n    mpg \n  \n \n\n  \n    Mazda RX4 \n    2.620 \n    21.0 \n  \n  \n    Mazda RX4 Wag \n    2.875 \n    21.0 \n  \n  \n    Datsun 710 \n    2.320 \n    22.8 \n  \n  \n    Hornet 4 Drive \n    3.215 \n    21.4 \n  \n  \n    Hornet Sportabout \n    3.440 \n    18.7 \n  \n  \n    Valiant \n    3.460 \n    18.1 \n  \n\n\n\n\n\nVisualizing bivariate data\n\n\nCode\n(bind_rows(\n    (mtcars %>% \n      mutate(mpg = mean(mpg),\n             wt = mean(wt),\n             type = 'Average Y')),\n    (mtcars %>% \n     mutate(wt = mean(wt),\n            type = 'Variation of Y')),\n\n    (mtcars %>% \n      mutate(type = 'Variation of Y\\nexplained by X'))\n  ) %>% \n    ggplot(aes(x=wt, y = mpg, frame = type)) +\n    geom_point(color = 'darkblue',\n               fill='lightblue',\n               size=2.5,\n               shape=21,\n               alpha=.9) +\n    geom_hline(aes(yintercept = mean(mpg)), \n            linetype = \"dashed\", \n            color = \"red\", \n            alpha=.5) +\n    theme_classic()\n ) %>% ggplotly() %>% \n  animation_opts(\n    2000, easing = \"elastic\", redraw = F)"
  },
  {
    "objectID": "i_bivariate.html#simple-linear-regression",
    "href": "i_bivariate.html#simple-linear-regression",
    "title": "4  Bivariate Data",
    "section": "4.2 Simple Linear Regression",
    "text": "4.2 Simple Linear Regression\nIn the Univariate Data section, we only focused on the mpg data where we found central tendency and variability. Regression is the process of explaining a variable with another variable(s).\n\n4.2.1 Explaining a Variable With Another Variable\nSimple linear regression is a statistical method used to model the relationship between a single independent variable (explanatory) and a single dependent variable (response). The goal is to find the best-fitting straight line that represents the relationship between the two variables, allowing us to make predictions and understand the underlying trends in the data. In this simple model, we only model a single explanatory variable (not multiple \\(X\\)’s)\n\n\\(X\\) - A single quantitative explanatory variable (independent)\n\\(Y\\) - A single quantitative response variable (dependent)\n\nA line with points\nWith points on a graph, we can see if they make a line.\n\nThe True Line: This is usually an unknown association that we try to estimate\nObserved Points: While this is not a line, this represents the actual observed points\nThe Regression Line: This is our estimate of the True Line\n\nTrue, Observed, and Estimated \\(Y\\)\nThe True Regression Line\n\\[\nE\\{Y\\}_i = \\beta_0 + \\beta_1 X_i\n\\]\nThe Observed Points\nEach point, assuming no assumptions are violated, are derived from the formula below\n\\[\nY_i = \\beta_0 + \\beta_1 X_i+ \\epsilon_i\n\\]\nWhile \\(Y_i = b_0 + b_1X_i + r_i\\) is mathematically true, it is important to note that the notation with population parameters is used as this is where the points actually are derived from.\nThe Estimated Regression Line obtained from a regression analysis from the observed points\n\\[\n\\hat{Y}_i = b_0 + b_1 X_i\n\\]\n\n\n\n\n\n\nTerminology\n\n\n\n\n\n\n\\(E\\{Y\\}_i\\) - True mean y-value, also \\(\\mu_{Y|X}_i\\) or \\(E\\{Y|X\\}_i\\)\n\\(\\beta_0\\) - True \\(y\\)-intercept\n\\(\\beta_1\\) - True slope\n\\(Y_i\\) - Response or dependent variable for the \\(i^{\\text{th}}\\) observation.\n\\(\\epsilon_i\\) - Error, distance of dot to true line. \\(\\epsilon_i = Y_i - E\\{Y\\}_i\\)\n\\(r_i\\) - Residual, distance of dot to estimated line. \\(r_i = Y_i - \\hat{Y}_i\\)\n\\(\\hat{Y}_i\\) - The fitted line\n\\(b_0\\) - Estimated \\(y\\)-intercept, also \\(\\hat{\\beta}_0\\)\n\\(b_1\\) - Estimated slope, also \\(\\hat{\\beta}_1\\)\n\n\n\n\n\n\n\n\n\n\nR code for calling your data\n\n\n\n\n\n\nlibrary(tidyverse)\n\nggplot(mtcars, aes(x=wt, y=mpg)) + \n  geom_point(shape = 21, color = 'skyblue4', fill = 'skyblue', size = 3) +\n  labs(title=\"Miles per Gallon vs. Weight of Car\", \n       x=\"Weight of Car\", \n       y=\"Miles per Gallon\") +\n  theme_classic()  \n\n\n\n\n\nY <- mtcars$mpg \nX <- mtcars$wt\n\ni <- 3 #random number\n\n\nX[i]\n\n[1] 2.32\n\n\n\nY[i]\n\n[1] 22.8\n\n\n\nlibrary(tidyverse)\n\nggplot(mtcars, aes(x=wt, y=mpg)) + \n  geom_point(shape = 21, color = 'skyblue4', fill = 'skyblue', size = 3) +\n  labs(title=\"Miles per Gallon vs. Weight of Car\", \n       x=\"Weight of Car\", y=\"Miles per Gallon\") +\n  theme_classic() +\n  geom_vline(aes(xintercept = wt[i] ), color = \"red\", linetype = \"dashed\") +\n  geom_hline(aes(yintercept = mpg[i]), color = \"red\", linetype = \"dashed\")\n\n\n\n\n\n\n\nEstimations\n\n\\(\\hat{Y}_i\\) estimates \\(E\\{Y\\}_i\\)\n\\(b_0\\) estimates \\(\\beta_0\\)\n\\(b_1\\) estimates \\(\\beta_1\\)\n\\(r_i\\) estimates \\(\\epsilon_i\\)\n\n\n\n4.2.2 Calculating the Estimated Regression Line\nThere are multiple ways to calculate the estimated regression line like with calculus or linear algebra.\n\n\n\n\n\n\nThe Calculus Behind Best Fit\n\n\n\n\n\n\nThere exists a combination of \\(b_0\\) and \\(b_1\\) that results in the lowest \\(\\text{SSE}\\). Geometrically, we can imagine a shape in three dimensions where two inputs (\\(b_0\\) and \\(b_1\\)) produce an output (\\(\\text{SSE}\\)). Roughly speaking, where the three dimensional slope is equal to 0 is where the combination of \\(b_0\\) and \\(b_1\\) will produce the lowest possible \\(\\text{SSE}\\). Since the derivative of an equation can tell us the slope at any given point, we can set both equations’ derivatives to zero to find where slope is zero for both equations.\nStep One: Take the partial derivatives with respect to \\(b_0\\) and \\(b_1\\)\nThe partial derivative with respect to \\(b_0\\) is calculated as so:\n\\[\n\\frac{\\partial}{\\partial b_0}\\sum_{i=1}^n\\left(Y_i - (b_0+b_1X_i)\\right)^2\n\\]\n\\[\n\\begin{align*}\n\\mathscr{}\n&= \\sum_{i=1}^n\\frac{\\partial}{\\partial b_0}\\left(Y_i - (b_0+b_1X_i)\\right)^2  &\\text{Sum Rule}\\\\\n&= \\sum_{i=1}^n2\\left(Y_i - (b_0+b_1X_i)\\right)\\frac{\\partial}{\\partial b_0}\\left(Y_i - (b_0+b_1X_i)\\right) &\\text{Power Rule}\\\\\n&= \\sum_{i=1}^n 2\\left(Y_i - (b_0+b_1X_i)\\right)(-1) &\\text{Simplify}\\\\\n&= -2\\sum_{i=1}^n\\left(Y_i - (b_0+b_1X_i)\\right) &\\text{Constant Multiple Rule for Sums}\n\\end{align*}\n\\]\nThe partial derivative with respect to \\(b_1\\) is calculated as so:\n\\[\n\\frac{\\partial}{\\partial b_1}\\sum_{i=1}^n\\left(Y_i - (b_0+b_1X_i)\\right)^2\n\\]\n\\[\n\\begin{align*}\n\\mathscr{}\n&= \\sum_{i=1}^n\\frac{\\partial}{\\partial b_1}\\left(Y_i - (b_0+b_1X_i)\\right)^2  &\\text{Sum Rule}\\\\\n&= \\sum_{i=1}^n2\\left(Y_i - (b_0+b_1X_i)\\right)\\frac{\\partial}{\\partial b_1}\\left(Y_i - (b_0+b_1X_i)\\right) &\\text{Power Rule}\\\\\n&= \\sum_{i=1}^n 2\\left(Y_i - (b_0+b_1X_i)\\right)(-X_i) &\\text{Simplify}\\\\\n&= -2\\sum_{i=1}^nX_i\\left(Y_i - (b_0+b_1X_i)\\right) &\\text{Constant Multiple Rule for Sums}\n\\end{align*}\n\\]\nStep Two: Set the simplified partial derivatives to zero and solve for \\(b_0\\) and \\(b_1\\) respectively\nSolving for \\(b_0\\): \\[0 = -2\\sum_{i=1}^n\\left(Y_i - (b_0+b_1X_i)\\right)\\]\n\\[\n\\begin{align*}\n\\mathscr{}\n&0 = \\sum_{i=1}^n\\left(Y_i - b_0-b_1x_i\\right) &\\text{Simplify}\\\\\n&0 = \\sum_{i=1}^nY_i - \\sum_{i=1}^nb_0-\\sum_{i=1}^nb_1X_i &\\text{Sum Rule}\\\\\n&0 = \\sum_{i=1}^nY_i - nb_0-b_1\\sum_{i=1}^nX_i &\\text{Constant Multiple Rule for Sums}\\\\\n& nb_0 = \\sum_{i=1}^nY_i -b_1\\sum_{i=1}^nX_i &\\text{Simplify}\\\\\n& b_0 = \\frac{\\sum_{i=1}^nY_i}{n} -\\frac{b_1\\sum_{i=1}^nX_i}{n} &\\text{Simplify}\\\\\n& b_0 = \\bar{Y} - b_1\\bar{X} &\\text{Simplify to average}\\\\\n\\end{align*}\n\\]\nSolving for \\(b_1\\) while substituting \\(b_0\\):\n\\[\n0 = -2\\sum_{i=1}^nX_i\\left(Y_i - (b_0+b_1X_i)\\right)\n\\]\n\\[\n\\begin{align*}\n\\mathscr{}\n&0 = \\sum_{i=1}^nX_i\\left(Y_i - ((\\bar{Y} - b_1\\bar{X})+b_1X_i)\\right) &\\text{Substitute }b_0\\\\\n&0 = \\sum_{i=1}^nX_i(Y_i-\\bar{Y}) - \\sum_{i=1}^nb_1X_i(X_i-\\bar{X})&\\text{Sum Rule}\\\\\n&0 = \\sum_{i=1}^nX_i(Y_i-\\bar{Y}) - b_1\\sum_{i=1}^nX_i(X_i-\\bar{X})&\\text{Constant Multiple Rule for Sums}\\\\\n& b_1= \\frac{\\sum_{i=1}^nX_i(Y_i-\\bar{Y})}{\\sum_{i=1}^nX_i(X_i-\\bar{X})} &\\text{Simplify}\\\\\n& b_1= \\frac{\\sum_{i=1}^n(X_i-\\bar{X})(Y_i-\\bar{Y})}{\\sum_{i=1}^n(X_i-\\bar{X})^2} &\\text{Simplify (steps skipped)}\\\\\n\\end{align*}\n\\]\nStep Three: Construct the formula\nOnce you have found \\(b_0\\) and \\(b_1\\) as so:\n\n\\(b_0 = \\bar{Y} - b_1\\bar{X}\\)\n\\(b_1= \\frac{\\sum_{i=1}^n(X_i-\\bar{X})(Y_i-\\bar{Y})}{\\sum_{i=1}^n(X_i-\\bar{X})^2}\\)\n\n(\\(\\bar{Y}\\) represents the mean of the \\(Y\\) values and \\(\\bar{X}\\) represents the mean of the \\(X\\) values)\nWe can construct the formula for \\(\\hat{Y}_i\\) as the following:\n\\[\n\\hat{Y}_i = b_0 + b_1X_i\n\\]\nWhich means \\(b_0\\) is the change in the mean of \\(Y\\) for a \\(1\\) unit increase in \\(X\\).\n\n\n\n\n\n\n\n\n\nLeast Squares in Linear Algebra\n\n\n\n\n\nStep One: Organize the data into vectors Let’s assume you have \\(n\\) observations of the independent variable \\(\\mathbf{X}\\) and dependent variable \\(\\mathbf{Y}\\). Organize the data into two vectors:\n\\[\n\\mathbf{X} = \\begin{bmatrix} X_1 \\\\ X_2 \\\\ \\vdots \\\\ X_n \\end{bmatrix}, \\quad \\mathbf{Y} = \\begin{bmatrix} Y_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_n \\end{bmatrix}\n\\]\nStep Two: Create a design matrix\nCreate a design matrix \\(\\mathbf{A}\\) by adding a column of ones to represent the intercept term\n\\[\n\\mathbf{A} = \\begin{bmatrix} 1 & X_1 \\\\ 1 & X_2 \\\\ \\vdots & \\vdots \\\\ 1 & X_n \\end{bmatrix}\n\\]\nStep Three: Make a Projection:\nThe goal of simple linear regression is to find the best-fitting line, which can be viewed as projecting the dependent variable \\(\\mathbf{Y}\\) onto the column space of the design matrix \\(\\mathbf{A}\\). This projection, denoted by \\(\\mathbf{\\hat{Y}}\\), can be computed as:\n\\[\n\\mathbf{\\hat{Y}} = \\mathbf{A}(\\mathbf{A}^T\\mathbf{A})^{-1}\\mathbf{A}^T\\mathbf{Y}\n\\]\nThis formula comes from the orthogonal projection of \\(\\mathbf{Y}\\) onto the column space of \\(\\mathbf{A}\\), which minimizes the squared residuals.\nStep Four: Compute the coefficients:\nThe coefficients of the best-fitting line can be found by solving the following linear system:\n\\[\n\\mathbf{A}^T\\mathbf{A}\\mathbf{\\beta} = \\mathbf{A}^T\\mathbf{Y}\n\\]\nHere, \\(\\mathbf{\\beta}\\) is a vector containing the coefficients \\(\\beta_0\\) and \\(\\beta_1\\):\n\\[\n\\mathbf{\\beta} = \\begin{bmatrix} \\beta_0 \\\\ \\beta_1 \\end{bmatrix}\n\\]\nTo solve for \\(\\mathbf{\\beta}\\), you can use the following formula:\n\\[\n\\mathbf{\\beta} = (\\mathbf{A}^T\\mathbf{A})^{-1}\\mathbf{A}^T\\mathbf{Y}\n\\]\nStep Five: Construct the formula:\nOnce you have computed the coefficients \\(\\beta_0\\) and \\(\\beta_1\\), you can write the estimated regression line as:\n\\[\n\\hat{Y}_i = \\beta_0 + \\beta_1 X_i\n\\]\nThe estimated slope, \\(\\beta_1\\), indicates the average change in the dependent variable (\\(Y\\)) for a one-unit increase in the independent variable (\\(X\\)). The estimated intercept, \\(\\beta_0\\), represents the predicted value of \\(Y\\) when \\(X = 0\\).\n\n\n\nThe formula from calculus to find the coefficients for the estimated regression line is as so:\n\n\\(b_0 = \\bar{Y} - b_1\\bar{X}\\)\n\\(b_1= \\frac{\\sum_{i=1}^n(X_i-\\bar{X})(Y_i-\\bar{Y})}{\\sum_{i=1}^n(X_i-\\bar{X})^2}\\)\n\n(\\(\\bar{Y}\\) represents the mean of the \\(Y\\) values and \\(\\bar{X}\\) represents the mean of the \\(X\\) values)\n\n\n\n\n\n\nR code for calculating the coefficients for the estimated regression line\n\n\n\n\n\n\nmylm <- lm(mpg ~ wt, data = mtcars)\npander::pander(summary(mylm))\n\n\n\n\n\n\n\n\n\n\n\n \nEstimate\nStd. Error\nt value\nPr(>|t|)\n\n\n\n\n(Intercept)\n37.29\n1.878\n19.86\n8.242e-19\n\n\nwt\n-5.344\n0.5591\n-9.559\n1.294e-10\n\n\n\n\nFitting linear model: mpg ~ wt\n\n\n\n\n\n\n\n\nObservations\nResidual Std. Error\n\\(R^2\\)\nAdjusted \\(R^2\\)\n\n\n\n\n32\n3.046\n0.7528\n0.7446\n\n\n\n\n\n\nb_0 <- mylm$coefficients[1]\nb_1 <- mylm$coefficients[2]\n\n\nb_0\n\n(Intercept) \n   37.28513 \n\n\n\nb_1\n\n       wt \n-5.344472 \n\n\n\nggplot(mtcars, aes(x=wt, y=mpg)) +\ngeom_point(shape = 21, color = 'skyblue4', fill = 'skyblue', size = 3) +\nstat_function(fun = function(x) b_0 + b_1 * x) + \ntheme_classic() \n\n\n\n\n\n\n\n\n\n4.2.3 Visualizing Simple Linear Regression\nBelow, we have charted both variables to try and see if some of the variance of \\(Y\\) is explained by \\(X\\). The blue line represents the estimated regression line.\nThe goal of the blue line below is to show the association between the \\(X\\) and \\(Y\\) variable. The slope can tell us how much our Y value changes by a change in our \\(X\\) value.\n\n\nCode\nlibrary(tidyverse)\nlibrary(plotly)\n\n\n(ggplot(mtcars, aes(x = wt, y = mpg)) +\n  geom_point(color = 'darkblue',\n               fill='lightblue',\n               size=2.5,\n               shape=21,\n               alpha=.9) +\n  theme_classic() +\n  geom_smooth(method = \"lm\", se = FALSE) \n  ) %>% \n  ggplotly()"
  },
  {
    "objectID": "i_bivariate.html#attributes-of-regression",
    "href": "i_bivariate.html#attributes-of-regression",
    "title": "4  Bivariate Data",
    "section": "4.3 Attributes of Regression",
    "text": "4.3 Attributes of Regression\n\n4.3.1 Residuals\nAs you can see from the plot, the points do not lie exactly on the estimated regression line. For any given point, there is a distance between the point and the regression line. This distance is called the residual.\nThe formula to calculate any given residual is below:\n\\[\n\\begin{align*}\nr_i &= \\left(Y_i - \\hat{Y_i}\\right)\\\\\nr_i &= \\left(Y_i - (b_0+b_1X_i)\\right)\n\\end{align*}\n\\]\n\n\n4.3.2 SSE: the Sum of Squared Errors\n\n\nCode\nmodel <- lm(mpg ~ wt, data = mtcars)\n\nmtcars$residuals <- residuals(model)\n\n(ggplot(mtcars, aes(x = wt, y = mpg)) +\n  theme_classic() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  geom_segment(aes(x = wt, xend = wt, y = mpg, yend = mpg - residuals), color = 'red') +\n  geom_point(color = 'darkblue',\n               fill='lightblue',\n               size=2.5,\n               shape=21,\n               alpha=.9)\n  ) %>% \n  ggplotly()\n\n\n\n\n\n\nIn the graph above, we can see the vertical distance between the actual points and the predicted line. Some points have a very large red line which means that the model may not explain that specific point very well. Conversely, the points with a small red line seem to be explained by the model very well.\nTo make sure the residuals (distances from the regression line) are positive, we will square them. We can add them all up to get a sum. Below is the formula to get the sum of the squared residuals\n\n\nCode\n(ggplot(mtcars, aes(x = wt, y = mpg)) +\n  theme_classic() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  geom_rect(aes(xmin=wt - abs(mpg-mylm$fit)*.13, xmax=wt, ymin=mylm$fit, ymax=mpg), color = 'red',alpha=.2) +\n  geom_segment(aes(x = wt, xend = wt, y = mpg, yend = mpg - residuals), color = 'red') +\n  geom_point(color = 'darkblue',\n               fill='lightblue',\n               size=2.5,\n               shape=21,\n               alpha=.9) +\n  coord_cartesian(xlim=c(1,6)) \n ) %>%\n  ggplotly()\n\n\n\n\n\n\n\\[\n\\begin{align*}\n\\text{SSE} &= \\sum_{i=1}^n \\left(r_i\\right)^2 \\\\\n\\text{SSE} &= \\sum_{i=1}^n\\left(Y_i - \\hat{Y_i}\\right)^2\\\\\n\\text{SSE} &= \\sum_{i=1}^n\\left(Y_i - (b_0+b_1X_i)\\right)^2\n\\end{align*}\n\\]\n(Note: SSE is used because E stands for error, the statistic that residuals try to approximate)\n(Note: The estimated regression line or line of best fit from the observed points is represented by the equation \\(\\hat{Y}_i=b_0+b_1X_i\\) , where \\(b_1\\) is the estimated slope of the line and \\(b_0\\) is the estimated y-intercept.)\n\n\n4.3.3 MSE: Mean Squared Error\nGeometrically, we can imagine the error “amount” as the area of the boxes. This is why when we add up all the boxes, we get the sum of squares error or \\(\\text{SSE}\\). This means we can estimate the average size of the box with a formula you might expect:\n\\[\n\\text{MSE} = \\frac{\\text{SSE}}{\\text{degrees of freedom}} \\\\\\;\\;\\\\\\;(\\text{where degrees of freedom}=n-2)\n\\]\n\n\n\n\n\n\nDegrees of Freedom\n\n\n\n\n\nSince two estimates (\\(b_0\\) and \\(b_1\\)) are used to calculate \\(\\text{MSE}\\) from our sample about the population, we have lost two degrees of freedom from our sample. Thus, degrees of freedom is now equal to \\(n-2\\)\n\n\n\nThus we can integrate it with our previous formulas\n\\[\n\\begin{align*}\n\\text{MSE} &= \\frac{\\sum_{i=1}^n \\left(r_i\\right)^2}{\\text{degrees of freedom}} \\\\\n\\text{MSE} &= \\frac{\\sum_{i=1}^n\\left(Y_i - \\hat{Y_i}\\right)^2}{\\text{degrees of freedom}}\\\\\n\\text{MSE} &= \\frac{\\sum_{i=1}^n\\left(Y_i - (b_0+b_1X_i)\\right)^2}{\\text{degrees of freedom}}\n\\\\\\;\\\\&(\\text{where degrees of freedom}=n-2)\n\\end{align*}\n\\]\n\n\n4.3.4 RMSE: Root Mean Squared Error or Residual Standard Error\nThe above formula calculates the average box size- but what about the average line size? Another similarly intuitive formula , we get:\n\\[\n\\text{RMSE} = \\sqrt{\\text{MSE}}\n\\]\nThus we can integrate it with our previous formulas\n\\[\n\\begin{align*}\n\\text{RMSE} &= \\sqrt{\\frac{\\text{SSE}}{\\text{degrees of freedom}}} \\\\\n\\text{RMSE} &= \\sqrt{\\frac{\\sum_{i=1}^n \\left(r_i\\right)^2}{\\text{degrees of freedom}}} \\\\\n\\text{RMSE} &= \\sqrt{\\frac{\\sum_{i=1}^n\\left(Y_i - \\hat{Y_i}\\right)^2}{\\text{degrees of freedom}}}\\\\\n\\text{RMSE} &= \\sqrt{\\frac{\\sum_{i=1}^n\\left(Y_i - (b_0+b_1X_i)\\right)^2}{\\text{degrees of freedom}}}\n\\\\\\;\\\\&(\\text{where degrees of freedom}=n-2)\n\\end{align*}\n\\]\n\n\n\n\n\n\nMinimizing Residuals\n\n\n\n\n\nA single residual, \\(r_i\\), is the distance between an observed point and the estimated regression line. Given any line, we can calculate how different our predicted value, \\(\\hat{Y_i}\\) is from our actual data point, \\(Y_i\\). This vertical distance from the real y output and the predicted y output is called the residual. The formula for any given residual is represented as such\n\\[\n\\begin{align*}\nr_i &= Y_i-\\hat{Y}_i\\\\\nr_i &= Y_i - (b_0+b_1X_i)\n\\end{align*}\n\\]\nThe goal of simple linear regression is to minimize the sum of the squared errors (\\(\\text{SSE}\\)). This is represented by the equations:\n\\[\n\\begin{align*}\n\\text{SSE} &= \\sum_{i=1}^n \\left(r_i\\right)^2 \\\\\n\\text{SSE} &= \\sum_{i=1}^n\\left(Y_i - \\hat{Y_i}\\right)^2\\\\\n\\text{SSE} &= \\sum_{i=1}^n\\left(Y_i - (b_0+b_1X_i)\\right)^2\n\\end{align*}\n\\]\nThe closer any given line is to the best fit line, the lower the \\(\\text{SSE}\\) will be. In fact, the best fit line represents the line with the lowest possible \\(\\text{SSE}\\). This line is found by manipulating the values for \\(b_0\\) and \\(b_1\\) since the \\(X\\) values must stay the same.\n\n\n\n\n\n4.3.5 SSR: The Sum of Squares Regression\nThe point of regression is to try to find an association between two variables. If a line explains some of the association between two variables, we should be able to “count” how much association it explains.\n\n\nCode\nmylm <- lm(mpg ~ wt, data = mtcars)\n\n\n\n(ggplot(mtcars %>% mutate(mpg = predict(mylm, data.frame(wt = wt))), aes(x = wt, y = mpg)) +\n  theme_classic() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  geom_rect(aes(xmin=wt, xmax=wt + (mean(mpg)-mpg)*.1, ymin=mpg, ymax=mean(mpg)), color = 'red',alpha=.2) +\n  geom_segment(aes(x = wt, xend = wt, y = mpg, yend = mean(mpg)), color = 'red') +\n  geom_point(color = 'darkblue',\n               fill='lightblue',\n               size=2.5,\n               shape=21,\n               alpha=.9) +\n  geom_hline(aes(yintercept = mean(mpg)), \n           linetype = \"dashed\", \n           color = \"red\", \n           alpha=.3) +\n  coord_cartesian(xlim=c(1,6), ylim=c(8,31.5))\n ) %>%\n  ggplotly()\n\n\n\n\n\n\nSSR essentially measures the extent to which the regression line captures the relationship between the X and Y. A higher SSR value indicates that the regression line is better at explaining the observed variation in the data. Conversely, a lower SSR value suggests that the regression line may not be a good fit for the data.\nThe SSR formula is as so:\n\\[\n\\text{SSR} = \\sum_{i=1}^{n} (\\hat{Y}_{i} - \\bar{Y})^2\n\\]\n\n\n4.3.6 Measures of Distances And Their Names\n\n\nCode\nmylm <- lm(mpg~wt, data=mtcars)\n\ndf1 <- bind_rows(\n  mtcars %>% mutate(\n           wt=mean(wt),\n           mpg=mean(mpg),\n           type='0, empty',\n           xmin = wt,\n           xmax = wt + (mean(mpg)-mpg)+.1,\n           ymin = mpg,\n           ymax = mean(mpg),\n           rect_alpha=.01),\n  mtcars %>% mutate(\n           wt=mean(wt),\n           type='1, ssto',\n           xmin = wt,\n           xmax = (wt+(mean(mpg)-mpg)*.07),\n           ymin = mpg,\n           ymax = mean(mpg),\n           rect_alpha=.2),\n  mtcars %>% mutate(\n           type='2, sse',\n           xmin = wt - abs(mpg-mylm$fit)*.07, \n           xmax = wt,\n           ymin = mylm$fit,\n           ymax = mpg,\n           rect_alpha=.2),\n  mtcars %>% mutate(\n           mpg = predict(mylm, data.frame(wt = wt)),\n           type='3, ssr',\n           xmin = wt, \n           xmax=wt + (mean(mpg)-mpg)*.07,\n           ymin = mpg,\n           ymax = mean(mpg),\n           rect_alpha=.2),\n  mtcars %>% mutate(\n           mpg = mean(mpg), \n           type='4, mean',\n           xmin = wt,\n           xmax = wt + (mean(mpg)-mpg) +.1,\n           ymin = mpg,\n           ymax = mean(mpg),\n           rect_alpha=.01)\n  ) %>% \n    select(wt, mpg, type, xmin, xmax, ymin, ymax,rect_alpha)\n\n(df1 %>% \n  ggplot(aes(x = wt, y = mpg)) +\n  theme_classic() +\n  stat_function(fun = function(x) b_0 + b_1 * x, \n                linetype = \"dashed\", \n                color = \"blue\", \n                alpha=.3) +\n  geom_rect(aes(frame = type, xmin=xmin, xmax=xmax, ymin=ymin, ymax=ymax), alpha=.3, fill='gray',color='red') +\n  geom_hline(aes(yintercept = mean(mpg)), \n             linetype = \"dashed\", \n             color = \"red\", \n             alpha=.3) +\n  geom_point(aes(frame = type),\n               color = 'darkblue',\n               fill='lightblue',\n               size=2.5,\n               shape=21,\n               alpha=.9) +\n  coord_cartesian(xlim=c(1.5,5.5), ylim=c(7,35))\n  ) %>% \n  ggplotly() %>% animation_opts(\n    2000, easing = \"elastic\", redraw = FALSE\n  )\n\n\n\n\n\n\nIn the example above, take note of three distances.\n\n\\(0 \\rightarrow 1\\): \\(\\text{SSTO}\\): The sum of “each point’s distance to the mean, squared”\n\\(1 \\rightarrow 2\\): Regression: adding in an explanatory variable\n\\(2 \\rightarrow 3\\): \\(\\text{SSE}\\): The sum of “each point’s distance to the line, squared”\n\\(3 \\rightarrow 4\\): \\(\\text{SSR}\\): The sum of “each point’s expected distance to the mean”\n\n\n\n\n\n\n\nR code for calculating the above terms\n\n\n\n\n\n\nmylm <- lm(mpg ~ wt, data=mtcars)\n\nfitted_values <- mylm$fitted.values\ny_bar <- mean(mtcars$mpg)\n\nSSE <- sum(mylm$residuals^2)\nSSR <- sum((fitted_values - y_bar)^2)\nSSTO <- sum((mtcars$mpg - y_bar)^2)\n\npander::pander(cat(\"Sum of Squared Errors (SSE):\", SSE, \"\\n\"))\n\nSum of Squared Errors (SSE): 278.3219\n\npander::pander(cat(\"Sum of Squares Regression (SSR):\", SSR, \"\\n\"))\n\nSum of Squares Regression (SSR): 847.7252\n\npander::pander(cat(\"Total Sum of Squares (SSTO):\", SSTO, \"\\n\"))\n\nTotal Sum of Squares (SSTO): 1126.047"
  },
  {
    "objectID": "i_bivariate.html#coefficient-of-determination",
    "href": "i_bivariate.html#coefficient-of-determination",
    "title": "4  Bivariate Data",
    "section": "4.4 Coefficient of Determination",
    "text": "4.4 Coefficient of Determination\nRatio of explained variance and total variance\n\nWe know why the points would be, in total, \\(\\text{SSR}\\) away from the average (since the estimated regression line explains the variance), but we don’t know why the points are \\(\\text{SSE}\\) away from the estimated regression line.\nNotice how \\(\\text{SSTO} = \\text{SSE} + \\text{SSR}\\). This can be equivalently written as \\(\\text{Total variance} = \\text{Unexplained variance} + \\text{Explained variance}\\)\nWe can show this relationship of explained/total variance as a coefficient.\n\n4.4.1 Calculating the Coefficient of Determination\nUsing the terms above, we can calculate a ratio of the variance of \\(Y\\) explained by the estimated regression line (\\(\\text{SSR}\\)) and the total variance of \\(Y\\) from the average \\(Y\\) value (\\(\\text{SSTO})\\).\n\\[\nR^2 = \\frac{\\text{SSR}}{\\text{SSTO}} = \\frac{\\text{Explained Variance}}{\\text{Total Variance}}\n\\]\n\\[\nR^2 = 1 - \\frac{\\text{SSE}}{\\text{SSTO}} = \\frac{\\text{Unexplained Variance}}{\\text{Total Variance}}\n\\]\nAll \\(R^2\\) represents is the ratio between variance explained by the model and total variance.\n\n\n\n\n\n\n\nR code for calculating the coefficient of determination\n\n\n\n\n\n\nlm_summary <- summary(mylm)\n\npander::pander(lm_summary$r.squared)\n\n0.7528\n\n#or\n\npander::pander(SSR/SSTO)\n\n0.7528\n\n#or\n\npander::pander(1 - SSE/SSTO)\n\n0.7528\n\n\n\n\n\n\n\n4.4.2 Slope and significance vs the \\(R^2\\)\nSlope: The slope indicates how much \\(Y\\) changes for a one-unit change in \\(X\\). This means that if our slope is significantly different than 0 (\\(\\text{p-value} < \\alpha\\)), we reject the notion that there is no association between \\(X\\) and \\(Y\\).\n\n\\(R^2\\): For the graphic above, notice how the slope is exactly the same for each graph- the difference is how much the data varies given the same slope. This is important since given the rule (slope), \\(R^2\\) will describe how ‘obedient’ the data is to the law.\n\n\n\n\n\n\nAssumptions for Simple Linear Regression\n\n\n\n\n\nIf we are to make a model off the points displayed as such:\n\\[\nY_i = \\beta_0 + \\beta_1 X_i+ \\epsilon_i\n\\]\nWe need to confirm the following assumptions\n\n1) Linear relation: Linear relationship between \\(X\\) and \\(Y\\)\n2) Normal errors: \\(\\epsilon \\sim \\mathcal{N}(0,\\sigma^2)\\), the error terms are normally distributed with a mean of 0\n3) Constant variance: The varianceB \\(O\u0003^2\\) of the error terms is constant (the same) across all values of \\(X_i\\)\n4) Fixed \\(X\\): the \\(X\\) values can be considered fixed and measured without error\n5) Independent errors: \\(\\epsilon\\) is independent\n\n\\(Y_i = \\beta_0 + \\beta_1 X_i+ \\epsilon_i :\\epsilon \\sim \\mathcal{N}(0,\\sigma^2)\\)\nGiven \\(X\\), \\(Y \\sim \\mathcal{N}(\\beta_0 +\\beta_1X,\\sigma^2)\\)\n\n\n\n\n\n\n\n\n\nR code for testing the assumptions for simple linear regression\n\n\n\n\n\nThree plots in R can be used to test the assumptions for simple linear regression:\nResiduals versus Fitted-values Plot The linear relationship and constant variance assumptions can be diagnosed using a residuals versus fitted-values plot. The fitted values are the \\(Y^i\\)\n\nThe residuals are the \\(r_i\\)\nThis plot compares the residual to the magnitude of the fitted-value. No discernable pattern in this plot is desirable.\n\nQ-Q Plot of the Residuals\nThe normality of the error terms can be assessed by considering a normal probability plot (Q-Q Plot) of the residuals. If the residuals appear to be normal, then the error terms are also considered to be normal. If the residuals do not appear to be normal, then the error terms are also assumed to violate the normality assumption.\nResiduals versus Order Plot\nWhen the data is collected in a specific order, or has some other important ordering to it, then the independence of the error terms can be assessed. This is typically done by plotting the residuals against their order of occurrance. If any dramatic trends are visible in the plot, then the independence assumption is violated.\n\npar(mfrow=c(1,3)) #Residuals versus Fitted-values Plot: Checks Assumptions #1 and #3\n\nplot(mylm,which=1:2) #Q-Q Plot of the Residuals: Checks Assumption #2\n\nplot(mylm$residuals) #Residuals versus Order Plot: Checks Assumption #5"
  },
  {
    "objectID": "i_univartests.html#null-and-alternative-hypotheses",
    "href": "i_univartests.html#null-and-alternative-hypotheses",
    "title": "3  Univariate Hypothesis Testing",
    "section": "3.1 Null and Alternative Hypotheses",
    "text": "3.1 Null and Alternative Hypotheses\nA null hypothesis and alternative hypothesis are fundamental concepts in hypothesis testing, a key aspect of inferential statistics. Hypothesis testing is a statistical method used to test the validity of a claim or assertion based on sample data, with the goal of making inferences about a population.\nNull Hypothesis\nThe null hypothesis (denoted as \\(H_0\\)) is a statement of no effect or no difference. It represents the default assumption that there is no relationship between the variables under investigation or that the treatment has no effect. The null hypothesis typically asserts that the observations are the result of chance or that the observed differences can be attributed to random variation.\nAlternative Hypothesis\nThe alternative hypothesis (denoted as \\(H_a\\)) is a statement that contradicts the null hypothesis. It represents the claim that there is a relationship between the variables or that the treatment has an effect. The alternative hypothesis is what researchers hope to provide evidence for through the process of hypothesis testing."
  },
  {
    "objectID": "i_univartests.html#null-hypothesis-significance-testing",
    "href": "i_univartests.html#null-hypothesis-significance-testing",
    "title": "3  Univariate Hypothesis Testing",
    "section": "3.1 Null Hypothesis Significance Testing",
    "text": "3.1 Null Hypothesis Significance Testing\nThe central dogma of statistics is to make estimates about the population based on samples. Null Hypothesis Significance Testing (NHST) is a widely used statistical approach for testing the validity of a claim or hypothesis about a population based on sample data. The process involves comparing the observed data to what would be expected under a null hypothesis, which usually assumes no effect or relationship between variables.\nNull Hypothesis\nThe null hypothesis (denoted as \\(H_0\\)) is usually a statement of no effect or no difference. It represents the default assumption that there is no relationship between the variables under investigation or that the treatment has no effect.\nAlternative Hypothesis\nThe alternative hypothesis (denoted as \\(H_a\\)) is a statement that contradicts the null hypothesis. It represents the claim that there is a relationship between the variables or that the treatment has an effect. The alternative hypothesis is what researchers hope to provide evidence for through the process of hypothesis testing.\n\n3.1.1 The Process of Null Hypothesis Significance Testing\n\nState the null hypothesis (\\(H_0\\)) and alternative hypothesis (\\(H_a\\)). The null hypothesis is a statement of no effect or relationship between the variables being tested, while the alternative hypothesis is a statement that claims some effect or relationship between the variables.\nChoose a significance level. The significance level, denoted by \\(\\alpha\\), is the probability of rejecting the null hypothesis when it is true. The most commonly used significance level is 0.05\nCollect data and calculate the test statistic. Gather your sample data and perform the appropriate statistical test to calculate the test statistic. The test statistic is a numerical value that measures the difference between the observed data and what would be expected under the null hypothesis. Different tests use different test statistics meaning each one is denoted differently dependent on which test. Common test statistics include \\(z\\), \\(t\\), \\(F\\), or \\(\\chi^2\\).\nDetermine the p-value. The p-value, denoted as \\(p\\), is the probability of observing a test statistic as extreme or more extreme than the one calculated from the sample data, assuming the null hypothesis is true. It quantifies the evidence against the null hypothesis. Lower p-values indicate stronger evidence against the null hypothesis since they imply that the observed results are less likely to have occurred by chance alone under the null hypothesis. In other words, a smaller p-value suggests that the observed effect or relationship between the variables is more likely to be genuine and not just a random occurrence. Conversely, higher p-values indicate weaker evidence against the null hypothesis. This means that the observed results are more likely to have occurred by chance under the null hypothesis, and there is insufficient evidence to suggest that the effect or relationship is genuine.\nCompare the p-value to the significance level (\\(\\alpha\\)). If the p-value is less than or equal to the chosen significance level (\\(p \\leq α\\)), then reject the null hypothesis in favor of the alternative hypothesis. This implies that there is statistically significant evidence to support the claim made by the alternative hypothesis. If the p-value is greater than the significance level (\\(p > α\\)), then do not reject the null hypothesis, as there is not enough evidence to support the alternative hypothesis\nInterpret the results and draw conclusions. Based on the comparison between the p-value and the significance level, make a conclusion about the null hypothesis. If you rejected the null hypothesis, this suggests the alternative hypothesis is supported by the data. If you failed to reject the null hypothesis, this means there is insufficient evidence to support the alternative hypothesis. Keep in mind that failing to reject the null hypothesis does not prove it is true; it simply means that the data does not provide strong evidence against it.\n\nIt is crucial to note that failing to reject the null hypothesis does not prove it is true; rather, it indicates that there is not enough evidence to conclude that the alternative hypothesis is true. Additionally, the p-value is subject to various limitations and potential misinterpretations, so it is essential to consider effect sizes, confidence intervals, and other aspects of the data when making conclusions from hypothesis testing."
  },
  {
    "objectID": "i_univartests.html",
    "href": "i_univartests.html",
    "title": "3  Univariate Hypothesis Testing",
    "section": "",
    "text": "4 Z-tests\nBackground\nImagine a\nStep One\nLet \\(\\mu_1\\) represent the mean height of the American male population, and \\(\\mu_2\\) represent the mean height of male NBA players. The null hypothesis (\\(H_0\\)) and alternative hypothesis (\\(H_a\\)) are as follows:\n\\[\n\\begin{align*}\nH_0 &: \\mu_1 = \\mu_2 \\\\\nH_a &: \\mu_1 \\neq \\mu_2\n\\end{align*}\n\\]\nIn other words, the null or status quo hypothesis states that there is no difference in heights between the American male population and NBA players. The alternative hypothesis states that NBA players could be taller or shorter than the American male average.\nStep Two\nI will choose a significance level of 0.05. In other words, If we assumed our null hypothesis were true and the sample we obtain would have been so extreme that we would have only have had a 5 percent chance or less of getting it, we will reject the null hypothesis.\nStep Three\nSince we know the population variance for height, we will use a z-test.\nOur collected data is below. It is a set of univariate data about height of NBA players. The population standard deviation is 2.75 inches for adult men in the US.\nA z-test is a statistical test used to determine whether there is a significant difference between a sample mean and a population mean or between two population means, assuming that the population standard deviation is known. The z-test is based on the standard normal distribution (z-distribution), and it is used when the sample size is large (typically n ≥ 30) or when the population is normally distributed.\nThe Data"
  },
  {
    "objectID": "i_references.html",
    "href": "i_references.html",
    "title": "References",
    "section": "",
    "text": "Stevens, S. S. 1946. “On the Theory of Scales of\nMeasurement.” Science 103 (2684): 677–80. https://doi.org/10.1126/science.103.2684.677.\n\n\nWarne, R. T. 2020. Statistics for the Social Sciences: A General\nLinear Model Approach. Cambridge University Press. https://books.google.com/books?id=Lw0FEAAAQBAJ."
  },
  {
    "objectID": "i_bivartests.html#null-hypothesis-significance-testing",
    "href": "i_bivartests.html#null-hypothesis-significance-testing",
    "title": "5  Regression Hypothesis Testing",
    "section": "5.1 Null Hypothesis Significance Testing",
    "text": "5.1 Null Hypothesis Significance Testing\nThe central dogma of statistics is to make estimates about the population based on samples. Null Hypothesis Significance Testing (NHST) is a widely used statistical approach for testing the validity of a claim or hypothesis about a population based on sample data. The process involves comparing the observed data to what would be expected under a null hypothesis, which usually assumes no effect or relationship between variables.\nNull Hypothesis\nThe null hypothesis (denoted as \\(H_0\\)) is a statement of no effect or no difference. In linear regression, it represents the default assumption like that there is no relationship between the variables under investigation or that the dependent variable is zero when the observed independent variable is zero.\nAlternative Hypothesis\nThe alternative hypothesis (denoted as \\(H_a\\)) is a statement that contradicts the null hypothesis. In linear regression, it represents the possibility that there is a relation between the variables like that there is a change in the dependent variable when the independent variable is observed to increase or that the dependent variable is not zero when the observed independent variable is zero.\nHypotheses for Simple Linear Regression\nThe pair of hypotheses stated above about the slope and intercept can be formally written as so below:\n\\[\n\\left.\\begin{array}{ll}\nH_0: \\beta_1 = 0 \\\\  \nH_a: \\beta_1 \\neq 0\n\\end{array}\n\\right\\} \\ \\text{Slope Hypotheses}\\\\\\;\\\\\\;\n\\left.\\begin{array}{ll}\nH_0: \\beta_0 = 0 \\\\  \nH_a: \\beta_0 \\neq 0\n\\end{array}\n\\right\\} \\ \\text{Intercept Hypotheses}\n\\]\n\n5.1.1 The Process of Null Hypothesis Significance Testing\n\nState the null hypothesis (\\(H_0\\)) and alternative hypothesis (\\(H_a\\)). Since there are two estimated parameters, two pairs of hypotheses are made.\nChoose a significance level. The significance level, denoted by \\(\\alpha\\), is the probability of rejecting the null hypothesis when it is true. The most commonly used significance level is 0.05\nCollect data and calculate the test statistic. Gather your sample data and perform the appropriate statistical test to calculate the test statistic. The test statistic is a numerical value that measures the difference between the observed data and what would be expected under the null hypothesis. The Central Limit Theorem says that the distribution of sample estimates for \\(\\beta_0\\) and \\(\\beta_1\\) will be normally distributed. Since the variance for this distribution is uncertain, we use a t-test and thus a t-statistic for each hypothesis.\nDetermine the p-value. The p-value, denoted as \\(p\\), is the probability of observing a test statistic as extreme or more extreme than the one calculated from the sample data, assuming the null hypothesis is true. It quantifies the evidence against the null hypothesis. Lower p-values indicate stronger evidence against the null hypothesis since they imply that the observed results are less likely to have occurred by chance alone under the null hypothesis. In other words, a smaller p-value suggests that the observed effect or relationship between the variables is more likely to be genuine and not just a random occurrence. Conversely, higher p-values indicate weaker evidence against the null hypothesis. This means that the observed results are more likely to have occurred by chance under the null hypothesis, and there is insufficient evidence to suggest that the effect or relationship is genuine.\nCompare the p-value to the significance level (\\(\\alpha\\)). If the p-value is less than or equal to the chosen significance level (\\(p \\leq \\alpha\\)), then reject the null hypothesis in favor of the alternative hypothesis. This implies that there is statistically significant evidence to support the claim made by the alternative hypothesis. If the p-value is greater than the significance level (\\(p > \\alpha\\)), then do not reject the null hypothesis, as there is not enough evidence to support the alternative hypothesis"
  },
  {
    "objectID": "i_bivartests.html#testing-the-intercept-and-slope",
    "href": "i_bivartests.html#testing-the-intercept-and-slope",
    "title": "5  Regression Hypothesis Testing",
    "section": "5.2 Testing the Intercept and Slope",
    "text": "5.2 Testing the Intercept and Slope\nBelow is a sample of 32 cars from the early 20th century. Each point represents a car where the X axis shows the independent variable (car weight) and the Y axis shows gross horsepower.\n\n\nCode\nlibrary(tidyverse)\nlibrary(plotly)\n\n(ggplot(mtcars, aes(x = wt, y = hp)) +\n  geom_point(color = 'darkblue',\n               fill='lightblue',\n               size=2.5,\n               shape=21,\n               alpha=.9) +\n  theme_classic() +\n  geom_smooth(method = \"lm\", se = FALSE) \n  ) %>% \n  ggplotly()\n\n\n\n\n\n\n\n5.2.1 Our Measly Sample\nThere are more than 32 cars out there in the world. See below; our sample of 32 cars happens to show the Y-intercept being -1.821 when we try to model weight and horsepower and an increase of 46.16 horsepower for every increase of 1000 pounds. Suppose we sampled a different set of cars and found that the intercept was actually 5 horsepower for a 0 pound car… or -3 horsepower… or 100 horsepower for a zero pound car. Similarly, for the slope, maybe if we drew another sample of cars- we would get a different slope. We could get a nearly endless amount of combinations depending on what our sample was.\n\n\nCode\nlm(hp~wt, data=mtcars) %>% pander::pander()\n\n\n\nFitting linear model: hp ~ wt\n\n\n\n\n\n\n\n\n\n \nEstimate\nStd. Error\nt value\nPr(>|t|)\n\n\n\n\n(Intercept)\n-1.821\n32.32\n-0.05633\n0.9555\n\n\nwt\n46.16\n9.625\n4.796\n4.146e-05\n\n\n\n\n\nWhat would happen if we kept collecting samples from cars and checking each sample’s Y-intercept and slope? We would get a distribution of samples.\nA sampling distribution\nThe sampling distribution of the y-intercept (\\(b_0\\)) would be the collection of all possible estimates of the y-intercept obtained from different samples of the same population. Similarly, the sampling distribution of the slope (\\(b_1\\)) would be the collection of all possible estimates of the slope obtained from different samples of the same population.\n\n\n5.2.2 The Central Limit Theorem\nIn the above section, we mentioned the existence of a sampling distribution for each estimated parameter. The Central Limit Theorem (CLT) is a fundamental concept in probability and statistics that states that, given a large enough sample size, the distribution of the sample parameter, in this case, potential Y-intercepts or slopes of a random variable will approach a normal distribution, regardless of the shape of the underlying population distribution. This theorem is crucial because many statistical methods, including linear regression, rely on the assumption that the data are normally distributed.\nY-intercept (\\(\\beta_0\\))\nThe sampling distribution of the y-intercept (\\(\\beta_0\\)) is the collection of all possible estimates of the y-intercept estimates (\\(b_0\\)) obtained from different samples of the same population. It centers around the unknown true Y intercept parameter and has an unknown spread. It describes the probability distribution of the estimates.\n\n\n\n\n\nSlope (\\(\\beta_1\\))\nThe sampling distribution of the slopes (\\(\\beta_1\\)) is the collection of all possible estimates of the slope estimates (\\(b_0\\)) obtained from different samples of the same population. It centers around the unknown true slope parameter and has an unknown spread. It describes the probability distribution of the estimates. It describes the probability distribution of the estimates and provides insight into the variability of the estimates.\n\n\n\n\n\nVisualizing the Central Limit Theorem\nBelow is an applet that allows you to draw a parent population distribution and see what different\n\n\n\n\n\n\n\n5.2.3 Estimating the Distribution of Sample Intercepts Under the Null Hypothesis\nY-intercept (\\(\\beta_0\\))\nUnder the null hypothesis, we would assume the null hypothesis is \\(H_0:\\beta_0\\). What we don’t know is \\(\\sigma_{\\beta_0}\\). We can estimate this parameter with \\(s_{b_0}\\).\n\\[\n\\begin{align*}\ns_{b_0} &= \\sqrt{\\text{MSE} \\left(\\frac{1}{n} + \\frac{\\bar{X}^2}{\\text{SSTO}(X)}\\right)}\\\\\n&=\\sqrt{\\left(\\frac{\\text{MSE}}{n}\\right) +\\left(\\frac{\\text{MSE}}{\\text{SSTO}(X)}\\bar{X}\\right)}\\\\\n\\end{align*}\n\\]\nAdditionally, this formula can be thought of as so:\n\\(s_{b_0}\\) = \\(\\sqrt{} (\\) Constant variation \\(+\\) Variation caused since data is centered away from the intercept.\\()\\)\nConceptually, the formula above computes the standard error of the estimated intercept (\\(\\beta_{0}\\)), which measures the uncertainty or variability associated with the intercept estimate. In other words, it quantifies how precise the estimate is and how much it is expected to vary from one sample to another.\n\n\\(\\text{MSE}\\): also called \\(\\sigma^2\\). This term represents the variance of the residuals (estimated errors) in the regression model. It reflects the degree of dispersion in the observed values around the predicted values. When you multiply with the rest of the formula, you are essentially scaling the standard error of the intercept according to the overall uncertainty or variability in the model’s predictions. A larger variance in residuals (\\(\\text{MSE}\\)) will result in a larger standard error for the intercept, indicating a higher degree of uncertainty in the intercept estimate.\n\\(n\\): This is the number of observations in the dataset.\n\\(\\frac{1}{n}\\) This term reflects the effect of sample size on the precision of the estimated intercept. As the sample size (\\(n\\)) increases, the standard error of the intercept will generally decrease, which means that the intercept estimate becomes more precise with more observations.\n\\(\\bar{X}\\): This is the mean of the predictor (independent) variable values.\n\\(\\text{SSTO}(X)\\): This is the sum of the squared differences between each predictor variable value (\\(X_i\\)) and the mean predictor variable value (̄\\(\\bar{X}\\)).It can also be written as so: \\(\\sum_{i=1}^{n}{\\left(X_i - \\bar{X}\\right)^2}\\) Geometrically, this term represents the sum of the squared horizontal distances between each data point and the vertical line that passes through the mean of the predictor variable values.\n\\(\\frac{\\bar{X}^2}{\\text{SSTO}(X)}\\) This term captures the influence of the distribution of the predictor variable values on the precision of the estimated intercept. The denominator (\\(\\text{SSTO}(X)\\)) measures the spread of the predictor variable values around their mean (\\(\\bar{X}\\)̄). A larger spread in the predictor variable values will generally result in a smaller standard error for the intercept, assuming that the variance of the residuals (\\(\\sigma^2\\)) remains constant.\n\nSince we have our \\(\\beta_0\\) and our \\(s_{b_0}\\), we can estimate the sampling distribution. We will use a t-distribution with \\(n-2\\) degrees of freedom since the spread is estimated:\n\n\n\n\n\nSlope (\\(\\beta_1\\))\nUnder the null hypothesis, we would assume the null hypothesis is \\(H_0:\\beta_1\\). What we don’t know is \\(\\sigma_{\\beta_1}\\). We can estimate this parameter with \\(s_{b_1}\\).\n\\[\n\\begin{align*}\ns_{b_1} &= \\sqrt{\\frac{MSE}{\\text{SSTO}(X)}}\\\\\n&= \\sqrt{\\frac{MSE}{\\sum(X_i-\\bar{X})^2}}\\\\\n\\end{align*}\n\\]\n\n\\(\\text{MSE}\\): also called \\(\\sigma^2\\). This term represents the variance of the residuals (estimated errors) in the regression model. It reflects the degree of dispersion in the observed values around the predicted values. A larger variance in residuals (\\(\\text{MSE}\\)) will result in a larger standard error for the intercept, indicating a higher degree of uncertainty in the intercept estimate.\n\\(\\text{SSTO}(X)\\): This is the sum of the squared differences between each predictor variable value (\\(X_i\\)) and the mean predictor variable value (̄\\(\\bar{X}\\)).It can also be written as so: \\(\\sum_{i=1}^{n}{\\left(X_i - \\bar{X}\\right)^2}\\) Geometrically, this term represents the sum of the squared horizontal distances between each data point and the vertical line that passes through the mean of the predictor variable values.\n\nThis formula gets the square root of the ratio of the squared residuals and the total sum of squares. Since we have our \\(\\beta_1\\) and our \\(s_{b_1}\\), we can estimate the sampling distribution. We will use a t-distribution with \\(n-2\\) degrees of freedom since the spread is estimated:\n\n\n\n\n\n\n\n5.2.4 Calculating our Test-Statistic Under the Null Hypothesis\nY-intercept (\\(\\beta_0\\))\nThis is the formula for the test statistic for \\(\\beta_0\\). It measures how far the estimated Y-intercept \\(b_0\\) is from the null hypothesis for \\(\\beta_0\\) in units of standard errors (\\(s_{b_0}\\)) of \\(b_0\\)- thus the division by \\(s_{b_0}\\). Though the hypothesized value of \\(\\beta_0\\) is typically 0, it could be any number.\n\\[\nt = \\frac{b_0 - (H_0:\\beta_0)}{s_{b_0}}\n\\]\nSlope (\\(\\beta_1\\))\nThis is the formula for the test statistic for \\(\\beta_1\\). It measures how far the estimated slope \\(b_1\\) is from the null hypothesis for \\(\\beta_1\\) in units of standard errors (\\(s_{b_1}\\)) of \\(b_1\\)- thus the division by \\(s_{b_1}\\). Though the hypothesized value of \\(\\beta_0\\) is typically 0 (no relationship), it could be any number.\n\\[\nt = \\frac{b_1 - (H_:\\beta_1)}{s_{b_1}}\n\\]\n\n\n5.2.5 Calculating our P-value Under the Null Hypothesis\nThe p-value is the probability of obtaining a test statistic as extreme or more extreme than the observed value, assuming the null hypothesis is true. We can mathematically calculate this by using the t-distribution\nLet \\(T \\sim \\mathcal{t}_{(n-2)}\\) be a t-distributed random variable with \\(n-2\\) degrees of freedom and \\(t\\) be the observed t-score. Given this observed t-score, we can then compute the p-value, which is the probability of observing a t-score as extreme as \\(t\\) under the null hypothesis.\nY-intercept (\\(\\beta_0\\))\n\nFor a two-tailed t-test (where we’re testing for a Y-intercept greater or less than what we are hypothesizing): When \\(H_a: \\beta_0 \\neq 0\\), then \\(p = 2 \\times (1 - \\text{Pr}(T \\leq |t|))\\)\nFor a right-tailed t-test (where we’re testing for a Y-intercept greater than what we are hypothesizing): When \\(H_a: \\beta_0 > 0\\), then \\(p = 1 - \\text{Pr}(T \\leq t)\\)\nFor a left-tailed t-test (where we’re testing for a Y-intercept less than what we are hypothesizing): When \\(H_a: \\beta_0 < 0\\), then \\(p = \\text{Pr}(T \\leq t)\\)\n\nSlope (\\(\\beta_1\\))\n\nFor a two-tailed t-test (where we’re testing for a slope greater or less than what we are hypothesizing): When \\(H_a: \\beta_1 \\neq 0\\), then \\(p = 2 \\times (1 - \\text{Pr}(T \\leq |t|))\\)\nFor a right-tailed t-test (where we’re testing for a slope greater than what we are hypothesizing): When \\(H_a: \\beta_1 > 0\\), then \\(p = 1 - \\text{Pr}(T \\leq t)\\)\nFor a left-tailed t-test (where we’re testing for a slope less than what we are hypothesizing): When \\(H_a: \\beta_1 < 0\\), then \\(p = \\text{Pr}(T \\leq t)\\)\n\nWhere \\(t\\) is the t-statistic from the sample and \\(t_{n-2}\\) is the t-distribution with \\(n-2\\) degrees of freedom with a mean of \\(H_0:\\beta_0\\) or \\(H_0:\\beta_1\\) and a standard deviation of \\(s_{b_0}\\) or \\(s_{b_1}\\). This can be illustrated below."
  },
  {
    "objectID": "i_bivartests.html#simulating-the-nhst-process",
    "href": "i_bivartests.html#simulating-the-nhst-process",
    "title": "5  Regression Hypothesis Testing",
    "section": "5.3 Simulating the NHST Process",
    "text": "5.3 Simulating the NHST Process\nBelow we set up hypothetical population parameters.\n\n5.3.1 Parameters\nPopulation parameters\n\n\\(X \\sim \\mathcal{N}(70, 3)\\). This will represent height of men.\n\\(\\beta_0 = -179\\). This would represent the weight of a person who is 0 inches tall.\n\\(\\beta_1 = 5.3\\). This represents how much additional weight someone will be when they are 1 inch taller.\n\\(\\sigma = 38\\). This represents the variance of the data around the true line.\n\nSimulation variables\n\n\\(N = 700\\). This represents that we are taking 700 simple random samples from the population.\n\\(n = 100\\). This represents that for every sample, we are observing 100 people.\n\n\n\nCode\nn = 100         #sample size\nbeta_0 = -179   #true y-intercept\nbeta_1 = 5.3    #true slope\nsigma = 38      #choice of st. deviation of error terms\n\nX = rnorm(n, mean = 70, sd = 3)\nN <- 1000 #number of samples in our sampling distribution\n\n\n\n\n5.3.2 Step One: State our pair of null and alternative hypotheses.\nWe are going under the assumption we do not know the population parameters. Because of this, we are going to formulate hypotheses about the relationship between height (our independent variable) and weight (our dependent variable). The first pair of hypotheses state that when American male height is 0, weight should be 0 as well- The second pair of hypotheses state that when American male height increases by 1 inch, there is no increase in weight.\n\\[\n\\left.\\begin{array}{ll}\nH_0: \\beta_1 = 0 \\\\  \nH_a: \\beta_1 \\neq 0\n\\end{array}\n\\right\\} \\ \\text{Slope Hypotheses}\\\\\\;\\\\\\;\n\\left.\\begin{array}{ll}\nH_0: \\beta_0 = 0 \\\\  \nH_a: \\beta_0 \\neq 0\n\\end{array}\n\\right\\} \\ \\text{Intercept Hypotheses}\n\\]\n\n\n5.3.3 Step Two: Choose a significance level\nWe are going to go with the traditional \\(\\alpha = 0.05\\).\n\n\nCode\nalpha = 0.05\n\n\n\n\n5.3.4 Step Three- Part 1: Collect data\nFor this mock study, we are going to “measure” the heights and weights of 100 random men in the United States.\nFirst few rows of the mock sample data\n\n\nCode\nX_sample <- rnorm(n, mean = 70, sd = 3)\nY_sample <- beta_0 + beta_1*X_sample + rnorm(n, 0, sigma)\n\nsample = tibble(X_sample, Y_sample)\n\nsample %>% head() %>% pander::pander()\n\n\n\n\n\n\n\n\n\nX_sample\nY_sample\n\n\n\n\n72.38\n183.1\n\n\n68.46\n178.4\n\n\n70.42\n182.2\n\n\n70.71\n202.7\n\n\n69.42\n240.8\n\n\n71.18\n103.8\n\n\n\n\n\nThe mock sample data\n\n\nCode\n(ggplot(sample, aes(X_sample,Y_sample)) + \n  geom_point(color = 'darkblue',\n               fill='lightblue',\n               size=2.5,\n               shape=21,\n               alpha=.9) + \n  geom_smooth(se=F, method = 'lm') +\n  theme_classic() +\n  labs(\n    x = \"Sample Height\",\n    y = \"Sample Weight\",\n    title = \"Weight by height among American men\"\n  )) %>% \n  ggplotly()\n\n\n\n\n\n\nThe summary of a linear model on the single mock data\n\n\nCode\nmylm = lm(Y_sample~X_sample, data=sample)\nb = mylm$coef \ntibble(Intercept = b[1], Slope = b[2]) %>% pander::pander()\n\n\n\n\n\n\n\n\n\nIntercept\nSlope\n\n\n\n\n-51.36\n3.396\n\n\n\n\n\nCode\nb0 = b[1]\nb1 = b[2]\n\n\n\n\n5.3.5 Step Three- Part 2: Calculate the test statistic\nWe are going to estimate the distribution of all possible sample test statistics that someone could get from the population distribution assuming the null hypothesis is true. This allows us to see, under the null hypothesis, see how likely we are to get a sample as or more extreme than what we would ideally expect.\nCalculating \\(s_{b_0}\\) and \\(s_{b_1}\\)\n\n\nCode\nn = 100\nmse = sum((sample$Y_sample - mylm$fitted.values)^2) / n\n\ns_b0 = sqrt(mse*((1 / n) + (mean(sample$X_sample)^2 / sum((sample$X_sample - mean(sample$X_sample))^2))))\n\ns_b1 = sqrt(mse/sum((sample$X_sample - mean(sample$X_sample))^2))\n\npander::pander(paste(\"$s_{b_0}$ is\",s_b0 %>% round(2), \"and $s_{b_1}$ is\",s_b1 %>% round(2)))\n\n\n\\(s_{b_0}\\) is 82.17 and \\(s_{b_1}\\) is 1.18\n\n\nCalculating the t-value for \\(\\beta_0\\) and \\(\\beta_1\\)\n\n\nCode\nt_b0 = (b0 - 0)/s_b0\nt_b1 = (b1 - 0)/s_b1\n\npander::pander(paste(\"the t-value for $b_0$ is\",t_b0 %>% round(2), \"the t-value for $b_1$ is\",t_b1 %>% round(2)))\n\n\nthe t-value for \\(b_0\\) is -0.63 the t-value for \\(b_1\\) is 2.88\n\n\n\n\n5.3.6 Step Four: Determine the p-value\nCalculate the p-value for the Y intercept hypothesis and the slope hypothesis\n\n\nCode\np_b0 = pt(-abs(t_b0), n-2)*2\n\np_b1 = pt(-abs(t_b1), n-2)*2\n\npander::pander(paste(\"the p-value for $b_0$ is\",p_b0 %>% round(2), \"the p-value for $b_1$ is\",p_b1 %>% round(2)))\n\n\nthe p-value for \\(b_0\\) is 0.53 the p-value for \\(b_1\\) is 0\n\n\n\n\n5.3.7 Step Five: Compare the p-value to the significance level\n\n\nCode\nreject_null_b0 = (p_b0 <= alpha)\nreject_null_b1 = (p_b1 <= alpha)\n\npander::pander(paste(\"The P-value for the Y-intercept hypothesis is\",p_b0 %>% round(6),\".\"))\n\n\nThe P-value for the Y-intercept hypothesis is 0.533392 .\n\n\nCode\npander::pander(paste(\"Since it is\",reject_null_b0 %>% tolower(),\"that (p<a), we\",ifelse(reject_null_b0,\"\",\"fail to\"),\"reject the null hypothesis\"))\n\n\nSince it is false that (p<a), we fail to reject the null hypothesis\n\n\nCode\npander::pander(paste(\"The P-value for the slope hypothesis is\",p_b1 %>% round(6),\".\"))\n\n\nThe P-value for the slope hypothesis is 0.004848 .\n\n\nCode\npander::pander(paste(\"Since it is\",reject_null_b1 %>% tolower(),\"that (p<a), we\",ifelse(reject_null_b1,\"\",\"fail to\"),\"reject the null hypothesis\"))\n\n\nSince it is true that (p<a), we reject the null hypothesis\n\n\nHere is the R calculated code from the sample.\n\n\nCode\nmylm %>% summary %>% pander::pander()\n\n\n\n\n\n\n\n\n\n\n\n\n \nEstimate\nStd. Error\nt value\nPr(>|t|)\n\n\n\n\n(Intercept)\n-51.36\n83\n-0.6188\n0.5375\n\n\nX_sample\n3.396\n1.19\n2.854\n0.005275\n\n\n\n\nFitting linear model: Y_sample ~ X_sample\n\n\n\n\n\n\n\n\nObservations\nResidual Std. Error\n\\(R^2\\)\nAdjusted \\(R^2\\)\n\n\n\n\n100\n37.41\n0.07672\n0.06729\n\n\n\n\n\nThe probability of picking a sample as extreme or more extreme given the null hypothesis is visualized below as the area under the curve except for the middle section.\n\n\nCode\npar(mfrow=c(2,1))\ncurve(dt(x, df=n-1), from=-10, to=10, main=\"Null hypothesis distribution with the\\nprobability of the intercept sample result\", ylab=\"\", xlab=\"Possible t-values\", yaxt=\"n\")\nabline(v=c(abs(t_b0), -abs(t_b0)))\ncurve(dt(x, df=n-1), from=-10, to=10, main=\"Null hypothesis distribution with the\\nprobability of the slope sample result\", ylab=\"\", xlab=\"Possible t-values\", yaxt=\"n\")\nabline(v=c(abs(t_b1), -abs(t_b1)))\n\n\n\n\n\n\n\n5.3.8 Step Three and Four through simulation\nIn the steps above, we estimate what it would be like if we were to take many samples using an estimated sample distribution of both intercepts and slopes. Below, using the population parameters defined above, 1000 samples will be taken of Americans in which an actual sampling distribution will be constructed.\nResults from 1000 samples\nBelow is the result of overlaying every proposed line from each of the 1000 samples.\n\n\nCode\nsimulations <- tibble(\n  b0 = rep(NA, N),\n  b1 = rep(NA, N),\n  rmse = rep(NA, N)\n)\n\nfor (i in 1:N) {\n  Y <- beta_0 + beta_1*X + rnorm(n, 0, sigma) #Sample Y from true model\n  mylm <- lm(Y ~ X)\n  simulations$b0[i] <- coef(mylm)[1]\n  simulations$b1[i] <- coef(mylm)[2]\n  simulations$rmse[i] <- summary(mylm)$sigma\n}\n\ndata <- tibble(X = X, Y = Y)\n\n\n\n\nCode\n(ggplot(data, aes(X, Y)) +\n  geom_point(color = \"gray\") +\n  geom_abline(aes(intercept = b0, slope = b1), data = simulations, color = \"#5A5A5A\") +\n  geom_abline(intercept = beta_0, slope = beta_1, color = \"green\", size = 1) +\n  geom_abline(intercept = beta_0 + sigma, slope = beta_1, color = \"darkgreen\", size = 0.5) +\n  geom_abline(intercept = beta_0 - sigma, slope = beta_1, color = \"darkgreen\", size = 0.5) +\n  geom_abline(intercept = beta_0 + 2 * sigma, slope = beta_1, color = \"darkgreen\", size = 0.25) +\n  geom_abline(intercept = beta_0 - 2 * sigma, slope = beta_1, color = \"darkgreen\", size = 0.25) +\n  geom_abline(intercept = beta_0 + 3 * sigma, slope = beta_1, color = \"darkgreen\", size = 0.1) +\n  geom_abline(intercept = beta_0 - 3 * sigma, slope = beta_1, color = \"darkgreen\", size = 0.1) +\n  labs(title = \"Regression Lines from many Samples\\n Plus Residual Standard Deviation Lines\",\n       x = \"X\",\n       y = \"Y\") +\n  ggthemes::theme_stata())\n\n\n\n\n\nBelow is the histogram of the combined samples to make a sampling distribution of intercepts on the left and slopes on the right. Notice from the above graph and below distribution that the slope intercept has very large variation due to the increased variation since the data is centered away from the intercept.\n\n\nCode\nb0_samples = (ggplot(simulations, aes(x = b0)) +\n  geom_histogram(aes(y = ..density..), fill = \"skyblue3\", bins = 30) +\n  geom_density(color = \"green\", size = 1) +\n  labs(title = \"Sampling Distribution\\n Y-intercept\",\n       x = expression(paste(\"Estimates of \", beta[0], \" from each Sample\")),\n       y = \"Density\") +\n  ggthemes::theme_stata() +\n    theme(axis.text.y=element_blank(),\n          axis.ticks.y=element_blank()) +\n    geom_vline(xintercept = beta_0, color='purple'))\n\nb1_samples = (ggplot(simulations, aes(x = b1)) +\n  geom_histogram(aes(y = ..density..), fill = \"skyblue3\", bins = 30) +\n  geom_density(color = \"green\", size = 1) +\n  labs(title = \"Sampling Distribution\\n Slope\",\n       x = expression(paste(\"Estimates of \", beta[1], \" from each Sample\")),\n       y = \"Density\") +\n  ggthemes::theme_stata() +\n    theme(axis.text.y=element_blank(),\n          axis.ticks.y=element_blank()) +\n    geom_vline(xintercept = beta_1, color='purple'))\n\ngridExtra::grid.arrange(b0_samples , b1_samples, ncol = 2)\n\n\n\n\n\nResults from the sampling distribution\n\n\nCode\npander::pander(paste(\"The mean of the sampling distribution for the Y-intercept turned out to be\", simulations$b0 %>% mean %>% round(2),\". Our estimate was\",b0 %>% round(2)))\n\n\nThe mean of the sampling distribution for the Y-intercept turned out to be -175.32 . Our estimate was -51.36\n\n\nCode\npander::pander(paste(\"The mean of the sampling distribution for the slope turned out to be\", simulations$b1 %>% mean %>% round(2),\". Our estimate was\",b1 %>% round(2)))\n\n\nThe mean of the sampling distribution for the slope turned out to be 5.25 . Our estimate was 3.4\n\n\n\n\nCode\npander::pander(paste(\"The variance of the sampling distribution for the Y-intercept turned out to be\", simulations$b0 %>% sd %>% round(2),\". Our estimate was\",s_b0 %>% round(2)))\n\n\nThe variance of the sampling distribution for the Y-intercept turned out to be 98.98 . Our estimate was 82.17\n\n\nCode\npander::pander(paste(\"The variance of the sampling distribution for the slope turned out to be\", simulations$b1 %>% sd %>% round(2),\". Our estimate was\",s_b1 %>% round(2)))\n\n\nThe variance of the sampling distribution for the slope turned out to be 1.41 . Our estimate was 1.18"
  },
  {
    "objectID": "i_bivartests.html#setting-confidence-intervals",
    "href": "i_bivartests.html#setting-confidence-intervals",
    "title": "5  Regression Hypothesis Testing",
    "section": "5.4 Setting Confidence Intervals",
    "text": "5.4 Setting Confidence Intervals\nConfidence intervals, instead of exploiting the null hypothesis distribution to calculate uncertainty, use the sample distribution itself to calculate the likelihood that the true population parameter is withing the boundaries. With linear regression, confidence intervals are calculated for each coefficient of the regression model, \\(\\beta_0\\) (intercept) and \\(\\beta_1\\) (slope). The main idea is to provide a range in which the true population parameters are likely to lie, given the data and a specified level of confidence (usually 95%).\nThe formula to calculate the confidence interval is below.\n\\[\n\\text{CI}(\\beta_0) = b_0 \\pm t^*_{n-2}\\times s_{b_0} : \\alpha=0.05\n\\]\n\\[\n\\text{CI}(\\beta_1) = b_1 \\pm t^*_{n-2}\\times s_{b_1} : \\alpha=0.05\n\\]\n\n\nCode\ntstar = qt(0.975, n-2)\n\nci_b0upper = b0 + tstar*s_b0\nci_b0lower = b0 - tstar*s_b0\n\nci_b1upper = b1 + tstar*s_b1\nci_b1lower = b1 - tstar*s_b1\n\n\nthe \\(t^*\\) is used to represent the t-value in which the area in between the negative and positive t-value on a t-distribution is .95.\nWhen we say that the confidence interval captures the true parameter values “95% of the time,” it means that if we were to repeat the sampling process many times and compute the confidence intervals for each sample, about 95% of those intervals would contain the true population parameter. Note that this doesn’t mean there’s a 95% probability that the true parameter lies within the specific confidence interval calculated from one sample; rather, it means that the procedure we use to construct the interval, when repeated across multiple samples, would produce intervals that contain the true parameter value in 95% of cases.\n\n5.4.1 Confidence Intervals for our simulation\nWe can directly visualize the 95 percent confidence interval with the visualization below. The purple line represents the true population parameter. The black line represents the sample estimation. The gray transparent box represents the 95 percent confidence interval.\nNote: The visualization below may be a bit misleading since the 95 percent confidence interval does not represent the sample below, but the percentage of all samples.\n\n\nCode\nb0_samples_conf = (ggplot(simulations, aes(x = b0)) +\n  geom_histogram(aes(y = ..density..), fill = \"skyblue3\", bins = 30) +\n  geom_rect(xmin = ci_b0lower, xmax = ci_b0upper, ymin=0, ymax=.3, alpha=.01) +\n  geom_density(color = \"green\", size = 1) +\n  labs(title = \"Sampling Distribution\\n Y-intercept\",\n       x = expression(paste(\"Estimates of \", beta[0], \" from each Sample\")),\n       y = \"Density\") +\n  ggthemes::theme_stata() +\n    theme(axis.text.y=element_blank(),\n          axis.ticks.y=element_blank()) +\n    geom_vline(xintercept = beta_0, color='purple') +\n    geom_vline(xintercept = b0, color='black', size=2)\n    )\n\nb1_samples_conf = (ggplot(simulations, aes(x = b1)) +\n  geom_histogram(aes(y = ..density..), fill = \"skyblue3\", bins = 30) +\n  geom_rect(xmin = ci_b1lower, xmax = ci_b1upper, ymin=0, ymax=.3, alpha=.01) +\n  geom_density(color = \"green\", size = 1) +\n  labs(title = \"Sampling Distribution\\n Slope\",\n       x = expression(paste(\"Estimates of \", beta[1], \" from each Sample\")),\n       y = \"Density\") +\n  ggthemes::theme_stata() +\n    theme(axis.text.y=element_blank(),\n          axis.ticks.y=element_blank()) +\n    geom_vline(xintercept = beta_1, color='purple') +\n    geom_vline(xintercept = b1, color='black', size=2))\n\ngridExtra::grid.arrange(b0_samples_conf , b1_samples_conf, ncol = 2)"
  },
  {
    "objectID": "i_distributions.html#the-normal-distribution",
    "href": "i_distributions.html#the-normal-distribution",
    "title": "2  Univariate Distributions",
    "section": "2.1 The Normal Distribution",
    "text": "2.1 The Normal Distribution\nA normal distribution, also known as a Gaussian distribution, is a continuous probability distribution that has a bell-shaped curve. The normal distribution is characterized by two parameters: the mean (\\(\\mu\\)) and the standard deviation (\\(\\sigma\\)). The following are the key properties of a normal density curve:\n\nSymmetry: The curve is symmetric around the mean.\nUnimodal: The curve has a single peak, known as the mode, which coincides with the mean and the median.\nAsymptotic: The tails of the curve extend infinitely in both directions, approaching but never touching the horizontal axis.\nTotal area: The total area under the curve is equal to 1, representing the probability of all possible outcomes."
  },
  {
    "objectID": "i_univartests.html#one-sample",
    "href": "i_univartests.html#one-sample",
    "title": "3  Univariate Hypothesis Testing",
    "section": "4.1 One-Sample",
    "text": "4.1 One-Sample\nOne-Sample Test A one-sample test is used when you have raw data and a population mean. These tests check if the mean of a sample significantly differs from the known population mean."
  },
  {
    "objectID": "i_univartests.html#two-sample",
    "href": "i_univartests.html#two-sample",
    "title": "3  Univariate Hypothesis Testing",
    "section": "4.2 Two-Sample",
    "text": "4.2 Two-Sample\nPaired Samples Test A paired samples test, also called a dependent samples test, is used when you have two related samples or pairs of samples. This test checks if the means of these two samples significantly differ.\nIndependent Samples Test An independent samples test is used when you have two independent samples and aim to compare their means. This test checks if the means of these two samples significantly differ."
  },
  {
    "objectID": "i_univartests.html#parametric-tests-for-comparing-means",
    "href": "i_univartests.html#parametric-tests-for-comparing-means",
    "title": "3  Univariate Hypothesis Testing",
    "section": "3.2 Parametric Tests for Comparing Means",
    "text": "3.2 Parametric Tests for Comparing Means\nParametric tests are statistical procedures that make specific assumptions about the parameters or the distribution of the populations from which the data is drawn. They are powerful and can provide more reliable results when the assumptions are met. The most common type of parametric test involves the comparison of means.\n\n3.2.1 One-Sample Test\nA one-sample test is used when you have raw data and a population mean. This kind of test aims to determine whether there is a statistically significant difference between the mean of the sample and the known population mean. One-sample tests are particularly useful when you want to test a specific hypothesis about a population mean.\n\nExample Usage: A researcher wants to determine if the average height of a specific type of tree in a national park differs from the known average height of that type of tree worldwide.\n\n\n\n3.2.2 Two-Sample Tests\nTwo-sample tests are utilized when we are interested in comparing the difference of means or the mean of the differences of two different groups or samples.\nPaired Samples Test\nA paired samples test, also known as a dependent samples test or paired test, is used when you have two related samples or pairs of samples. This test is commonly used in ‘before-and-after’ scenarios where the same group is tested twice, under different conditions. The paired samples test assesses if the means of these two samples significantly differ by comparing the mean of the differences.\n\nExample Usage: A researcher wants to assess the effect of a new teaching method on students’ performance by comparing test scores before and after the implementation of the new method.\n\nIndependent Samples Test\nAn independent samples test, also known as two sample test, is used when you have two independent samples and aim to compare their means. The samples are independent in the sense that the selection of individuals in one group does not influence the selection of individuals in the other group. This test investigates if the means of these two independent samples significantly differ by comparing the difference of the means.\n\nExample Usage: A researcher wants to compare the average heights of two different species of trees to determine if one species tends to be taller than the other."
  },
  {
    "objectID": "i_univartests.html#z-tests",
    "href": "i_univartests.html#z-tests",
    "title": "3  Univariate Hypothesis Testing",
    "section": "3.3 z-tests",
    "text": "3.3 z-tests\n\n3.3.1 One-Sample Test\nA one-sample z-test is used to compare the mean of a sample to a specified value. It is a parametric test that assumes the population is normally distributed and the population standard deviation is known. The test statistic for a one-sample z-test is the z-score, which is calculated using the sample mean, the population mean, and the population standard deviation.\n\\[\n\\sigma = \\text{known population standard deviation}\n\\]\nSet of Hypotheses: For a one-sample z-test, the null hypothesis (\\(H_0\\)) is that the population mean (\\(\\mu\\)) is equal to a specified value (\\(\\mu_0\\)). The alternative hypothesis (\\(H_a\\)) is that the population mean (\\(\\mu\\)) is not equal to the specified value (\\(\\mu_0\\)).\n\\[\n\\begin{align*}\nH_0: \\mu = \\mu_0\\\\ \\;\\;\\;\\;\\;\nH_a: \\mu \\neq \\mu_0\n\\end{align*}\n\\]\nSignificance Level:\nThe significance level (\\(\\alpha\\)) is the probability of rejecting the null hypothesis when it is true. We will use the most commonly used significance level, 0.05.\n\\[\n\\alpha = 0.05\n\\]\nMaking the Sampling Distribution\nImagine we took a million samples of the population. Each sample would have a slightly different sample mean. This is called a sampling distribution. This is useful since we can use the sampling distribution to calculate the probability of obtaining a sample mean as extreme or more extreme than what we would expect. Since sampling a population a million times is not feasible, we can use the Central Limit Theorem to create a sampling distribution of the mean. The Central Limit Theorem states that the sampling distribution of the mean will be normally distributed with a mean equal to the population mean and a standard deviation equal to the population standard deviation divided by the square root of the sample size. Thus the standard deviation of the sampling distribution of \\(\\bar{X}\\) would be equal to \\(\\sigma_{\\bar{X}}\\)\n\\[\n\\sigma_{\\bar{X}} = \\frac{\\sigma}{\\sqrt{n}}\n\\]\nUnder the null hypothesis, the sampling distribution of the mean would be centered at \\(\\mu_0\\), rather than the actual population mean, \\(\\mu\\). We do this because we are assuming that the null hypothesis is true. This is useful because we can see if it is probable that our sample would have come from a population with a mean equal to \\(\\mu_0\\)- meaning if our sample mean is significantly different from \\(\\mu_0\\), we have found something interesting.\nTo clarify, this null distribution is what the distribution of test statistics would look like if the null hypothesis were true. The null distribution is a normal distribution with a mean equal to \\(\\mu_0\\) and a standard deviation equal to \\(\\sigma_{\\bar{X}}\\).\nBelow is a visualization of the hypothesized null distribution\n\nOur Sample\nThe sample mean (\\(\\bar{X}\\)) is the average of all the values (\\(X_i\\)’s’) in the sample that we collected. It is calculated by summing up all the values in the sample and dividing by the sample size (\\(n\\)).\n\\[\n\\bar{X} = \\frac{\\sum_{i=1}^{n}{X_i}}{n}\n\\]\nOur Sample’s Test-Statistic:\nWe have just caluclated the sample mean, \\(\\bar{X}\\). We hypothesized that the population mean is equal to a specified value, \\(\\mu_0\\). We want to know if the sample mean is significantly different from the population mean. We can calculate our test-statistic, the z-score, to find out where on the sampling distribution our sample mean lies. The z-score is calculated using the sample mean, the population mean, and the population standard deviation.\n\\[\nz = \\frac{\\bar{X} - \\mu_0}{\\sigma_{\\bar{X}}}\n\\]\nCalculating the p-value In the context of a z-test, we use a standard normal distribution to derive a p-value. We begin by standardizing our test statistic (the sample mean) to a z-score using the population mean and standard deviation. This z-score can then be used to find the p-value of our test.\nLet \\(Z \\sim \\mathcal{N}(0,1)\\) be a standard normal random variable and \\(z\\) be the observed z-score.\nGiven this observed z-score, we can then compute the p-value, which is the probability of observing a z-score as extreme as \\(z\\) under the null hypothesis.\n\nFor a two-tailed z-test (where we’re testing for a difference in either direction): When \\(H_a: \\mu \\neq \\mu_0\\), then \\(p = 2 \\times (1 - \\text{Pr}(Z \\leq |z|))\\)\nFor a right-tailed z-test (where we’re testing for a difference in the positive direction): When \\(H_a: \\mu > \\mu_0\\), then \\(p = 1 - \\text{Pr}(Z \\leq z)\\)\nFor a left-tailed z-test (where we’re testing for a difference in the negative direction): When \\(H_a: \\mu < \\mu_0\\), then \\(p = \\text{Pr}(Z \\leq z)\\)\n\nApplet to calculate the p-value\n\n\n\n\n\n\n\n3.3.2 Paired-Samples Test\nA paired-samples z-test, also known as a dependent sample z-test, is used to compare the mean differences of two related groups to assess whether the mean difference between paired observations in the population is significantly different from a number (usually zero). This test assumes the differences between pairs are normally distributed and the population standard deviation of the differences is known. The test statistic for a paired-samples z-test is the z-score, which is calculated using the sample mean difference, the population mean difference, and the population standard deviation of the differences.\n\\[\n\\sigma_d = \\text{known population standard deviation of differences}\n\\]\nAdditionally, the population mean difference (\\(\\mu_d\\)) is calculated by taking the mean of all the differences between pairs in the population. This is done by subtracting the second value in each pair from the first value in each pair and then taking the mean of all the differences. \\[\n\\mu_d = \\frac{1}{N} \\sum_{i=1}^{N} d_i \\;\\;\\;\\text{where}\\;\\; d_i = X_{2i} - X_{1i}\n\\]\nWhere \\(d_i\\) is the list of differences between pairs for the whole population and \\(N\\) is the number of pairs in the population.\nSet of Hypotheses: For a paired-samples z-test, the null hypothesis (\\(H_0\\)) is that the population mean difference (\\(\\mu_d\\)) is equal to \\(\\delta_0\\), usually 0. The alternative hypothesis (\\(H_a\\)) is that the population mean difference is not equal to \\(\\delta_0\\).\n\\[\n\\begin{align*}\nH_0: \\mu_d = \\delta_0\\\\\nH_a: \\mu_d \\neq \\delta_0\n\\end{align*}\n\\]\nSignificance Level:\nThe significance level (\\(\\alpha\\)) is the probability of rejecting the null hypothesis when it is true. We will use the most commonly used significance level, 0.05.\n\\[\n\\alpha = 0.05\n\\]\nMaking the Sampling Distribution\nImagine we took a million samples of pairs from the population. Each sample of pairs would have a slightly different mean difference. This is the sampling distribution for a paired test. This is useful since we can use the sampling distribution to calculate the probability of obtaining a sample mean difference as extreme or more extreme than what we would expect. Since sampling a population a million times is not feasible, we can use the Central Limit Theorem to create a sampling distribution of the mean difference. The Central Limit Theorem states that the sampling distribution of the mean difference will be normally distributed with a mean equal to the population mean difference and a standard deviation equal to the population standard deviation of the differences divided by the square root of the number of pairs. Thus the standard deviation of the sampling distribution of \\(\\bar{d}\\) would be equal to \\(\\sigma_{\\bar{d}}\\)\n\\[\n\\sigma_{\\bar{d}} = \\frac{\\sigma_d}{\\sqrt{n}}\n\\]\nUnder the null hypothesis, the sampling distribution of the mean difference would be centered at \\(\\delta_0\\), rather than the actual population mean difference, \\(\\mu_d\\). We do this because we are assuming that the null hypothesis is true. This is useful because we can see if it is probable that our sample would have come from a population with a mean difference equal to \\(\\delta_0\\) - meaning if our sample mean difference is significantly different from \\(\\delta_0\\), we have found something interesting.\nTo clarify, this null distribution is what the distribution of test statistics would look like if the null hypothesis were true. The null distribution is a normal distribution with a mean equal to \\(\\delta_0\\) and a standard deviation equal to \\(\\sigma_{\\bar{d}}\\).\n\nOur Sample\nThe sample mean difference (\\(\\bar{d}\\)) is the average of all the differences (\\(d_i\\)’s) in the sample that we collected. It is calculated by summing up all the differences in the sample and dividing by the number of pairs (\\(n\\)).\n\\[\n\\bar{d} = \\frac{\\sum_{i=1}^{n}{d_i}}{n} \\;\\;\\;\\text{where}\\;\\; d_i = X_{2i} - X_{1i}\n\\]\nOur Sample’s Test-Statistic:\nWe have just calculated the sample mean difference, \\(\\bar{d}\\). We hypothesized that the population mean difference is equal to a \\(\\delta_0\\). Since want to know if the sample mean difference is significantly different from \\(\\delta_0\\), we can calculate our test-statistic, the z-score, to find out where on the sampling distribution our sample mean difference lies. The z-score is calculated using the sample mean difference, the population mean difference (\\(\\delta_0\\)), and the population standard deviation of differences.\n\\[\nz = \\frac{\\bar{d}-\\delta_0}{\\sigma_{\\bar{d}}}\n\\]\nCalculating the p-value In the context of a z-test, we use a standard normal distribution to derive a p-value. We begin by standardizing our findings (the sample mean difference) to our z-score test statistic using the population mean difference (\\(\\delta_0\\)), and the population standard deviation of differences. This z-score can then be used to find the p-value of our test.\nLet \\(Z \\sim \\mathcal{N}(0,1)\\) be a standard normal random variable and \\(z\\) be the observed z-score.\nGiven this observed z-score, we can then compute the p-value, which is the probability of observing a z-score as extreme as \\(z\\) under the null hypothesis.\n\nFor a two-tailed z-test (where we’re testing for a difference in either direction): When \\(H_a: \\mu_d \\neq \\delta_0\\), then \\(p = 2 \\times (1 - \\text{Pr}(Z \\leq |z|))\\)\nFor a right-tailed z-test (where we’re testing for a difference in the positive direction): When \\(H_a: \\mu_d > \\delta_0\\), then \\(p = 1 - \\text{Pr}(Z \\leq z)\\)\nFor a left-tailed z-test (where we’re testing for a difference in the negative direction): When \\(H_a: \\mu_d < \\delta_0\\), then \\(p = \\text{Pr}(Z \\leq z)\\)\n\nApplet to calculate the p-value\n\n\n\n\n\n\n\n3.3.3 Independent-Samples Test\nAn independent-samples z-test, also known as a two-sample z-test, is used to compare the means of two independent groups to assess whether there is a statistically significant difference between the two population means. This test assumes both populations are normally distributed, and the population standard deviations are known.\nEach group has its own population standard deviation:\n\\[\n\\sigma_1 = \\text{known population standard deviation of group 1}\n\\] \\[\n\\sigma_2 = \\text{known population standard deviation of group 2}\n\\]\nSet of Hypotheses: For an independent-samples z-test, the null hypothesis (\\(H_0\\)) is that the difference between the population means (\\(\\mu_1 - \\mu_2\\)) is equal to a specified value, usually zero. The alternative hypothesis (\\(H_a\\)) is that the difference between the population means is not equal to this specified value.\n\\[\n\\begin{align*}\nH_0: \\mu_1 - \\mu_2 = \\delta_0\\\\ \\;\\;\\;\\;\\;\\;\nH_a: \\mu_1 - \\mu_2 \\neq \\delta_0\n\\end{align*}\n\\]\nSignificance Level:\nThe significance level (\\(\\alpha\\)) is the probability of rejecting the null hypothesis when it is true. We will use the most commonly used significance level, 0.05.\n\\[\n\\alpha = 0.05\n\\]\nMaking the Sampling Distribution\nSimilar to the tests above, if we took a million samples of two independent groups from the population, each sample would have a slightly different difference in means. This is the sampling distribution for an independent-samples test. This is useful since we can use the sampling distribution to calculate the probability of obtaining a difference in means as extreme or more extreme than what we would expect. Since sampling a population a million times is not feasible, we can use the Central Limit Theorem to create a sampling distribution of the difference in means. The Central Limit Theorem states that the sampling distribution of the difference in means will be normally distributed with a mean equal to the difference in population means and a standard deviation equal to the square root of the sum of the squares of the population standard deviations divided by the square root of the sample sizes. Thus the standard deviation of the sampling distribution of \\(\\bar{X_1} - \\bar{X_2}\\) would be equal to \\(\\sigma_{\\bar{X_1} - \\bar{X_2}}\\)\nThe standard error of the difference in sample means can be calculated from the population standard deviations of the two groups and the sizes of the two groups (\\(n_1\\) and \\(n_2\\)). This is the standard deviation of the sampling distribution of the difference in sample means.\n\\[\n\\sigma_{\\bar{X_1} - \\bar{X_2}} = \\sqrt{\\left(\\frac{\\sigma_1^2}{n_1}\\right) + \\left(\\frac{\\sigma_2^2}{n_2}\\right)}\n\\]\nOur Sample\nThe sample means (\\(\\bar{X_1}\\) and \\(\\bar{X_2}\\)) are the averages of all the values in each group. They are calculated by summing up all the values in each group and dividing by the size of each group.\n\\[\n\\bar{X_1} = \\frac{\\sum_{i=1}^{n_1}{X_{1i}}}{n_1}\n\\]\n\\[\n\\bar{X_2} = \\frac{\\sum_{i=1}^{n_2}{X_{2i}}}{n_2}\n\\]\nOur Sample’s Test-Statistic:\nThe test-statistic, the z-score, is calculated using the difference of the sample means, the hypothesized difference of the population means (which we’ve assumed to be \\(\\delta_0\\)), and the standard error.\n\\[\nz = \\frac{(\\bar{X_1} - \\bar{X_2}) - \\delta_0}{\\sigma_{\\bar{X_1} - \\bar{X_2}}}\n\\]\nCalculating the p-value In the context of two independent samples z-test, we use a standard normal distribution to derive a p-value. We begin by standardizing our test statistic (the difference in sample means) to a z-score using the hypothesized difference in population means and the standard error. This z-score can then be used to find the p-value of our test.\nLet’s denote the z-score for this test as \\(z\\) and standard normal random variable as \\(Z \\sim \\mathcal{N}(0,1)\\). The z-score is calculated considering the difference between sample means, the hypothesized difference (often zero), and the standard error of the difference between means. Once we have the calculated z-score, we can use it to find the p-value of our test. The p-value, similar to the one-sample case, is the probability of observing a z-score as extreme as \\(z\\) under the null hypothesis, and its calculation depends on the alternative hypothesis:\n\nFor a two-tailed z-test (where we’re testing for a difference in either direction): When \\(H_a: \\mu_1 \\neq \\mu_2\\), then \\(p = 2 \\times (1 - \\text{Pr}(Z \\leq |z|))\\)\nFor a right-tailed z-test (where we’re testing for a difference in the positive direction): When \\(H_a: \\mu_1 > \\mu_2\\), then \\(p = 1 - \\text{Pr}(Z \\leq z)\\)\nFor a left-tailed z-test (where we’re testing for a difference in the negative direction): When \\(H_a: \\mu_1 < \\mu_2\\), then \\(p = \\text{Pr}(Z \\leq z)\\)\n\nApplet to calculate the p-value\n\n\n\n\n\nCohen’s d Effect Size\nWe often want to measure the size of the difference between our two groups, not just whether this difference is statistically significant. This is where effect size comes in.\nFor a two-sample z-test, we can calculate the effect size using Cohen’s d. It is defined as the difference between two means divided by a standard deviation for the data.\n\\[\nd = \\frac{(\\bar{X_1} - \\bar{X_2})}{\\sqrt{\\left(\\frac{\\sigma_1^2}{2}\\right) + \\left(\\frac{\\sigma_2^2}{2}\\right)}}\n\\]\nCohen’s d is an appropriate effect size for the comparison between two means."
  },
  {
    "objectID": "i_univartests.html#t-tests",
    "href": "i_univartests.html#t-tests",
    "title": "3  Univariate Hypothesis Testing",
    "section": "3.4 t-tests",
    "text": "3.4 t-tests\n\n3.4.1 One-Sample Test\nA one-sample t-test is used to compare the mean of a sample to a specified value. It is a parametric test that assumes the population is normally distributed and that the population standard deviation is unknown. The test statistic for a one-sample t-test is the t-score, which is calculated using the sample mean, the specified value, and the sample standard deviation.\nSet of Hypotheses: For a one-sample t-test, the null hypothesis (\\(H_0\\)) is that the population mean (\\(\\mu\\)) is equal to a specified value (\\(\\mu_0\\)). The alternative hypothesis (\\(H_a\\)) is that the population mean (\\(\\mu\\)) is not equal to the specified value (\\(\\mu_0\\)).\n\\[\n\\begin{align*}\nH_0: \\mu = \\mu_0\\\\ \\;\\;\\;\\;\\;\nH_a: \\mu \\neq \\mu_0\n\\end{align*}\n\\]\nSignificance Level:\nThe significance level (\\(\\alpha\\)) is the probability of rejecting the null hypothesis when it is true. We will use the most commonly used significance level, 0.05.\n\\[\n\\alpha = 0.05\n\\]\nMaking the Sampling Distribution\nJust like with the z-test, we can imagine taking many samples from the population. Each sample would have a slightly different mean. These sample means would form a distribution, called the sampling distribution. Due to the Central Limit Theorem, the sampling distribution of the sample means will be approximately normally distributed, with a mean equal to the population mean and a standard error (SE) equal to the sample standard deviation divided by the square root of the sample size. Since we are estimating the normal distribution with the sample standard deviation, we use the t-distribution instead of the normal distribution to estimate the sampling distribution.\n\\[\ns_{\\bar{X}} = \\frac{s}{\\sqrt{n}}\n\\]\nUnder the null hypothesis, the sampling distribution of the mean would be centered at \\(\\mu_0\\), rather than the actual population mean, \\(\\mu\\). We do this because we are assuming that the null hypothesis is true. This is useful because we can see if it is probable that our sample would have come from a population with a mean equal to \\(\\mu_0\\)- meaning if our sample mean is significantly different from \\(\\mu_0\\), we have found something interesting.\nTo clarify, this null distribution is what the distribution of test statistics would look like if the null hypothesis were true. The null distribution is a normal distribution with a mean equal to \\(\\mu_0\\) and a standard deviation equal to \\(s_{\\bar{X}}\\). Since we are estimating the population standard deviation with the sample standard deviation, we use the t-distribution instead of the normal distribution to estimate the sampling distribution.\nBelow is a visualization of the hypothesized null distribution\n\nOur Sample\nThe sample mean (\\(\\bar{X}\\)) is the average of all the values (\\(X_i\\)’s’) in the sample that we collected. It is calculated by summing up all the values in the sample and dividing by the sample size (\\(n\\)).\n\\[\n\\bar{X} = \\frac{\\sum_{i=1}^{n}{X_i}}{n}\n\\]\nOur Sample’s Test-Statistic:\nWe have just caluclated the sample mean, \\(\\bar{X}\\). We hypothesized that the population mean is equal to a specified value, \\(\\mu_0\\). We want to know if the sample mean is significantly different from the population mean. We can calculate our test-statistic, the t-value, to find out where on the sampling distribution our sample mean lies. The t-value is calculated using the sample mean, the population mean, and the sample standard deviation.\n\\[\nt = \\frac{\\bar{X} - \\mu_0}{s_{\\bar{X}}}\n\\]\nCalculating the p-value In the context of a one-sample t-test, we use a t-distribution to derive a p-value. We begin by standardizing our test statistic (the sample mean) to a t-score using the hypothesized population mean and the sample standard deviation. This t-score can then be used to find the p-value of our test.\nLet \\(T \\sim \\mathcal{t}_{(n-1)}\\) be a t-distributed random variable with \\(n-1\\) degrees of freedom and \\(t\\) be the observed t-score. Given this observed t-score, we can then compute the p-value, which is the probability of observing a t-score as extreme as \\(t\\) under the null hypothesis.\n\nFor a two-tailed t-test (where we’re testing for a difference in either direction): When \\(H_a: \\mu \\neq \\mu_0\\), then \\(p = 2 \\times (1 - \\text{Pr}(T \\leq |t|))\\)\nFor a right-tailed t-test (where we’re testing for a difference in the positive direction): When \\(H_a: \\mu > \\mu_0\\), then \\(p = 1 - \\text{Pr}(T \\leq t)\\)\nFor a left-tailed t-test (where we’re testing for a difference in the negative direction): When \\(H_a: \\mu < \\mu_0\\), then \\(p = \\text{Pr}(T \\leq t)\\)\n\nApplet to calculate the p-value\n\n\n\n\n\n\n\n3.4.2 Paired-Samples Test\nA paired-samples t-test, also known as a dependent sample t-test, is used to compare the mean differences of two related groups to assess whether the mean difference between paired observations in the population is significantly different from a number (usually zero). This test assumes the differences between pairs are normally distributed and the population standard deviation of the differences is unknown. The test statistic for a paired-samples t-test is the t-value, which is calculated using the sample mean difference, the population mean difference, and the population standard deviation of the differences.\nAdditionally, the population mean difference (\\(\\mu_d\\)) is calculated by taking the mean of all the differences between pairs in the population. This is done by subtracting the second value in each pair from the first value in each pair and then taking the mean of all the differences. \\[\n\\mu_d = \\frac{1}{N} \\sum_{i=1}^{N} d_i \\;\\;\\;\\text{where}\\;\\; d_i = X_{2i} - X_{1i}\n\\]\nWhere \\(d_i\\) is the list of differences between pairs for the whole population and \\(N\\) is the number of pairs in the population.\nSet of Hypotheses: For a paired-samples t-test, the null hypothesis (\\(H_0\\)) is that the population mean difference (\\(\\mu_d\\)) is equal to \\(\\delta_0\\), usually 0. The alternative hypothesis (\\(H_a\\)) is that the population mean difference is not equal to \\(\\delta_0\\).\n\\[\n\\begin{align*}\nH_0: \\mu_d = \\delta_0\\\\\nH_a: \\mu_d \\neq \\delta_0\n\\end{align*}\n\\]\nSignificance Level:\nThe significance level (\\(\\alpha\\)) is the probability of rejecting the null hypothesis when it is true. We will use the most commonly used significance level, 0.05.\n\\[\n\\alpha = 0.05\n\\]\nMaking the Sampling Distribution\nImagine we took a million samples of pairs from the population. Each sample of pairs would have a slightly different mean difference. This is the sampling distribution for a paired test. This is useful since we can use the sampling distribution to calculate the probability of obtaining a sample mean difference as extreme or more extreme than what we would expect. Since sampling a population a million times is not feasible, we can use the Central Limit Theorem to create a sampling distribution of the mean difference. The Central Limit Theorem states that the sampling distribution of the mean difference will be normally distributed with a mean equal to the population mean difference and a standard deviation equal to the population standard deviation of the differences divided by the square root of the number of pairs. Thus the standard deviation of the sampling distribution of \\(\\bar{d}\\) would be equal to \\(s_{\\bar{d}}\\)\n\\[\ns_{\\bar{d}} = \\frac{s_d}{\\sqrt{n}}\n\\]\nUnder the null hypothesis, the sampling distribution of the mean difference would be centered at \\(\\delta_0\\), rather than the actual population mean difference, \\(\\mu_d\\). We do this because we are assuming that the null hypothesis is true. This is useful because we can see if it is probable that our sample would have come from a population with a mean difference equal to \\(\\delta_0\\) - meaning if our sample mean difference is significantly different from \\(\\delta_0\\), we have found something interesting.\nTo clarify, this null distribution is what the distribution of test statistics would look like if the null hypothesis were true. The null distribution is a normal distribution with a mean equal to \\(\\delta_0\\) and a standard deviation equal to \\(s_{\\bar{d}}\\). Since we are estimating the population standard deviation with the sample standard deviation, we use the t-distribution instead of the normal distribution to estimate the sampling distribution.\n\nOur Sample\nThe sample mean difference (\\(\\bar{d}\\)) is the average of all the differences (\\(d_i\\)’s) in the sample that we collected. It is calculated by summing up all the differences in the sample and dividing by the number of pairs (\\(n\\)).\n\\[\n\\bar{d} = \\frac{\\sum_{i=1}^{n}{d_i}}{n} \\;\\;\\;\\text{where}\\;\\; d_i = X_{2i} - X_{1i}\n\\]\nOur Sample’s Test-Statistic:\nWe have just calculated the sample mean difference, \\(\\bar{d}\\). We hypothesized that the population mean difference is equal to a \\(\\delta_0\\). Since want to know if the sample mean difference is significantly different from \\(\\delta_0\\), we can calculate our test-statistic, the t-value, to find out where on the sampling distribution our sample mean difference lies. The t-value is calculated using the sample mean difference, the population mean difference (\\(\\delta_0\\)), and the population standard deviation of differences.\n\\[\nt = \\frac{\\bar{d}-\\delta_0}{s_{\\bar{d}}}\n\\]\nCalculating the p-value In the context of a t-test, we use a standard normal distribution to derive a p-value. We begin by standardizing our findings (the sample mean difference) to our t-value test statistic using the population mean difference (\\(\\delta_0\\)), and the population standard deviation of differences. This t-value can then be used to find the p-value of our test.\nLet \\(T \\sim \\mathcal{t}_{(n-1)}\\) be a t-distributed random variable with \\(n-1\\) degrees of freedom and \\(t\\) be the observed t-score. Given this observed t-score, we can then compute the p-value, which is the probability of observing a t-score as extreme as \\(t\\) under the null hypothesis.\n\nFor a two-tailed t-test (where we’re testing for a difference in either direction): When \\(H_a: \\mu_d \\neq \\delta_0\\), then \\(p = 2 \\times (1 - \\text{Pr}(T \\leq |t|))\\)\nFor a right-tailed t-test (where we’re testing for a difference in the positive direction): When \\(H_a: \\mu_d > \\delta_0\\), then \\(p = 1 - \\text{Pr}(T \\leq t)\\)\nFor a left-tailed t-test (where we’re testing for a difference in the negative direction): When \\(H_a: \\mu_d < \\delta_0\\), then \\(p = \\text{Pr}(T \\leq t)\\)\n\nApplet to calculate the p-value\n\n\n\n\n\n\n\n3.4.3 Independent-Samples Test\nAn independent-samples t-test, also known as a two-sample t-test, is used to compare the means of two independent groups to assess whether there is a statistically significant difference between the two population means. This test assumes both populations are normally distributed, and the population standard deviations are unknown.\nEach group has its own population standard deviation:\nSet of Hypotheses: For an independent-samples t-test, the null hypothesis (\\(H_0\\)) is that the difference between the population means (\\(\\mu_1 - \\mu_2\\)) is equal to a specified value, usually zero. The alternative hypothesis (\\(H_a\\)) is that the difference between the population means is not equal to this specified value.\n\\[\n\\begin{align*}\nH_0: \\mu_1 - \\mu_2 = \\delta_0\\\\ \\;\\;\\;\\;\\;\\;\nH_a: \\mu_1 - \\mu_2 \\neq \\delta_0\n\\end{align*}\n\\]\nSignificance Level:\nThe significance level (\\(\\alpha\\)) is the probability of rejecting the null hypothesis when it is true. We will use the most commonly used significance level, 0.05.\n\\[\n\\alpha = 0.05\n\\]\nMaking the Sampling Distribution\nSimilar to the tests above, if we took a million samples of two independent groups from the population, each sample would have a slightly different difference in means. This is the sampling distribution for an independent-samples test. This is useful since we can use the sampling distribution to calculate the probability of obtaining a difference in means as extreme or more extreme than what we would expect. Since sampling a population a million times is not feasible, we can use the Central Limit Theorem to create a sampling distribution of the difference in means. The Central Limit Theorem states that the sampling distribution of the difference in means will be normally distributed with a mean equal to the difference in population means and a standard deviation equal to the square root of the sum of the squares of the population standard deviations divided by the square root of the sample sizes. Thus the standard deviation of the sampling distribution of \\(\\bar{X_1} - \\bar{X_2}\\) would be equal to \\(s_{\\bar{X_1} - \\bar{X_2}}\\)\nThe standard error of the difference in sample means can be calculated from the population standard deviations of the two groups and the sizes of the two groups (\\(n_1\\) and \\(n_2\\)). This is the standard deviation of the sampling distribution of the difference in sample means.\n\\[\ns_{\\bar{X_1} - \\bar{X_2}} = \\sqrt{\\left(\\frac{s_1^2}{n_1}\\right) + \\left(\\frac{s_2^2}{n_2}\\right)}\n\\]\nOur Sample\nThe sample means (\\(\\bar{X_1}\\) and \\(\\bar{X_2}\\)) are the averages of all the values in each group. They are calculated by summing up all the values in each group and dividing by the size of each group.\n\\[\n\\bar{X_1} = \\frac{\\sum_{i=1}^{n_1}{X_{1i}}}{n_1}\n\\]\n\\[\n\\bar{X_2} = \\frac{\\sum_{i=1}^{n_2}{X_{2i}}}{n_2}\n\\]\nOur Sample’s Test-Statistic:\nThe test-statistic, the t-value, is calculated using the difference of the sample means, the hypothesized difference of the population means (which we’ve assumed to be \\(\\delta_0\\)), and the standard error.\n\\[\nt = \\frac{(\\bar{X_1} - \\bar{X_2}) - \\delta_0}{s_{\\bar{X_2} - \\bar{X_1}}}\n\\]\nCalculating the p-value For two independent samples t-test, we use a t-distribution to derive a p-value. We begin by standardizing our test statistic (the difference in sample means) to a t-score using the hypothesized difference in population means and the standard error. This t-score can then be used to find the p-value of our test.\nLet’s denote the t-score for this test as \\(t\\) and a t-distributed random variable with \\(n_1 + n_2 - 2\\) degrees of freedom as \\(T \\sim \\mathcal{t}_{(n_1 + n_2 - 2)}\\). The t-score is calculated considering the difference between sample means, the hypothesized difference (often zero), and the standard error of the difference between means. Once we have the calculated t-score, we can use it to find the p-value of our test. The p-value is the probability of observing a t-score as extreme as \\(t\\) under the null hypothesis, and its calculation depends on the alternative hypothesis:\n\nFor a two-tailed t-test (where we’re testing for a difference in either direction): When \\(H_a: \\mu_1 \\neq \\mu_2\\), then \\(p = 2 \\times (1 - \\text{Pr}(T \\leq |t|))\\)\nFor a right-tailed t-test (where we’re testing for a difference in the positive direction): When \\(H_a: \\mu_1 > \\mu_2\\), then \\(p = 1 - \\text{Pr}(T \\leq t)\\)\nFor a left-tailed t-test (where we’re testing for a difference in the negative direction): When \\(H_a: \\mu_1 < \\mu_2\\), then \\(p = \\text{Pr}(T \\leq t)\\)\n\nJust like the z-test, we compare the calculated p-value with our significance level to decide whether to reject or not reject the null hypothesis. If the p-value is less than the chosen significance level (often 0.05), then we reject the null hypothesis in favor of the alternative hypothesis. If the p-value is greater than the significance level, we do not reject the null hypothesis.\nApplet to calculate the p-value\n\n\n\n\n\nCohen’s d Effect Size\nWe often want to measure the size of the difference between our two groups, not just whether this difference is statistically significant. This is where effect size comes in.\nFor a two-sample t-test, we can calculate the effect size using Cohen’s d. It is defined as the difference between two means divided by a standard deviation for the data.\n\\[\nd = \\frac{(\\bar{X_1} - \\bar{X_2})}{\\sqrt{\\left(\\frac{s_1^2}{2}\\right) + \\left(\\frac{s_2^2}{2}\\right)}}\n\\]\nCohen’s d is an appropriate effect size for the comparison between two means."
  }
]