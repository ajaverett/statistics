[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Statistics",
    "section": "",
    "text": "Statistics and Data"
  },
  {
    "objectID": "index.html#the-central-dogma-of-statistics",
    "href": "index.html#the-central-dogma-of-statistics",
    "title": "Statistics",
    "section": "The Central Dogma of Statistics",
    "text": "The Central Dogma of Statistics\nSimilar to the central dogma in biology, statistical inference also has a core principle that guides its objectives. This central dogma centers around trying to know a characteristic about a large population. Since measuring the entire population is costly and challenging, a subset of a population called a sample is taken to estimate the characteristics of the entire population using probability."
  },
  {
    "objectID": "index.html#levels-of-data",
    "href": "index.html#levels-of-data",
    "title": "Statistics",
    "section": "Levels of Data",
    "text": "Levels of Data\nMeasurement, or data collection, is “the assignment of numerals to objects or events according to rules” (Stevens 1946). In general, when collecting and analyzing data, it is important to understand the levels of measurement. These levels determine the precision of the variables, which in turn affects the types of statistical analyses that can be conducted. There are four levels of data measurement: nominal, ordinal, interval, and ratio. Each level builds upon the previous one, increasing in precision and mathematical complexity.\n\nNominal Data\nNominal data is the simplest level of data in which objects are assigned a number based on the category they belong to. It refers to variables that are categorized into qualitative groups or labels without any order or hierarchy. The only information provided by nominal data is the group an observation belongs to. Examples of nominal data include hair color, gender, and religion. Mathematical operations are limited for nominal data, with the mode being the only appropriate measure. Nominal data requires that categories be mutually exclusive (categories do not overlap) and exhaustive (every object belongs to a category and there are no leftover objects).\n\n\nOrdinal Data\nOrdinal data builds upon nominal data by categorizing variables into ordered or hierarchical groups. Although the categories follow a specific order, the difference between them is not defined. Ordinal data allows for comparisons like “less than” and “greater than,” as well as equality and inequality. However, operations such as addition and subtraction are not applicable. Examples of ordinal data include education level, academic grades, and the Likert Scale.\n\n\nInterval Data\nInterval data further builds upon ordinal data by specifying equal intervals between subsequent values. This allows for the calculation of the arithmetic mean, as well as the mode and median. Interval data also allows for linear transformations, but ratios cannot be applied due to the lack of a meaningful zero value. Examples of interval data include temperature measured in degrees Celsius or Fahrenheit and calendar years.\n\n\nRatio Data\nRatio data is similar to interval data but includes a meaningful zero value, allowing for the calculation of ratios between data points. With this level of measurement, all mathematical operations, including multiplication and division, are applicable. Examples of ratio data include weight, height, age, and amounts of money. Since this data type has an absolute zero, this means that we can natively make ratio comparisons. For example, if a boy weighs 100 lb. and a girl weighs 200 lbs. it is legal from this information to say that since 100 = 200/2, the boy is half the weight of the girl. This is not legal for temperature. If place A is 15 degrees Celsius (59 F) and place B is 30 degrees Celsius (86 F), it is not legal to say that place B is twice as hot as place A.\n\n\nLevels of measurement chart\n\nAcceptable mathematical operations for each level of data. (Warne 2020)\n\n\nMathematical Function\nNominal\nOrdinal\nInterval\nRatio\n\n\n\n\nClassification\nY\nY\nY\nY\n\n\nCounting\nY\nY\nY\nY\n\n\nProportion/Percentages\nY\nY\nY\nY\n\n\nRank Ordering\nN\nY\nY\nY\n\n\nAddition\nN\nN\nY\nY\n\n\nSubtraction\nN\nN\nY\nY\n\n\nDividing to form averages\nN\nN\nY\nY\n\n\nDividing to form ratios\nN\nN\nN\nY\n\n\n\n\n\n\n\nStevens, S. S. 1946. “On the Theory of Scales of Measurement.” Science 103 (2684): 677–80. https://doi.org/10.1126/science.103.2684.677.\n\n\nWarne, R. T. 2020. Statistics for the Social Sciences: A General Linear Model Approach. Cambridge University Press. https://books.google.com/books?id=Lw0FEAAAQBAJ."
  },
  {
    "objectID": "i_univariate.html#the-data",
    "href": "i_univariate.html#the-data",
    "title": "1  Univariate Data",
    "section": "1.1 The Data",
    "text": "1.1 The Data\nUnivariate data is a fundamental concept in the field of statistics, providing valuable insights into a single variable’s characteristics within a dataset. The analysis of univariate data focuses on two primary aspects: central tendency and variability. Central tendency describes the central value of the data, such as the mean, median, or mode, while variability explores how the data points spread around the central value, using measures like range, variance, and standard deviation. In this section, we will delve into the intricacies of univariate data analysis, using the ‘mpg’ (miles per gallon) column from the well-known ‘mtcars’ dataset as our example. This approach will enable us to better understand the distribution and behavior of fuel efficiency in various car models, ultimately illuminating the importance of univariate data analysis in making informed decisions and predictions.\nThe univariate data\nThe ‘mtcars’ dataset is a classic dataset in the field of statistics and data analysis, originating from the 1974 edition of Motor Trend magazine. It contains data on 32 different car models, focusing primarily on various technical aspects and performance indicators of automobiles from the early 1970s. The dataset includes information such as miles per gallon (mpg), number of cylinders, horsepower, weight, and other specifications. The ‘mpg’ column within the ‘mtcars’ dataset represents the fuel efficiency of each car model, measured in miles per gallon. In other words, it indicates the number of miles a car can travel on one gallon of fuel.\n\n\nCode\nlibrary(tidyverse)\nlibrary(knitr)\nlibrary(kableExtra)\nlibrary(plotly)\n\nmtcars %>% \n    head(6) %>%\n    select(mpg) %>% \n    kbl() %>%\n    kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\"))\n\n\n\n\n \n  \n      \n    mpg \n  \n \n\n  \n    Mazda RX4 \n    21.0 \n  \n  \n    Mazda RX4 Wag \n    21.0 \n  \n  \n    Datsun 710 \n    22.8 \n  \n  \n    Hornet 4 Drive \n    21.4 \n  \n  \n    Hornet Sportabout \n    18.7 \n  \n  \n    Valiant \n    18.1 \n  \n\n\n\n\n\nVisualizing univariate data\nIn the visualizations below, notice two things. The a lot of the points tend to cluster around a central point, and that the rest of the points tend to deviate away.\n\n\nCode\nfig1 = (bind_rows(\n    (mtcars %>% \n     mutate(mpg = mean(mpg),\n            X=0,\n            type='0: The average')),\n    (mtcars %>% \n      mutate(X=0,\n             type='1: The values'))\n  ) %>% \n    ggplot(aes(x=X, y = mpg, frame = type)) +\n    geom_point(color = 'darkblue',\n               fill='lightblue',\n               size=2.5,\n               shape=21,\n               alpha=.9) +\n    geom_hline(aes(yintercept = mean(mpg)), \n            linetype = \"dashed\", \n            color = \"red\", \n            size = .75,\n            alpha=.5) +\n    theme_classic()  +\n    xlab(\"\") +\n    theme(axis.ticks.x = element_blank(), \n          axis.text.x = element_blank()) +\n    coord_cartesian(xlim=c(-1,10))\n) %>% ggplotly() %>% \n  animation_opts(\n    200, easing = \"elastic\", redraw = F)\n\n\nfig2 = (bind_rows(\n  mtcars %>% mutate(type='1: The values'),  \n  mtcars %>% mutate(type='0: The average',mpg = mean(mpg))\n  ) %>% \n  ggplot(aes(y=mpg, frame=type))+\n  geom_boxplot(color = 'darkblue',\n               fill='lightblue',\n               size=1)+\n  theme_classic() +\n  theme(axis.ticks.x = element_blank(), \n        axis.text.x = element_blank())) %>% \n  ggplotly() %>% \n  animation_opts(\n    200, easing = \"elastic\", redraw = F)\n\nsubplot(fig1, fig2) %>% \n  layout(title = 'Side By Side Subplots')"
  },
  {
    "objectID": "i_univariate.html#central-tendency",
    "href": "i_univariate.html#central-tendency",
    "title": "1  Univariate Data",
    "section": "1.2 Central Tendency",
    "text": "1.2 Central Tendency\nCentral tendency is a measure that aims to describe the center or typical value of a dataset. There are three primary measures of central tendency: mean, median, and mode. Each of these measures provides a different perspective on the central value of the data and can be more or less appropriate depending on the nature of the data and its distribution.\n\n1.2.1 Mean\nMean: The mean, also known as the arithmetic average, is the sum of all data points divided by the total number of data points. It is a widely used measure of central tendency and provides a good indication of the overall level of the dataset. However, the mean can be sensitive to outliers or extreme values, which may result in a distorted representation of the central value.\n\\[\n\\bar{Y} = \\frac{\\sum_{i=1}^n Y_i}{n}\n\\]\nCalculating the mean for the ‘mpg’ column involves summing up the fuel efficiency values of all car models and dividing by the total number of car models in the dataset. The mean mpg provides an indication of the average fuel efficiency across all car models, although it may be influenced by any extreme values or outliers in the data.\n\nn = 32            # number of cars\nY_i = mtcars$mpg  # list of car mpgs\n\n\nY_bar = sum(Y_i)/n\n\nprint(paste(\"The mean value of mpg is\", Y_bar))\n\n[1] \"The mean value of mpg is 20.090625\"\n\n\n\n\n1.2.2 Median\nMedian: The median is the middle value of a dataset when the data points are arranged in ascending or descending order. If there is an odd number of data points, the median is the middle value; if there is an even number of data points, the median is the average of the two middle values. The median is less sensitive to outliers or extreme values than the mean and is particularly useful for datasets with skewed distributions.\n\nY_median = median(Y_i)\n\nprint(paste(\"The median value of mpg is\", Y_median))\n\n[1] \"The median value of mpg is 19.2\"\n\n\n\n\n1.2.3 Mode\nMode: The mode is the value that occurs most frequently in a dataset. A dataset can have multiple modes, no mode, or a single mode. The mode can be a useful measure of central tendency for categorical or discrete data, where the mean and median may not be appropriate. However, the mode may not always provide a meaningful representation of the central value, especially in datasets with several modes or a low frequency of the most common value.\n\nY_mode = median(Y_i)\n\nprint(paste(\"The modal value of mpg is\", Y_mode))\n\n[1] \"The modal value of mpg is 19.2\""
  },
  {
    "objectID": "i_univariate.html#variability",
    "href": "i_univariate.html#variability",
    "title": "1  Univariate Data",
    "section": "1.3 Variability",
    "text": "1.3 Variability\n\n1.3.1 Variance\nVariance is a measure of variability or dispersion in a dataset, representing the average of the squared differences from the mean. It helps us understand how spread out the data points are from the average value. To calculate the variance for univariate data, follow these steps:\n\nCalculate the mean: First, find the mean (arithmetic average) of the dataset by adding up all the data points and dividing the sum by the total number of data points.\nCalculate the deviations: For each data point, calculate the deviation from the mean by subtracting the mean from the individual data point’s value.\nSquare the deviations: Square each deviation obtained in step 2. This step removes the negative signs and emphasizes larger deviations from the mean.\nCalculate the sum of squared deviations: Add up all the squared deviations from step 3. This is called the Total Sum of Squares.\nDivide the Total Sum of Squares by the sum: This is the variance. It represents the average squared deviation of the data points from the mean.\n\nStep One: The mean\nSee above\nStep Two: Distance from the mean\nEvery dot below represents a car in which each vertical location of the dot represents how high or low the mpg is. We can notice that there are some near the average and some farther away from the average. (The average is the horizontal dashed red line in the middle)\nWe can make a list of distances to represent how far away from the mean each car/point is. The cars/points with a very long red line either have a really high or really low mpg while the cars/points with a very small red line have a more average mpg.\n\n\nCode\nmtcars2 = mtcars %>% mutate(row = row_number())\n\nmylm <- lm(mpg~row, data=mtcars2)\n\n\ndf1 <- bind_rows(\n  \n  mtcars2 %>% mutate(\n           type='1, the distances',\n           xmin = row - (mpg-mylm$fit),\n           xmax = row,\n           ymin = mean(mpg),\n           ymax = mpg,\n           rect_alpha=.2),\n  \n  mtcars2 %>% mutate(\n           mpg = mean(mpg)+.1, \n           type='0, the average',\n           xmin = row,\n           xmax = row + (mean(mpg)-mpg) +1,\n           ymin = mpg,\n           ymax = mean(mpg),\n           rect_alpha=.01)\n  ) %>% \n    select(row, mpg, type, xmin, xmax, ymin, ymax,rect_alpha)\n\n(df1 %>% \n  ggplot(aes(row, mpg)) +\n  geom_segment(aes(x = row, xend = row, y = mpg, yend = mean(mpg), frame=type), \n               color = 'red', \n               alpha=.9)+\n  geom_point(aes(frame = type),color = 'darkblue',\n               fill='lightblue',\n               size=2.5,\n               shape=21,\n               alpha=.9) +\n  theme_classic() +\n  geom_hline(aes(yintercept = mean(mpg)), \n             linetype = \"dashed\", \n             color = \"red\", \n             size = .75,\n             alpha=.3) +\n  xlab(\"\") +\n  theme(axis.ticks.x = element_blank(), \n        axis.text.x = element_blank()) \n  \n  # geom_rect(aes(frame = type, xmin=xmin, xmax=xmax, ymin=ymin, ymax=ymax), \n  #           alpha=.3, \n  #           fill='gray',\n  #           color='red')\n  ) %>% \n  ggplotly() %>% animation_opts(\n    2000, easing = \"elastic\", redraw = FALSE\n  )\n\n\n\n\n\n\nTo calculate the distance from the mean for any given point/car, we just need two things. The miles per gallon of the car, and the mean mpg.\n\\[\n\\text{Distance from the mean} = Y_i - \\bar{Y}\n\\]\n\n# Car 22 has an mpg of 15.5 Below is the distance from the mean for car 22\n\n15.5 - Y_bar\n\n[1] -4.590625\n\n\nStep Three: Square the deviations:\nSquares are squares.\nWhen you have a literal square where the side is \\(n\\) units long, the area of that square is \\(n^2\\). We can take the distances and square them. Below is what this would look like visually.\n\n\nCode\nmtcars2 = mtcars %>% mutate(row = row_number())\n\nmylm <- lm(mpg~row, data=mtcars2)\n\n\ndf1 <- bind_rows(\n  \n  mtcars2 %>% mutate(\n           type='1, the squared distances',\n           xmin = row - (mpg-mylm$fit),\n           xmax = row,\n           ymin = mean(mpg),\n           ymax = mpg,\n           rect_alpha=.2),\n  \n  mtcars2 %>% mutate(\n           mpg = mean(mpg)+.1, \n           type='0, the average',\n           xmin = row,\n           xmax = row + (mean(mpg)-mpg) +1,\n           ymin = mpg,\n           ymax = mean(mpg),\n           rect_alpha=.01)\n  ) %>% \n    select(row, mpg, type, xmin, xmax, ymin, ymax,rect_alpha)\n\n(df1 %>% \n  ggplot(aes(row, mpg)) +\n  geom_point(aes(frame = type),color = 'darkblue',\n               fill='lightblue',\n               size=2.5,\n               shape=21,\n               alpha=.9) +\n  theme_classic() +\n  geom_hline(aes(yintercept = mean(mpg)), \n             linetype = \"dashed\", \n             color = \"red\", \n             size = .75,\n             alpha=.3) +\n  xlab(\"\") +\n  theme(axis.ticks.x = element_blank(), \n        axis.text.x = element_blank()) +\n  geom_rect(aes(frame = type, xmin=xmin, xmax=xmax, ymin=ymin, ymax=ymax),\n            alpha=.3,\n            fill='gray',\n            color='red') +\n  coord_cartesian(xlim=c(-6,40))\n  ) %>% \n  ggplotly() %>% animation_opts(\n    2000, easing = \"elastic\", redraw = FALSE\n  )\n\n\n\n\n\n\n\n\n\n\n\n\nWhy we square the deviations\n\n\n\n\n\nSquaring the deviations in the calculation of variance serves several important purposes in understanding the variability or dispersion of a dataset.\nFirst, squaring the deviations eliminates any negative signs that may arise from subtracting the mean from individual data points. Without squaring, the sum of deviations would always equal zero, rendering it an ineffective measure of variability. By squaring the deviations, we ensure that all values are positive, allowing us to properly assess the dispersion.\nSecond, squaring the deviations emphasizes larger deviations from the mean, giving more weight to outliers or extreme values in the dataset. This property is particularly useful in detecting and understanding the impact of outliers on the overall variability of the data. By emphasizing these larger deviations, the variance provides a comprehensive picture of the spread of the data points, including the influence of extreme values.\nLastly, squaring the deviations ensures that the variance has mathematical properties that are useful in statistical analysis, such as being additive for independent variables. This property allows for the combination of variances from multiple variables or datasets, facilitating more complex analyses and comparisons.\n\n\n\nBelow is the code to find the squared distance from the mean:\n\n# Car 22 has an mpg of 15.5 Below is the distance from the mean for car 22\n\n(15.5 - Y_bar)^2\n\n[1] 21.07384\n\n\nStep Four: Calculate the sum of squared deviations\n\nFrom the plot above, look at the squares create above. We can sum up all the squares to make a mega-square. This is called the \\(\\text{Total Sum of Squares}\\) or \\(\\text{SSTO}\\)\n\\[\n\\text{SSTO} = \\sum_{i=1}^n \\left(Y_i - \\bar{Y}\\right)^2\n\\]\nBelow is the code for the total sum of squares\n\nSSTO <- sum((Y_i - Y_bar)^2)\n\nFive: Divide the Total Sum of Squares by the sum\n\nThe sum of observations divided by the number of observations is an average. We have the total sum of squares, but instead of dividing it by the number of observations, we will divide it by number of observations - 1.\n\n\n\n\n\n\nDegrees of Freedom\n\n\n\n\n\nDegrees of freedom refer to the number of independent pieces of information that are available in the sample. Estimates about the population can be calculated from our sample. For example, the mean can be generated from our sample. However, if we use an estimate to get another estimate- we have lost a degree of freedom. For example, the formula for variance uses the mean, meaning a “degree of freedom” is lost- thus degrees of freedom is equal to \\(n-1\\).\n\n\n\n\\[\n\\text{Variance} = \\sigma^2= \\frac{\\text{SSTO}}{\\text{degrees of freedom}}\n\\]\nThe code for the variance is below\n\nvariance = SSTO / (n-1)\n\nvariance\n\n[1] 36.3241\n\n\n\n\n1.3.2 Standard Deviation\nThe variance represents the average squared distance away from the mean- so to see just the distance away from the mean, square root it.\n\\[\n\\sqrt{\\sigma^2} = \\sigma\n\\]\nor in other words:\n\\[\n\\sqrt{\\text{Variance}} = \\text{Standard Deviation}\n\\]\n\n\nstandard_deviation = sqrt(variance)\n\nstandard_deviation\n\n[1] 6.026948\n\n\nBelow, the purple dashed lines represent one standard deviation above and below the mean.\n\n\nCode\nmtcars2 = mtcars %>% mutate(row = row_number())\n\nmylm <- lm(mpg~row, data=mtcars2)\n\n\ndf1 <- bind_rows(\n  \n  mtcars2 %>% mutate(\n           type='1, the distances',\n           xmin = row - (mpg-mylm$fit),\n           xmax = row,\n           ymin = mean(mpg),\n           ymax = mpg,\n           rect_alpha=.2),\n  \n  mtcars2 %>% mutate(\n           mpg = mean(mpg)+.1, \n           type='0, the average',\n           xmin = row,\n           xmax = row + (mean(mpg)-mpg) +1,\n           ymin = mpg,\n           ymax = mean(mpg),\n           rect_alpha=.01)\n  ) %>% \n    select(row, mpg, type, xmin, xmax, ymin, ymax,rect_alpha)\n\n(df1 %>% \n  ggplot(aes(row, mpg)) +\n  geom_segment(aes(x = row, xend = row, y = mpg, yend = mean(mpg), frame=type), \n               color = 'red', \n               alpha=.9)+\n  geom_point(aes(frame = type), color = 'darkblue',\n               fill='lightblue',\n               size=2.5,\n               shape=21,\n               alpha=.9) +\n  theme_classic() +\n  geom_hline(aes(yintercept = mean(mpg)), \n             linetype = \"dashed\", \n             color = \"red\", \n             size = .75,\n             alpha=.3) +\n  xlab(\"\") +\n  geom_hline(aes(yintercept = Y_bar + standard_deviation), alpha=.5, color = \"purple\", linetype = \"dashed\") +\n  geom_hline(aes(yintercept = Y_bar - standard_deviation), alpha=.5, color = \"purple\", linetype = \"dashed\") +\n  theme(axis.ticks.x = element_blank(), \n        axis.text.x = element_blank()) \n  ) %>% \n  ggplotly() %>% animation_opts(\n    2000, easing = \"elastic\", redraw = FALSE\n  )"
  },
  {
    "objectID": "i_distributions.html#calculating-z-scores",
    "href": "i_distributions.html#calculating-z-scores",
    "title": "2  Univariate Distributions",
    "section": "2.1 Calculating Z-scores",
    "text": "2.1 Calculating Z-scores\nA z-score is a standardized measure that indicates the relative position of a data point within a distribution. It is calculated using the following formula:\n\\(z = \\frac{(x - \\mu)}{\\sigma}\\)\nwhere:\n\n\\(z\\) is the z-score\n\\(x\\) is the individual observation\n\\(\\mu\\) is the mean of the distribution\n\\(\\sigma\\) is the standard deviation of the distribution\n\n\n\n\n\n\n68-95-99.7 Rule: Approximately 68% of the observations fall within one standard deviation of the mean (\\(\\mu \\pm \\sigma\\)), 95% within two standard deviations (\\(\\mu \\pm 2\\sigma\\)), and 99.7% within three standard deviations (\\(\\mu \\pm 3\\sigma\\))."
  },
  {
    "objectID": "i_bivariate.html#the-data",
    "href": "i_bivariate.html#the-data",
    "title": "4  Bivariate Data",
    "section": "4.1 The Data",
    "text": "4.1 The Data\nThe ‘mtcars’ dataset is a classic dataset that has been widely used in the field of statistics and data analysis. It comprises data on 32 different car models from the 1970s, offering a comprehensive snapshot of various automobile attributes from that era. In this textbook, we will focus on two key variables from the ‘mtcars’ dataset: ‘mpg’ (miles per gallon) and ‘weight’ (the weight of the car in pounds).\n‘Mpg’ represents the fuel efficiency of a car, with higher values indicating better efficiency. It is an important metric for consumers and manufacturers alike, as it provides information about the overall performance and cost-effectiveness of a vehicle. On the other hand, ‘weight’ is a critical characteristic that impacts numerous aspects of a car’s performance, including acceleration, handling, and fuel consumption. By examining the relationship between ‘mpg’ and ‘weight’ through the lens of regression analysis, we will gain a better understanding of how these two variables interact and influence one another, providing valuable insights into the dynamics of automobile design and performance.\n\n\nCode\nlibrary(tidyverse)\nlibrary(knitr)\nlibrary(kableExtra)\nlibrary(plotly)\n\nmtcars %>% \n    head(6) %>%\n    select(wt, mpg) %>% \n    kbl() %>%\n    kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\"))\n\n\n\n\n \n  \n      \n    wt \n    mpg \n  \n \n\n  \n    Mazda RX4 \n    2.620 \n    21.0 \n  \n  \n    Mazda RX4 Wag \n    2.875 \n    21.0 \n  \n  \n    Datsun 710 \n    2.320 \n    22.8 \n  \n  \n    Hornet 4 Drive \n    3.215 \n    21.4 \n  \n  \n    Hornet Sportabout \n    3.440 \n    18.7 \n  \n  \n    Valiant \n    3.460 \n    18.1 \n  \n\n\n\n\n\nVisualizing bivariate data\n\n\nCode\n(bind_rows(\n    (mtcars %>% \n      mutate(mpg = mean(mpg),\n             wt = mean(wt),\n             type = 'Average Y')),\n    (mtcars %>% \n     mutate(wt = mean(wt),\n            type = 'Variation of Y')),\n\n    (mtcars %>% \n      mutate(type = 'Variation of Y\\nexplained by X'))\n  ) %>% \n    ggplot(aes(x=wt, y = mpg, frame = type)) +\n    geom_point(color = 'darkblue',\n               fill='lightblue',\n               size=2.5,\n               shape=21,\n               alpha=.9) +\n    geom_hline(aes(yintercept = mean(mpg)), \n            linetype = \"dashed\", \n            color = \"red\", \n            alpha=.5) +\n    theme_classic()\n ) %>% ggplotly() %>% \n  animation_opts(\n    2000, easing = \"elastic\", redraw = F)"
  },
  {
    "objectID": "i_bivariate.html#simple-linear-regression",
    "href": "i_bivariate.html#simple-linear-regression",
    "title": "4  Bivariate Data",
    "section": "4.2 Simple Linear Regression",
    "text": "4.2 Simple Linear Regression\nIn the Univariate Data section, we only focused on the mpg data where we found central tendency and variability. Regression is the process of explaining a variable with another variable(s).\n\n4.2.1 Explaining a Variable With Another Variable\nSimple linear regression is a statistical method used to model the relationship between a single independent variable (explanatory) and a single dependent variable (response). The goal is to find the best-fitting straight line that represents the relationship between the two variables, allowing us to make predictions and understand the underlying trends in the data. In this simple model, we only model a single explanatory variable (not multiple \\(X\\)’s)\n\n\\(X\\) - A single quantitative explanatory variable (independent)\n\\(Y\\) - A single quantitative response variable (dependent)\n\nA line with points\nWith points on a graph, we can see if they make a line.\n\nThe True Line: This is usually an unknown association that we try to estimate\nObserved Points: While this is not a line, this represents the actual observed points\nThe Regression Line: This is our estimate of the True Line\n\nTrue, Observed, and Estimated \\(Y\\)\nThe True Regression Line\n\\[\nE\\{Y\\}_i = \\beta_0 + \\beta_1 X_i\n\\]\nThe Observed Points\nEach point, assuming no assumptions are violated, are derived from the formula below\n\\[\nY_i = \\beta_0 + \\beta_1 X_i+ \\epsilon_i\n\\]\nWhile \\(Y_i = b_0 + b_1X_i + r_i\\) is mathematically true, it is important to note that the notation with population parameters is used as this is where the points actually are derived from.\nThe Estimated Regression Line obtained from a regression analysis from the observed points\n\\[\n\\hat{Y}_i = b_0 + b_1 X_i\n\\]\n\n\n\n\n\n\nTerminology\n\n\n\n\n\n\n\\(E\\{Y\\}_i\\) - True mean y-value, also \\(\\mu_{Y|X}_i\\) or \\(E\\{Y|X\\}_i\\)\n\\(\\beta_0\\) - True \\(y\\)-intercept\n\\(\\beta_1\\) - True slope\n\\(Y_i\\) - Response or dependent variable for the \\(i^{\\text{th}}\\) observation.\n\\(\\epsilon_i\\) - Error, distance of dot to true line. \\(\\epsilon_i = Y_i - E\\{Y\\}_i\\)\n\\(r_i\\) - Residual, distance of dot to estimated line. \\(r_i = Y_i - \\hat{Y}_i\\)\n\\(\\hat{Y}_i\\) - The fitted line\n\\(b_0\\) - Estimated \\(y\\)-intercept, also \\(\\hat{\\beta}_0\\)\n\\(b_1\\) - Estimated slope, also \\(\\hat{\\beta}_1\\)\n\n\n\n\n\n\n\n\n\n\nR code for calling your data\n\n\n\n\n\n\nlibrary(tidyverse)\n\nggplot(mtcars, aes(x=wt, y=mpg)) + \n  geom_point(shape = 21, color = 'skyblue4', fill = 'skyblue', size = 3) +\n  labs(title=\"Miles per Gallon vs. Weight of Car\", \n       x=\"Weight of Car\", \n       y=\"Miles per Gallon\") +\n  theme_classic()  \n\n\n\n\n\nY <- mtcars$mpg \nX <- mtcars$wt\n\ni <- 3 #random number\n\n\nX[i]\n\n[1] 2.32\n\n\n\nY[i]\n\n[1] 22.8\n\n\n\nlibrary(tidyverse)\n\nggplot(mtcars, aes(x=wt, y=mpg)) + \n  geom_point(shape = 21, color = 'skyblue4', fill = 'skyblue', size = 3) +\n  labs(title=\"Miles per Gallon vs. Weight of Car\", \n       x=\"Weight of Car\", y=\"Miles per Gallon\") +\n  theme_classic() +\n  geom_vline(aes(xintercept = wt[i] ), color = \"red\", linetype = \"dashed\") +\n  geom_hline(aes(yintercept = mpg[i]), color = \"red\", linetype = \"dashed\")\n\n\n\n\n\n\n\nEstimations\n\n\\(\\hat{Y}_i\\) estimates \\(E\\{Y\\}_i\\)\n\\(b_0\\) estimates \\(\\beta_0\\)\n\\(b_1\\) estimates \\(\\beta_1\\)\n\\(r_i\\) estimates \\(\\epsilon_i\\)\n\n\n\n4.2.2 Calculating the Estimated Regression Line\nThere are multiple ways to calculate the estimated regression line like with calculus or linear algebra.\n\n\n\n\n\n\nThe Calculus Behind Best Fit\n\n\n\n\n\n\nThere exists a combination of \\(b_0\\) and \\(b_1\\) that results in the lowest \\(\\text{SSE}\\). Geometrically, we can imagine a shape in three dimensions where two inputs (\\(b_0\\) and \\(b_1\\)) produce an output (\\(\\text{SSE}\\)). Roughly speaking, where the three dimensional slope is equal to 0 is where the combination of \\(b_0\\) and \\(b_1\\) will produce the lowest possible \\(\\text{SSE}\\). Since the derivative of an equation can tell us the slope at any given point, we can set both equations’ derivatives to zero to find where slope is zero for both equations.\nStep One: Take the partial derivatives with respect to \\(b_0\\) and \\(b_1\\)\nThe partial derivative with respect to \\(b_0\\) is calculated as so:\n\\[\n\\frac{\\partial}{\\partial b_0}\\sum_{i=1}^n\\left(Y_i - (b_0+b_1X_i)\\right)^2\n\\]\n\\[\n\\begin{align*}\n\\mathscr{}\n&= \\sum_{i=1}^n\\frac{\\partial}{\\partial b_0}\\left(Y_i - (b_0+b_1X_i)\\right)^2  &\\text{Sum Rule}\\\\\n&= \\sum_{i=1}^n2\\left(Y_i - (b_0+b_1X_i)\\right)\\frac{\\partial}{\\partial b_0}\\left(Y_i - (b_0+b_1X_i)\\right) &\\text{Power Rule}\\\\\n&= \\sum_{i=1}^n 2\\left(Y_i - (b_0+b_1X_i)\\right)(-1) &\\text{Simplify}\\\\\n&= -2\\sum_{i=1}^n\\left(Y_i - (b_0+b_1X_i)\\right) &\\text{Constant Multiple Rule for Sums}\n\\end{align*}\n\\]\nThe partial derivative with respect to \\(b_1\\) is calculated as so:\n\\[\n\\frac{\\partial}{\\partial b_1}\\sum_{i=1}^n\\left(Y_i - (b_0+b_1X_i)\\right)^2\n\\]\n\\[\n\\begin{align*}\n\\mathscr{}\n&= \\sum_{i=1}^n\\frac{\\partial}{\\partial b_1}\\left(Y_i - (b_0+b_1X_i)\\right)^2  &\\text{Sum Rule}\\\\\n&= \\sum_{i=1}^n2\\left(Y_i - (b_0+b_1X_i)\\right)\\frac{\\partial}{\\partial b_1}\\left(Y_i - (b_0+b_1X_i)\\right) &\\text{Power Rule}\\\\\n&= \\sum_{i=1}^n 2\\left(Y_i - (b_0+b_1X_i)\\right)(-X_i) &\\text{Simplify}\\\\\n&= -2\\sum_{i=1}^nX_i\\left(Y_i - (b_0+b_1X_i)\\right) &\\text{Constant Multiple Rule for Sums}\n\\end{align*}\n\\]\nStep Two: Set the simplified partial derivatives to zero and solve for \\(b_0\\) and \\(b_1\\) respectively\nSolving for \\(b_0\\): \\[0 = -2\\sum_{i=1}^n\\left(Y_i - (b_0+b_1X_i)\\right)\\]\n\\[\n\\begin{align*}\n\\mathscr{}\n&0 = \\sum_{i=1}^n\\left(Y_i - b_0-b_1x_i\\right) &\\text{Simplify}\\\\\n&0 = \\sum_{i=1}^nY_i - \\sum_{i=1}^nb_0-\\sum_{i=1}^nb_1X_i &\\text{Sum Rule}\\\\\n&0 = \\sum_{i=1}^nY_i - nb_0-b_1\\sum_{i=1}^nX_i &\\text{Constant Multiple Rule for Sums}\\\\\n& nb_0 = \\sum_{i=1}^nY_i -b_1\\sum_{i=1}^nX_i &\\text{Simplify}\\\\\n& b_0 = \\frac{\\sum_{i=1}^nY_i}{n} -\\frac{b_1\\sum_{i=1}^nX_i}{n} &\\text{Simplify}\\\\\n& b_0 = \\bar{Y} - b_1\\bar{X} &\\text{Simplify to average}\\\\\n\\end{align*}\n\\]\nSolving for \\(b_1\\) while substituting \\(b_0\\):\n\\[\n0 = -2\\sum_{i=1}^nX_i\\left(Y_i - (b_0+b_1X_i)\\right)\n\\]\n\\[\n\\begin{align*}\n\\mathscr{}\n&0 = \\sum_{i=1}^nX_i\\left(Y_i - ((\\bar{Y} - b_1\\bar{X})+b_1X_i)\\right) &\\text{Substitute }b_0\\\\\n&0 = \\sum_{i=1}^nX_i(Y_i-\\bar{Y}) - \\sum_{i=1}^nb_1X_i(X_i-\\bar{X})&\\text{Sum Rule}\\\\\n&0 = \\sum_{i=1}^nX_i(Y_i-\\bar{Y}) - b_1\\sum_{i=1}^nX_i(X_i-\\bar{X})&\\text{Constant Multiple Rule for Sums}\\\\\n& b_1= \\frac{\\sum_{i=1}^nX_i(Y_i-\\bar{Y})}{\\sum_{i=1}^nX_i(X_i-\\bar{X})} &\\text{Simplify}\\\\\n& b_1= \\frac{\\sum_{i=1}^n(X_i-\\bar{X})(Y_i-\\bar{Y})}{\\sum_{i=1}^n(X_i-\\bar{X})^2} &\\text{Simplify (steps skipped)}\\\\\n\\end{align*}\n\\]\nStep Three: Construct the formula\nOnce you have found \\(b_0\\) and \\(b_1\\) as so:\n\n\\(b_0 = \\bar{Y} - b_1\\bar{X}\\)\n\\(b_1= \\frac{\\sum_{i=1}^n(X_i-\\bar{X})(Y_i-\\bar{Y})}{\\sum_{i=1}^n(X_i-\\bar{X})^2}\\)\n\n(\\(\\bar{Y}\\) represents the mean of the \\(Y\\) values and \\(\\bar{X}\\) represents the mean of the \\(X\\) values)\nWe can construct the formula for \\(\\hat{Y}_i\\) as the following:\n\\[\n\\hat{Y}_i = b_0 + b_1X_i\n\\]\nWhich means \\(b_0\\) is the change in the mean of \\(Y\\) for a \\(1\\) unit increase in \\(X\\).\n\n\n\n\n\n\n\n\n\nLeast Squares in Linear Algebra\n\n\n\n\n\nStep One: Organize the data into vectors Let’s assume you have \\(n\\) observations of the independent variable \\(\\mathbf{X}\\) and dependent variable \\(\\mathbf{Y}\\). Organize the data into two vectors:\n\\[\n\\mathbf{X} = \\begin{bmatrix} X_1 \\\\ X_2 \\\\ \\vdots \\\\ X_n \\end{bmatrix}, \\quad \\mathbf{Y} = \\begin{bmatrix} Y_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_n \\end{bmatrix}\n\\]\nStep Two: Create a design matrix\nCreate a design matrix \\(\\mathbf{A}\\) by adding a column of ones to represent the intercept term\n\\[\n\\mathbf{A} = \\begin{bmatrix} 1 & X_1 \\\\ 1 & X_2 \\\\ \\vdots & \\vdots \\\\ 1 & X_n \\end{bmatrix}\n\\]\nStep Three: Make a Projection:\nThe goal of simple linear regression is to find the best-fitting line, which can be viewed as projecting the dependent variable \\(\\mathbf{Y}\\) onto the column space of the design matrix \\(\\mathbf{A}\\). This projection, denoted by \\(\\mathbf{\\hat{Y}}\\), can be computed as:\n\\[\n\\mathbf{\\hat{Y}} = \\mathbf{A}(\\mathbf{A}^T\\mathbf{A})^{-1}\\mathbf{A}^T\\mathbf{Y}\n\\]\nThis formula comes from the orthogonal projection of \\(\\mathbf{Y}\\) onto the column space of \\(\\mathbf{A}\\), which minimizes the squared residuals.\nStep Four: Compute the coefficients:\nThe coefficients of the best-fitting line can be found by solving the following linear system:\n\\[\n\\mathbf{A}^T\\mathbf{A}\\mathbf{\\beta} = \\mathbf{A}^T\\mathbf{Y}\n\\]\nHere, \\(\\mathbf{\\beta}\\) is a vector containing the coefficients \\(\\beta_0\\) and \\(\\beta_1\\):\n\\[\n\\mathbf{\\beta} = \\begin{bmatrix} \\beta_0 \\\\ \\beta_1 \\end{bmatrix}\n\\]\nTo solve for \\(\\mathbf{\\beta}\\), you can use the following formula:\n\\[\n\\mathbf{\\beta} = (\\mathbf{A}^T\\mathbf{A})^{-1}\\mathbf{A}^T\\mathbf{Y}\n\\]\nStep Five: Construct the formula:\nOnce you have computed the coefficients \\(\\beta_0\\) and \\(\\beta_1\\), you can write the estimated regression line as:\n\\[\n\\hat{Y}_i = \\beta_0 + \\beta_1 X_i\n\\]\nThe estimated slope, \\(\\beta_1\\), indicates the average change in the dependent variable (\\(Y\\)) for a one-unit increase in the independent variable (\\(X\\)). The estimated intercept, \\(\\beta_0\\), represents the predicted value of \\(Y\\) when \\(X = 0\\).\n\n\n\nThe formula from calculus to find the coefficients for the estimated regression line is as so:\n\n\\(b_0 = \\bar{Y} - b_1\\bar{X}\\)\n\\(b_1= \\frac{\\sum_{i=1}^n(X_i-\\bar{X})(Y_i-\\bar{Y})}{\\sum_{i=1}^n(X_i-\\bar{X})^2}\\)\n\n(\\(\\bar{Y}\\) represents the mean of the \\(Y\\) values and \\(\\bar{X}\\) represents the mean of the \\(X\\) values)\n\n\n\n\n\n\nR code for calculating the coefficients for the estimated regression line\n\n\n\n\n\n\nmylm <- lm(mpg ~ wt, data = mtcars)\npander::pander(summary(mylm))\n\n\n\n\n\n\n\n\n\n\n\n \nEstimate\nStd. Error\nt value\nPr(>|t|)\n\n\n\n\n(Intercept)\n37.29\n1.878\n19.86\n8.242e-19\n\n\nwt\n-5.344\n0.5591\n-9.559\n1.294e-10\n\n\n\n\nFitting linear model: mpg ~ wt\n\n\n\n\n\n\n\n\nObservations\nResidual Std. Error\n\\(R^2\\)\nAdjusted \\(R^2\\)\n\n\n\n\n32\n3.046\n0.7528\n0.7446\n\n\n\n\n\n\nb_0 <- mylm$coefficients[1]\nb_1 <- mylm$coefficients[2]\n\n\nb_0\n\n(Intercept) \n   37.28513 \n\n\n\nb_1\n\n       wt \n-5.344472 \n\n\n\nggplot(mtcars, aes(x=wt, y=mpg)) +\ngeom_point(shape = 21, color = 'skyblue4', fill = 'skyblue', size = 3) +\nstat_function(fun = function(x) b_0 + b_1 * x) + \ntheme_classic() \n\n\n\n\n\n\n\n\n\n4.2.3 Visualizing Simple Linear Regression\nBelow, we have charted both variables to try and see if some of the variance of \\(Y\\) is explained by \\(X\\). The blue line represents the estimated regression line.\nThe goal of the blue line below is to show the association between the \\(X\\) and \\(Y\\) variable. The slope can tell us how much our Y value changes by a change in our \\(X\\) value.\n\n\nCode\nlibrary(tidyverse)\nlibrary(plotly)\n\n\n(ggplot(mtcars, aes(x = wt, y = mpg)) +\n  geom_point(color = 'darkblue',\n               fill='lightblue',\n               size=2.5,\n               shape=21,\n               alpha=.9) +\n  theme_classic() +\n  geom_smooth(method = \"lm\", se = FALSE) \n  ) %>% \n  ggplotly()"
  },
  {
    "objectID": "i_bivariate.html#attributes-of-regression",
    "href": "i_bivariate.html#attributes-of-regression",
    "title": "4  Bivariate Data",
    "section": "4.3 Attributes of Regression",
    "text": "4.3 Attributes of Regression\n\n4.3.1 Residuals\nAs you can see from the plot, the points do not lie exactly on the estimated regression line. For any given point, there is a distance between the point and the regression line. This distance is called the residual.\nThe formula to calculate any given residual is below:\n\\[\n\\begin{align*}\nr_i &= \\left(Y_i - \\hat{Y_i}\\right)\\\\\nr_i &= \\left(Y_i - (b_0+b_1X_i)\\right)\n\\end{align*}\n\\]\n\n\n4.3.2 SSE: the Sum of Squared Errors\n\n\nCode\nmodel <- lm(mpg ~ wt, data = mtcars)\n\nmtcars$residuals <- residuals(model)\n\n(ggplot(mtcars, aes(x = wt, y = mpg)) +\n  theme_classic() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  geom_segment(aes(x = wt, xend = wt, y = mpg, yend = mpg - residuals), color = 'red') +\n  geom_point(color = 'darkblue',\n               fill='lightblue',\n               size=2.5,\n               shape=21,\n               alpha=.9)\n  ) %>% \n  ggplotly()\n\n\n\n\n\n\nIn the graph above, we can see the vertical distance between the actual points and the predicted line. Some points have a very large red line which means that the model may not explain that specific point very well. Conversely, the points with a small red line seem to be explained by the model very well.\nTo make sure the residuals (distances from the regression line) are positive, we will square them. We can add them all up to get a sum. Below is the formula to get the sum of the squared residuals\n\n\nCode\n(ggplot(mtcars, aes(x = wt, y = mpg)) +\n  theme_classic() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  geom_rect(aes(xmin=wt - abs(mpg-mylm$fit)*.13, xmax=wt, ymin=mylm$fit, ymax=mpg), color = 'red',alpha=.2) +\n  geom_segment(aes(x = wt, xend = wt, y = mpg, yend = mpg - residuals), color = 'red') +\n  geom_point(color = 'darkblue',\n               fill='lightblue',\n               size=2.5,\n               shape=21,\n               alpha=.9) +\n  coord_cartesian(xlim=c(1,6)) \n ) %>%\n  ggplotly()\n\n\n\n\n\n\n\\[\n\\begin{align*}\n\\text{SSE} &= \\sum_{i=1}^n \\left(r_i\\right)^2 \\\\\n\\text{SSE} &= \\sum_{i=1}^n\\left(Y_i - \\hat{Y_i}\\right)^2\\\\\n\\text{SSE} &= \\sum_{i=1}^n\\left(Y_i - (b_0+b_1X_i)\\right)^2\n\\end{align*}\n\\]\n(Note: SSE is used because E stands for error, the statistic that residuals try to approximate)\n(Note: The estimated regression line or line of best fit from the observed points is represented by the equation \\(\\hat{Y}_i=b_0+b_1X_i\\) , where \\(b_1\\) is the estimated slope of the line and \\(b_0\\) is the estimated y-intercept.)\n\n\n4.3.3 MSE: Mean Squared Error\nGeometrically, we can imagine the error “amount” as the area of the boxes. This is why when we add up all the boxes, we get the sum of squares error or \\(\\text{SSE}\\). This means we can estimate the average size of the box with a formula you might expect:\n\\[\n\\text{MSE} = \\frac{\\text{SSE}}{\\text{degrees of freedom}} \\\\\\;\\;\\\\\\;(\\text{where degrees of freedom}=n-2)\n\\]\n\n\n\n\n\n\nDegrees of Freedom\n\n\n\n\n\nSince two estimates (\\(b_0\\) and \\(b_1\\)) are used to calculate \\(\\text{MSE}\\) from our sample about the population, we have lost two degrees of freedom from our sample. Thus, degrees of freedom is now equal to \\(n-2\\)\n\n\n\nThus we can integrate it with our previous formulas\n\\[\n\\begin{align*}\n\\text{MSE} &= \\frac{\\sum_{i=1}^n \\left(r_i\\right)^2}{\\text{degrees of freedom}} \\\\\n\\text{MSE} &= \\frac{\\sum_{i=1}^n\\left(Y_i - \\hat{Y_i}\\right)^2}{\\text{degrees of freedom}}\\\\\n\\text{MSE} &= \\frac{\\sum_{i=1}^n\\left(Y_i - (b_0+b_1X_i)\\right)^2}{\\text{degrees of freedom}}\n\\\\\\;\\\\&(\\text{where degrees of freedom}=n-2)\n\\end{align*}\n\\]\n\n\n4.3.4 RMSE: Root Mean Squared Error or Residual Standard Error\nThe above formula calculates the average box size- but what about the average line size? Another similarly intuitive formula , we get:\n\\[\n\\text{RMSE} = \\sqrt{\\text{MSE}}\n\\]\nThus we can integrate it with our previous formulas\n\\[\n\\begin{align*}\n\\text{RMSE} &= \\sqrt{\\frac{\\text{SSE}}{\\text{degrees of freedom}}} \\\\\n\\text{RMSE} &= \\sqrt{\\frac{\\sum_{i=1}^n \\left(r_i\\right)^2}{\\text{degrees of freedom}}} \\\\\n\\text{RMSE} &= \\sqrt{\\frac{\\sum_{i=1}^n\\left(Y_i - \\hat{Y_i}\\right)^2}{\\text{degrees of freedom}}}\\\\\n\\text{RMSE} &= \\sqrt{\\frac{\\sum_{i=1}^n\\left(Y_i - (b_0+b_1X_i)\\right)^2}{\\text{degrees of freedom}}}\n\\\\\\;\\\\&(\\text{where degrees of freedom}=n-2)\n\\end{align*}\n\\]\n\n\n\n\n\n\nMinimizing Residuals\n\n\n\n\n\nA single residual, \\(r_i\\), is the distance between an observed point and the estimated regression line. Given any line, we can calculate how different our predicted value, \\(\\hat{Y_i}\\) is from our actual data point, \\(Y_i\\). This vertical distance from the real y output and the predicted y output is called the residual. The formula for any given residual is represented as such\n\\[\n\\begin{align*}\nr_i &= Y_i-\\hat{Y}_i\\\\\nr_i &= Y_i - (b_0+b_1X_i)\n\\end{align*}\n\\]\nThe goal of simple linear regression is to minimize the sum of the squared errors (\\(\\text{SSE}\\)). This is represented by the equations:\n\\[\n\\begin{align*}\n\\text{SSE} &= \\sum_{i=1}^n \\left(r_i\\right)^2 \\\\\n\\text{SSE} &= \\sum_{i=1}^n\\left(Y_i - \\hat{Y_i}\\right)^2\\\\\n\\text{SSE} &= \\sum_{i=1}^n\\left(Y_i - (b_0+b_1X_i)\\right)^2\n\\end{align*}\n\\]\nThe closer any given line is to the best fit line, the lower the \\(\\text{SSE}\\) will be. In fact, the best fit line represents the line with the lowest possible \\(\\text{SSE}\\). This line is found by manipulating the values for \\(b_0\\) and \\(b_1\\) since the \\(X\\) values must stay the same.\n\n\n\n\n\n4.3.5 SSR: The Sum of Squares Regression\nThe point of regression is to try to find an association between two variables. If a line explains some of the association between two variables, we should be able to “count” how much association it explains.\n\n\nCode\nmylm <- lm(mpg ~ wt, data = mtcars)\n\n\n\n(ggplot(mtcars %>% mutate(mpg = predict(mylm, data.frame(wt = wt))), aes(x = wt, y = mpg)) +\n  theme_classic() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  geom_rect(aes(xmin=wt, xmax=wt + (mean(mpg)-mpg)*.1, ymin=mpg, ymax=mean(mpg)), color = 'red',alpha=.2) +\n  geom_segment(aes(x = wt, xend = wt, y = mpg, yend = mean(mpg)), color = 'red') +\n  geom_point(color = 'darkblue',\n               fill='lightblue',\n               size=2.5,\n               shape=21,\n               alpha=.9) +\n  geom_hline(aes(yintercept = mean(mpg)), \n           linetype = \"dashed\", \n           color = \"red\", \n           alpha=.3) +\n  coord_cartesian(xlim=c(1,6), ylim=c(8,31.5))\n ) %>%\n  ggplotly()\n\n\n\n\n\n\nSSR essentially measures the extent to which the regression line captures the relationship between the X and Y. A higher SSR value indicates that the regression line is better at explaining the observed variation in the data. Conversely, a lower SSR value suggests that the regression line may not be a good fit for the data.\nThe SSR formula is as so:\n\\[\n\\text{SSR} = \\sum_{i=1}^{n} (\\hat{Y}_{i} - \\bar{Y})^2\n\\]\n\n\n4.3.6 Measures of Distances And Their Names\n\n\nCode\nmylm <- lm(mpg~wt, data=mtcars)\n\ndf1 <- bind_rows(\n  mtcars %>% mutate(\n           wt=mean(wt),\n           mpg=mean(mpg),\n           type='0, empty',\n           xmin = wt,\n           xmax = wt + (mean(mpg)-mpg)+.1,\n           ymin = mpg,\n           ymax = mean(mpg),\n           rect_alpha=.01),\n  mtcars %>% mutate(\n           wt=mean(wt),\n           type='1, ssto',\n           xmin = wt,\n           xmax = (wt+(mean(mpg)-mpg)*.07),\n           ymin = mpg,\n           ymax = mean(mpg),\n           rect_alpha=.2),\n  mtcars %>% mutate(\n           type='2, sse',\n           xmin = wt - abs(mpg-mylm$fit)*.07, \n           xmax = wt,\n           ymin = mylm$fit,\n           ymax = mpg,\n           rect_alpha=.2),\n  mtcars %>% mutate(\n           mpg = predict(mylm, data.frame(wt = wt)),\n           type='3, ssr',\n           xmin = wt, \n           xmax=wt + (mean(mpg)-mpg)*.07,\n           ymin = mpg,\n           ymax = mean(mpg),\n           rect_alpha=.2),\n  mtcars %>% mutate(\n           mpg = mean(mpg), \n           type='4, mean',\n           xmin = wt,\n           xmax = wt + (mean(mpg)-mpg) +.1,\n           ymin = mpg,\n           ymax = mean(mpg),\n           rect_alpha=.01)\n  ) %>% \n    select(wt, mpg, type, xmin, xmax, ymin, ymax,rect_alpha)\n\n(df1 %>% \n  ggplot(aes(x = wt, y = mpg)) +\n  theme_classic() +\n  stat_function(fun = function(x) b_0 + b_1 * x, \n                linetype = \"dashed\", \n                color = \"blue\", \n                alpha=.3) +\n  geom_rect(aes(frame = type, xmin=xmin, xmax=xmax, ymin=ymin, ymax=ymax), alpha=.3, fill='gray',color='red') +\n  geom_hline(aes(yintercept = mean(mpg)), \n             linetype = \"dashed\", \n             color = \"red\", \n             alpha=.3) +\n  geom_point(aes(frame = type),\n               color = 'darkblue',\n               fill='lightblue',\n               size=2.5,\n               shape=21,\n               alpha=.9) +\n  coord_cartesian(xlim=c(1.5,5.5), ylim=c(7,35))\n  ) %>% \n  ggplotly() %>% animation_opts(\n    2000, easing = \"elastic\", redraw = FALSE\n  )\n\n\n\n\n\n\nIn the example above, take note of three distances.\n\n\\(0 \\rightarrow 1\\): \\(\\text{SSTO}\\): The sum of “each point’s distance to the mean, squared”\n\\(1 \\rightarrow 2\\): Regression: adding in an explanatory variable\n\\(2 \\rightarrow 3\\): \\(\\text{SSE}\\): The sum of “each point’s distance to the line, squared”\n\\(3 \\rightarrow 4\\): \\(\\text{SSR}\\): The sum of “each point’s expected distance to the mean”\n\n\n\n\n\n\n\nR code for calculating the above terms\n\n\n\n\n\n\nmylm <- lm(mpg ~ wt, data=mtcars)\n\nfitted_values <- mylm$fitted.values\ny_bar <- mean(mtcars$mpg)\n\nSSE <- sum(mylm$residuals^2)\nSSR <- sum((fitted_values - y_bar)^2)\nSSTO <- sum((mtcars$mpg - y_bar)^2)\n\npander::pander(cat(\"Sum of Squared Errors (SSE):\", SSE, \"\\n\"))\n\nSum of Squared Errors (SSE): 278.3219\n\npander::pander(cat(\"Sum of Squares Regression (SSR):\", SSR, \"\\n\"))\n\nSum of Squares Regression (SSR): 847.7252\n\npander::pander(cat(\"Total Sum of Squares (SSTO):\", SSTO, \"\\n\"))\n\nTotal Sum of Squares (SSTO): 1126.047"
  },
  {
    "objectID": "i_bivariate.html#coefficient-of-determination",
    "href": "i_bivariate.html#coefficient-of-determination",
    "title": "4  Bivariate Data",
    "section": "4.4 Coefficient of Determination",
    "text": "4.4 Coefficient of Determination\nRatio of explained variance and total variance\n\nWe know why the points would be, in total, \\(\\text{SSR}\\) away from the average (since the estimated regression line explains the variance), but we don’t know why the points are \\(\\text{SSE}\\) away from the estimated regression line.\nNotice how \\(\\text{SSTO} = \\text{SSE} + \\text{SSR}\\). This can be equivalently written as \\(\\text{Total variance} = \\text{Unexplained variance} + \\text{Explained variance}\\)\nWe can show this relationship of explained/total variance as a coefficient.\n\n4.4.1 Calculating the Coefficient of Determination\nUsing the terms above, we can calculate a ratio of the variance of \\(Y\\) explained by the estimated regression line (\\(\\text{SSR}\\)) and the total variance of \\(Y\\) from the average \\(Y\\) value (\\(\\text{SSTO})\\).\n\\[\nR^2 = \\frac{\\text{SSR}}{\\text{SSTO}} = \\frac{\\text{Explained Variance}}{\\text{Total Variance}}\n\\]\n\\[\nR^2 = 1 - \\frac{\\text{SSE}}{\\text{SSTO}} = \\frac{\\text{Unexplained Variance}}{\\text{Total Variance}}\n\\]\nAll \\(R^2\\) represents is the ratio between variance explained by the model and total variance.\n\n\n\n\n\n\n\nR code for calculating the coefficient of determination\n\n\n\n\n\n\nlm_summary <- summary(mylm)\n\npander::pander(lm_summary$r.squared)\n\n0.7528\n\n#or\n\npander::pander(SSR/SSTO)\n\n0.7528\n\n#or\n\npander::pander(1 - SSE/SSTO)\n\n0.7528\n\n\n\n\n\n\n\n4.4.2 Slope and significance vs the \\(R^2\\)\nSlope: The slope indicates how much \\(Y\\) changes for a one-unit change in \\(X\\). This means that if our slope is significantly different than 0 (\\(\\text{p-value} < \\alpha\\)), we reject the notion that there is no association between \\(X\\) and \\(Y\\).\n\n\\(R^2\\): For the graphic above, notice how the slope is exactly the same for each graph- the difference is how much the data varies given the same slope. This is important since given the rule (slope), \\(R^2\\) will describe how ‘obedient’ the data is to the law.\n\n\n\n\n\n\nAssumptions for Simple Linear Regression\n\n\n\n\n\nIf we are to make a model off the points displayed as such:\n\\[\nY_i = \\beta_0 + \\beta_1 X_i+ \\epsilon_i\n\\]\nWe need to confirm the following assumptions\n\n1) Linear relation: Linear relationship between \\(X\\) and \\(Y\\)\n2) Normal errors: \\(\\epsilon \\sim \\mathcal{N}(0,\\sigma^2)\\), the error terms are normally distributed with a mean of 0\n3) Constant variance: The varianceB \\(O\u0003^2\\) of the error terms is constant (the same) across all values of \\(X_i\\)\n4) Fixed \\(X\\): the \\(X\\) values can be considered fixed and measured without error\n5) Independent errors: \\(\\epsilon\\) is independent\n\n\\(Y_i = \\beta_0 + \\beta_1 X_i+ \\epsilon_i :\\epsilon \\sim \\mathcal{N}(0,\\sigma^2)\\)\nGiven \\(X\\), \\(Y \\sim \\mathcal{N}(\\beta_0 +\\beta_1X,\\sigma^2)\\)\n\n\n\n\n\n\n\n\n\nR code for testing the assumptions for simple linear regression\n\n\n\n\n\nThree plots in R can be used to test the assumptions for simple linear regression:\nResiduals versus Fitted-values Plot The linear relationship and constant variance assumptions can be diagnosed using a residuals versus fitted-values plot. The fitted values are the \\(Y^i\\)\n\nThe residuals are the \\(r_i\\)\nThis plot compares the residual to the magnitude of the fitted-value. No discernable pattern in this plot is desirable.\n\nQ-Q Plot of the Residuals\nThe normality of the error terms can be assessed by considering a normal probability plot (Q-Q Plot) of the residuals. If the residuals appear to be normal, then the error terms are also considered to be normal. If the residuals do not appear to be normal, then the error terms are also assumed to violate the normality assumption.\nResiduals versus Order Plot\nWhen the data is collected in a specific order, or has some other important ordering to it, then the independence of the error terms can be assessed. This is typically done by plotting the residuals against their order of occurrance. If any dramatic trends are visible in the plot, then the independence assumption is violated.\n\npar(mfrow=c(1,3)) #Residuals versus Fitted-values Plot: Checks Assumptions #1 and #3\n\nplot(mylm,which=1:2) #Q-Q Plot of the Residuals: Checks Assumption #2\n\nplot(mylm$residuals) #Residuals versus Order Plot: Checks Assumption #5"
  },
  {
    "objectID": "i_univartests.html#null-and-alternative-hypotheses",
    "href": "i_univartests.html#null-and-alternative-hypotheses",
    "title": "3  Univariate Hypothesis Testing",
    "section": "3.1 Null and Alternative Hypotheses",
    "text": "3.1 Null and Alternative Hypotheses\nA null hypothesis and alternative hypothesis are fundamental concepts in hypothesis testing, a key aspect of inferential statistics. Hypothesis testing is a statistical method used to test the validity of a claim or assertion based on sample data, with the goal of making inferences about a population.\nNull Hypothesis\nThe null hypothesis (denoted as \\(H_0\\)) is a statement of no effect or no difference. It represents the default assumption that there is no relationship between the variables under investigation or that the treatment has no effect. The null hypothesis typically asserts that the observations are the result of chance or that the observed differences can be attributed to random variation.\nAlternative Hypothesis\nThe alternative hypothesis (denoted as \\(H_a\\)) is a statement that contradicts the null hypothesis. It represents the claim that there is a relationship between the variables or that the treatment has an effect. The alternative hypothesis is what researchers hope to provide evidence for through the process of hypothesis testing."
  },
  {
    "objectID": "i_univartests.html#null-hypothesis-significance-testing",
    "href": "i_univartests.html#null-hypothesis-significance-testing",
    "title": "3  Univariate Hypothesis Testing",
    "section": "3.1 Null Hypothesis Significance Testing",
    "text": "3.1 Null Hypothesis Significance Testing\nThe central dogma of statistics is to make estimates about the population based on samples. Null Hypothesis Significance Testing (NHST) is a widely used statistical approach for testing the validity of a claim or hypothesis about a population based on sample data. The process involves comparing the observed data to what would be expected under a null hypothesis, which usually assumes no effect or relationship between variables.\nNull Hypothesis\nThe null hypothesis (denoted as \\(H_0\\)) is a statement of no effect or no difference. It represents the default assumption that there is no relationship between the variables under investigation or that the treatment has no effect. The null hypothesis typically asserts that the observations are the result of chance or that the observed differences can be attributed to random variation.\nAlternative Hypothesis\nThe alternative hypothesis (denoted as \\(H_a\\)) is a statement that contradicts the null hypothesis. It represents the claim that there is a relationship between the variables or that the treatment has an effect. The alternative hypothesis is what researchers hope to provide evidence for through the process of hypothesis testing.\n\n3.1.1 The Process of Null Hypothesis Significance Testing\n\nState the null hypothesis (\\(H_0\\)) and alternative hypothesis (\\(H_a\\)). The null hypothesis is a statement of no effect or relationship between the variables being tested, while the alternative hypothesis is a statement that claims some effect or relationship between the variables.\nChoose a significance level. The significance level, denoted by \\(\\alpha\\), is the probability of rejecting the null hypothesis when it is true. The most commonly used significance level is 0.05\nCollect data and calculate the test statistic. Gather your sample data and perform the appropriate statistical test to calculate the test statistic. The test statistic is a numerical value that measures the difference between the observed data and what would be expected under the null hypothesis. Different tests use different test statistics meaning each one is denoted differently dependent on which test. Common test statistics include \\(z\\), \\(t\\), \\(F\\), or \\(\\chi^2\\).\nDetermine the p-value. The p-value, denoted as \\(p\\), is the probability of observing a test statistic as extreme or more extreme than the one calculated from the sample data, assuming the null hypothesis is true. It quantifies the evidence against the null hypothesis. Lower p-values indicate stronger evidence against the null hypothesis since they imply that the observed results are less likely to have occurred by chance alone under the null hypothesis. In other words, a smaller p-value suggests that the observed effect or relationship between the variables is more likely to be genuine and not just a random occurrence. Conversely, higher p-values indicate weaker evidence against the null hypothesis. This means that the observed results are more likely to have occurred by chance under the null hypothesis, and there is insufficient evidence to suggest that the effect or relationship is genuine.\nCompare the p-value to the significance level (\\(\\alpha\\)). If the p-value is less than or equal to the chosen significance level (\\(p \\leq α\\)), then reject the null hypothesis in favor of the alternative hypothesis. This implies that there is statistically significant evidence to support the claim made by the alternative hypothesis. If the p-value is greater than the significance level (\\(p > α\\)), then do not reject the null hypothesis, as there is not enough evidence to support the alternative hypothesis\nInterpret the results and draw conclusions. Based on the comparison between the p-value and the significance level, make a conclusion about the null hypothesis. If you rejected the null hypothesis, this suggests the alternative hypothesis is supported by the data. If you failed to reject the null hypothesis, this means there is insufficient evidence to support the alternative hypothesis. Keep in mind that failing to reject the null hypothesis does not prove it is true; it simply means that the data does not provide strong evidence against it.\n\nIt is crucial to note that failing to reject the null hypothesis does not prove it is true; rather, it indicates that there is not enough evidence to conclude that the alternative hypothesis is true. Additionally, the p-value is subject to various limitations and potential misinterpretations, so it is essential to consider effect sizes, confidence intervals, and other aspects of the data when making conclusions from hypothesis testing."
  },
  {
    "objectID": "i_univartests.html",
    "href": "i_univartests.html",
    "title": "3  Univariate Hypothesis Testing",
    "section": "",
    "text": "4 Z-test\nStep One\nLet \\(\\mu_1\\) represent the mean height of the American male population, and \\(\\mu_2\\) represent the mean height of male NBA players. The null hypothesis (\\(H_0\\)) and alternative hypothesis (\\(H_a\\)) are as follows:\n\\[\n\\begin{align*}\nH_0 &: \\mu_1 = \\mu_2 \\\\\nH_a &: \\mu_1 \\neq \\mu_2\n\\end{align*}\n\\]\nIn other words, the null or status quo hypothesis states that there is no difference in heights between the American male population and NBA players. The alternative hypothesis states that NBA players could be taller or shorter than the American male average.\nStep Two\nI will choose a significance level of 0.05\nStep Three\nSince we know the population variance for height, we will\nA z-test is a statistical test used to determine whether there is a significant difference between a sample mean and a population mean or between two population means, assuming that the population standard deviation is known. The z-test is based on the standard normal distribution (z-distribution), and it is used when the sample size is large (typically n ≥ 30) or when the population is normally distributed.\nThe Data\nBelow is a set of univariate data about height of NBA players, thus a sample of the American population. The population standard deviation is 2.75 inches for adult men in the US."
  },
  {
    "objectID": "i_references.html",
    "href": "i_references.html",
    "title": "References",
    "section": "",
    "text": "Stevens, S. S. 1946. “On the Theory of Scales of\nMeasurement.” Science 103 (2684): 677–80. https://doi.org/10.1126/science.103.2684.677.\n\n\nWarne, R. T. 2020. Statistics for the Social Sciences: A General\nLinear Model Approach. Cambridge University Press. https://books.google.com/books?id=Lw0FEAAAQBAJ."
  }
]