---
output: html_document
editor_options: 
  chunk_output_type: console
---

# Regression Hypothesis Testing

Finding the least squares line for linear regression can be highly useful. Since, if in reality, there is a linear relationship between two variables that could be modeled like the formula below, we can attempt to estimate these parameters using statistics and then predict novel data.

$$
Y_i = \beta_0 + \beta_1 X_i + \epsilon_i \quad \text{where} \
\epsilon_i \sim N(0, \sigma^2)
$$

Even though we may be able to show an estimate for $\beta_0$ and $\beta_1$, we may not know if these are significantly different than if they were the status quo (e.g. no relation between X and Y).

For example, if we wanted to see if there is a relationship between heavy machinery factories (independent) and air pollution (dependent) in a given area, the intercept estimate ($b_0$) would estimate air pollution with zero factories while the slope estimate ($b_1$) would estimate the increase in air pollution for each additional factory.

Hypothesis testing for the intercept would be used to see if air pollution at zero factories is significantly different than zero. Hypothesis testing for the slope would be used to see if the relationship between factories and air pollution is significantly than zero.

## Null Hypothesis Significance Testing

The central dogma of statistics is to make estimates about the population based on samples. Null Hypothesis Significance Testing (NHST) is a widely used statistical approach for testing the validity of a claim or hypothesis about a population based on sample data. The process involves comparing the observed data to what would be expected under a null hypothesis, which usually assumes no effect or relationship between variables.

**Null Hypothesis**

The null hypothesis (denoted as $H_0$) is a statement of no effect or no difference. In linear regression, it represents the default assumption like that there is no relationship between the variables under investigation or that the dependent variable is zero when the observed independent variable is zero. 

**Alternative Hypothesis**

The alternative hypothesis (denoted as $H_a$) is a statement that contradicts the null hypothesis. In linear regression, it represents the possibility that there is a relation between the variables like that there is a change in the dependent variable when the independent variable is observed to increase or that the dependent variable is not zero when the observed independent variable is zero. 

**Hypotheses for Simple Linear Regression**

The pair of hypotheses stated above about the slope and intercept can be formally written as so below:

$$
\left.\begin{array}{ll}
H_0: \beta_1 = 0 \\  
H_a: \beta_1 \neq 0
\end{array}
\right\} \ \text{Slope Hypotheses}\\\;\\\;
\left.\begin{array}{ll}
H_0: \beta_0 = 0 \\  
H_a: \beta_0 \neq 0
\end{array}
\right\} \ \text{Intercept Hypotheses}
$$

### The Process of Null Hypothesis Significance Testing

1.  State the **null hypothesis** ($H_0$) and **alternative hypothesis** ($H_a$). Since there are two estimated parameters, two pairs of hypotheses are made.

2.  Choose a **significance level**. The significance level, denoted by $\alpha$, is the probability of rejecting the null hypothesis when it is true. The most commonly used significance level is 0.05

3.  Collect data and calculate the **test statistic**. Gather your sample data and perform the appropriate statistical test to calculate the test statistic. The test statistic is a numerical value that measures the difference between the observed data and what would be expected under the null hypothesis. The Central Limit Theorem says that the distribution of sample estimates for $\beta_0$ and $\beta_1$ will be normally distributed. Since the variance for this distribution is uncertain, we use a t-test and thus a t-statistic for each hypothesis.

4.  Determine the **p-value**. The p-value, denoted as $p$, is the probability of observing a test statistic as extreme or more extreme than the one calculated from the sample data, assuming the null hypothesis is true. It quantifies the evidence against the null hypothesis. Lower p-values indicate stronger evidence against the null hypothesis since they imply that the observed results are less likely to have occurred by chance alone under the null hypothesis. In other words, a smaller p-value suggests that the observed effect or relationship between the variables is more likely to be genuine and not just a random occurrence. Conversely, higher p-values indicate weaker evidence against the null hypothesis. This means that the observed results are more likely to have occurred by chance under the null hypothesis, and there is insufficient evidence to suggest that the effect or relationship is genuine.

5.  Compare the p-value to the significance level ($\alpha$). If the p-value is less than or equal to the chosen significance level ($p \leq \alpha$), then reject the null hypothesis in favor of the alternative hypothesis. This implies that there is statistically significant evidence to support the claim made by the alternative hypothesis. If the p-value is greater than the significance level ($p > \alpha$), then do not reject the null hypothesis, as there is not enough evidence to support the alternative hypothesis

## Testing the Intercept and Slope

Below is a sample of 32 cars from the early 20th century. Each point represents a car where the X axis shows the independent variable (car weight) and the Y axis shows gross horsepower.

```{r}
#| warning: false
#| message: false
#| code-fold: true

library(tidyverse)
library(plotly)

(ggplot(mtcars, aes(x = wt, y = hp)) +
  geom_point(color = 'darkblue',
               fill='lightblue',
               size=2.5,
               shape=21,
               alpha=.9) +
  theme_classic() +
  geom_smooth(method = "lm", se = FALSE) 
  ) %>% 
  ggplotly()
```

### Our Measly Sample

There are more than 32 cars out there in the world. See below; our sample of 32 cars happens to show the Y-intercept being -1.821 when we try to model weight and horsepower and an increase of 46.16 horsepower for every increase of 1000 pounds. Suppose we sampled a different set of cars and found that the intercept was actually 5 horsepower for a 0 pound car... or -3 horsepower... or 100 horsepower for a zero pound car. Similarly, for the slope, maybe if we drew another sample of cars- we would get a different slope. We could get a nearly endless amount of combinations depending on what our sample was.


```{r}
#| warning: false
#| message: false
#| code-fold: true
lm(hp~wt, data=mtcars) %>% pander::pander()
```


What would happen if we kept collecting samples from cars and checking each sample's Y-intercept and slope? We would get a distribution of samples.

**A sampling distribution**

The sampling distribution of the y-intercept ($b_0$) would be the collection of all possible estimates of the y-intercept obtained from different samples of the same population. Similarly, the sampling distribution of the slope ($b_1$) would be the collection of all possible estimates of the slope obtained from different samples of the same population. 

### The Central Limit Theorem

In the above section, we mentioned the existence of a sampling distribution for each estimated parameter. The Central Limit Theorem (CLT) is a fundamental concept in probability and statistics that states that, given a large enough sample size, the distribution of the sample parameter, in this case, potential Y-intercepts or slopes of a random variable will approach a normal distribution, regardless of the shape of the underlying population distribution. This theorem is crucial because many statistical methods, including linear regression, rely on the assumption that the data are normally distributed.

**Y-intercept ($\beta_0$)**

The sampling distribution of the y-intercept ($\beta_0$) is the collection of all possible estimates of the y-intercept estimates ($b_0$) obtained from different samples of the same population. It centers around the unknown true Y intercept parameter and has an unknown spread. It describes the probability distribution of the estimates.

![](images/beta0s.png){fig-align="center" width="400"}

**Slope ($\beta_1$)**

The sampling distribution of the slopes ($\beta_1$) is the collection of all possible estimates of the slope estimates ($b_0$) obtained from different samples of the same population. It centers around the unknown true slope parameter and has an unknown spread. It describes the probability distribution of the estimates. It describes the probability distribution of the estimates and provides insight into the variability of the estimates.

![](images/beta1s.png){fig-align="center" width="400"}

**Visualizing the Central Limit Theorem**

Below is an applet that allows you to draw a parent population distribution and see how parent population, sample size, and number of samples drawn will change the shape, spread, and center of the sampling distribution.


```{=html}
<center>
<div width="80%">
<iframe src="https://ajaverett.github.io/data/index.html?width=800&z=1&height=500&left=1&mid=0&right=1&area=a&static=0&title=1&border=0" frameborder="0" width="850" height="750" data-external="1""></iframe>
</div>
</center>
```



### Estimating the Distribution of Sample Intercepts Under the Null Hypothesis

**Y-intercept ($\beta_0$)**

Under the null hypothesis, we would assume the null hypothesis is $H_0:\beta_0$. What we don't know is $\sigma_{\beta_0}$. We can estimate this parameter with $s_{b_0}$.

$$
\begin{align*}
s_{b_0} &= \sqrt{\text{MSE} \left(\frac{1}{n} + \frac{\bar{X}^2}{\text{SSTO}(X)}\right)}\\
&=\sqrt{\left(\frac{\text{MSE}}{n}\right) +\left(\frac{\text{MSE}}{\text{SSTO}(X)}\bar{X}\right)}\\
\end{align*}
$$

Additionally, this formula can be thought of as so:

$s_{b_0}$ = $\sqrt{} ($ Constant variation $+$ Variation caused since data is centered away from the intercept.$)$

Conceptually, the formula above computes the standard error of the estimated intercept ($\beta_{0}$), which measures the uncertainty or variability associated with the intercept estimate. In other words, it quantifies how precise the estimate is and how much it is expected to vary from one sample to another.

-   $\text{MSE}$: also called $\sigma^2$. This term represents the variance of the residuals (estimated errors) in the regression model. It reflects the degree of dispersion in the observed values around the predicted values. When you multiply with the rest of the formula, you are essentially scaling the standard error of the intercept according to the overall uncertainty or variability in the model's predictions. A larger variance in residuals ($\text{MSE}$) will result in a larger standard error for the intercept, indicating a higher degree of uncertainty in the intercept estimate.


-   $n$: This is the number of observations in the dataset.

-   $\frac{1}{n}$ This term reflects the effect of sample size on the precision of the estimated intercept. As the sample size ($n$) increases, the standard error of the intercept will generally decrease, which means that the intercept estimate becomes more precise with more observations.

-   $\bar{X}$: This is the mean of the predictor (independent) variable values. 

-   $\text{SSTO}(X)$: This is the sum of the squared differences between each predictor variable value ($X_i$) and the mean predictor variable value (̄$\bar{X}$).It can also be written as so: $\sum_{i=1}^{n}{\left(X_i - \bar{X}\right)^2}$ Geometrically, this term represents the sum of the squared horizontal distances between each data point and the vertical line that passes through the mean of the predictor variable values.

-   $\frac{\bar{X}^2}{\text{SSTO}(X)}$ This term captures the influence of the distribution of the predictor variable values on the precision of the estimated intercept. The denominator ($\text{SSTO}(X)$) measures the spread of the predictor variable values around their mean ($\bar{X}$̄). A larger spread in the predictor variable values will generally result in a smaller standard error for the intercept, assuming that the variance of the residuals ($\sigma^2$) remains constant.

Since we have our $\beta_0$ and our $s_{b_0}$, we can estimate the sampling distribution. We will use a t-distribution with $n-2$ degrees of freedom since the spread is estimated:

![](images/beta0s_estimate.png){fig-align="center" width="400"}

**Slope ($\beta_1$)**

Under the null hypothesis, we would assume the null hypothesis is $H_0:\beta_1$. What we don't know is $\sigma_{\beta_1}$. We can estimate this parameter with $s_{b_1}$.

$$
\begin{align*}
s_{b_1} &= \sqrt{\frac{MSE}{\text{SSTO}(X)}}\\
&= \sqrt{\frac{MSE}{\sum(X_i-\bar{X})^2}}\\
\end{align*}
$$

-   $\text{MSE}$: also called $\sigma^2$. This term represents the variance of the residuals (estimated errors) in the regression model. It reflects the degree of dispersion in the observed values around the predicted values. A larger variance in residuals ($\text{MSE}$) will result in a larger standard error for the intercept, indicating a higher degree of uncertainty in the intercept estimate.

-   $\text{SSTO}(X)$: This is the sum of the squared differences between each predictor variable value ($X_i$) and the mean predictor variable value (̄$\bar{X}$).It can also be written as so: $\sum_{i=1}^{n}{\left(X_i - \bar{X}\right)^2}$ Geometrically, this term represents the sum of the squared horizontal distances between each data point and the vertical line that passes through the mean of the predictor variable values.

This formula gets the square root of the ratio of the squared residuals and the total sum of squares. Since we have our $\beta_1$ and our $s_{b_1}$, we can estimate the sampling distribution. We will use a t-distribution with $n-2$ degrees of freedom since the spread is estimated:


![](images/beta1s_estimate.png){fig-align="center" width="400"}

### Calculating our Test-Statistic Under the Null Hypothesis

**Y-intercept ($\beta_0$)**

This is the formula for the test statistic for $\beta_0$. It measures how far the estimated Y-intercept $b_0$ is from the null hypothesis for $\beta_0$ in units of standard errors ($s_{b_0}$) of $b_0$- thus the division by $s_{b_0}$. Though the hypothesized value of $\beta_0$ is typically 0, it could be any number.


$$
t = \frac{b_0 - (H_0:\beta_0)}{s_{b_0}}
$$

**Slope ($\beta_1$)**


This is the formula for the test statistic for $\beta_1$. It measures how far the estimated slope $b_1$ is from the null hypothesis for $\beta_1$ in units of standard errors ($s_{b_1}$) of $b_1$- thus the division by $s_{b_1}$. Though the hypothesized value of $\beta_0$ is typically 0 (no relationship), it could be any number.


$$
t = \frac{b_1 - (H_:\beta_1)}{s_{b_1}}
$$

### Calculating our P-value Under the Null Hypothesis

The p-value is the probability of obtaining a test statistic as extreme or more extreme than the observed value, assuming the null hypothesis is true. We can mathematically calculate this by using the t-distribution

Let $T \sim \mathcal{t}_{(n-2)}$ be a t-distributed random variable with $n-2$ degrees of freedom and $t$ be the observed t-score. Given this observed t-score, we can then compute the p-value, which is the probability of observing a t-score as extreme as $t$ under the null hypothesis.


**Y-intercept ($\beta_0$)**

 -  For a two-tailed t-test (where we're testing for a Y-intercept greater or less than what we are hypothesizing):  When $H_a: \beta_0 \neq 0$, then $p = 2 \times (1 - \text{Pr}(T \leq |t|))$

 -  For a right-tailed t-test (where we're testing for a Y-intercept greater than what we are hypothesizing):  When $H_a: \beta_0 > 0$, then $p = 1 - \text{Pr}(T \leq t)$

 -  For a left-tailed t-test (where we're testing for a Y-intercept less than what we are hypothesizing):  When $H_a: \beta_0 < 0$, then $p = \text{Pr}(T \leq t)$

**Slope ($\beta_1$)**

 -  For a two-tailed t-test (where we're testing for a slope greater or less than what we are hypothesizing):  When $H_a: \beta_1 \neq 0$, then $p = 2 \times (1 - \text{Pr}(T \leq |t|))$

 -  For a right-tailed t-test (where we're testing for a slope greater than what we are hypothesizing):  When $H_a: \beta_1 > 0$, then $p = 1 - \text{Pr}(T \leq t)$

 -  For a left-tailed t-test (where we're testing for a slope less than what we are hypothesizing):  When $H_a: \beta_1 < 0$, then $p = \text{Pr}(T \leq t)$

Where $t$ is the t-statistic from the sample and $t_{n-2}$ is the t-distribution with $n-2$ degrees of freedom with a mean of $H_0:\beta_0$ or $H_0:\beta_1$ and a standard deviation of $s_{b_0}$ or $s_{b_1}$. This can be illustrated below. 

```{=html}
<center>
<div width="90%">
<iframe src="https://byuimath.com/apps/normprobwitht.html?width=600&z=1&height=400&left=1&mid=0&right=1&area=a&static=0&title=1&border=0" frameborder="0" width="650" height="450" data-external="1"></iframe>
</div>
</center>
```


## Setting Confidence Intervals

Confidence intervals, instead of exploiting the null hypothesis distribution to calculate uncertainty, use the sample distribution itself to calculate the likelihood that the true population parameter is withing the boundaries. With linear regression, confidence intervals are calculated for each coefficient of the regression model, $\beta_0$ (intercept) and $\beta_1$ (slope). The main idea is to provide a range in which the true population parameters are likely to lie, given the data and a specified level of confidence (usually 95%). 

The formula to calculate the confidence interval is below.

$$
\text{CI}(\beta_0) = b_0 \pm t^*_{n-2}\times s_{b_0} : \alpha=0.05
$$

$$
\text{CI}(\beta_1) = b_1 \pm t^*_{n-2}\times s_{b_1} : \alpha=0.05
$$

the $t^*$ is used to represent the t-value in which the area in between the negative and positive t-value on a t-distribution is .95.

When we say that the confidence interval captures the true parameter values "95% of the time," it means that if we were to repeat the sampling process many times and compute the confidence intervals for each sample, about 95% of those intervals would contain the true population parameter. Note that this doesn't mean there's a 95% probability that the true parameter lies within the specific confidence interval calculated from one sample; rather, it means that the procedure we use to construct the interval, when repeated across multiple samples, would produce intervals that contain the true parameter value in 95% of cases.
